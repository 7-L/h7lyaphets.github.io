[{"title":"HBase学习笔记（一）- 快速入门","date":"2017-11-21T09:25:39.000Z","path":"2017/11/21/大数据/HBase/HBase学习笔记（一）- 快速入门/","text":"1. HBase概述 高可靠性、高性能、面向列、可伸缩的分布式存储系统。 利用HDFS作为其文件存储系统。 利用MapReduce处理数据。 利用ZooKeeper作为协同服务。 与传统数据库对比的优势 线性扩展,数据增多,可以直接增加节点来扩容 数据存储在HDFS,备份机制健全 通过ZK协调查找数据,访问速度快 HBase集群中的角色 HMaster HRegionServer Q1: 为什么会有HBase？ 假设有100W个access.log，每个log大小为1KB，如果使用API向HDFS集群中写，NameNode的压力会很大。使用HBase可以解决这个问题（文件合并与拆分） Q2: HBase存储和HDFS存储的关系? HDFS: Client -&gt; NameNode -&gt; DataNode HBase: Client -&gt; HMaster -&gt; HRegionServer -&gt; Zookeeper(元数据) -&gt; HDFS(数据文件) 每条数据线缓存到HRegionServer的内存中,当达到Block大小时再写入HDFS。 HBase中只有表结构和列族,其他都是数据。 1个列族中有多个列,被存储到1个文件中。 Hbase中不能删除数据,如果对一个相同的行键插入数据（不同时间戳）,就认为是修改。 2. 安装与配置 下载 hbase-1.2.6-bin.tar.gz 解压、配置环境变量 修改配置文件 hbase-env.sh 12export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_71.jdk/Contents/Homeexport HBASE_MANAGES_ZK=false hbase-site.xml 1234567891011121314 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://127.0.0.1:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;127.0.0.1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; - regionservers 1localhost 启动 start-hbase.sh 监控 进入命令行 hbase shell 页面监控 http://master:16010","tags":[{"name":"大数据","slug":"大数据","permalink":"https://seawaylee.github.io/tags/大数据/"},{"name":"HBase","slug":"HBase","permalink":"https://seawaylee.github.io/tags/HBase/"}]},{"title":"Netty学习笔记(一)","date":"2017-10-08T11:32:37.000Z","path":"2017/10/08/Java基础/NIO/Netty学习本笔记(一)/","text":"","tags":[{"name":"Java","slug":"Java","permalink":"https://seawaylee.github.io/tags/Java/"},{"name":"NIO","slug":"NIO","permalink":"https://seawaylee.github.io/tags/NIO/"}]},{"title":"大数据架构辅助系统（二）- Azkaban学习笔记","date":"2017-10-08T11:24:28.000Z","path":"2017/10/08/大数据/辅助系统/Azkaban/","text":"1 工作流调度器1.1 为什么需要工作流调度系统 一个完整的数据分析系统通常都是由大量任务单元组成： shell脚本程序 java程序 mapreduce程序 hive脚本等、 各任务单元之间存在时间先后及前后依赖关系 为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行 例如，我们可能有这样一个需求，某个业务系统每天产生20G原始数据，我们每天都要对其进行处理，处理步骤如下所示： 通过Hadoop先将原始数据同步到HDFS上； 借助MapReduce计算框架对原始数据进行转换，生成的数据以分区表的形式存储到多张Hive表中； 需要对Hive中多个表的数据进行JOIN处理，得到一个明细数据Hive大表； 将明细数据进行复杂的统计分析，得到结果报表信息； 需要将统计分析得到的结果数据同步到业务系统中，供业务调用使用。 1.2 工作流调度实现方式 简单的任务调度：直接使用linux的crontab来定义； 复杂的任务调度： 开发调度平台 使用现成的开源调度系统，比如ooize、azkaban等 1.3 常见工作流调度系统市面上目前有许多工作流调度器在hadoop领域，常见的工作流调度器有Oozie, Azkaban,Cascading,Hamake等 各种调度工具特性对比 下面的表格对上述四种hadoop工作流调度器的关键特性进行了比较，尽管这些工作流调度器能够解决的需求场景基本一致，但在设计理念，目标用户，应用场景等方面还是存在显著的区别，在做技术选型的时候，可以提供参考 2 Azkaban Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。 它有如下功能特点： Web用户界面 方便上传工作流 方便设置任务之间的关系 调度工作流 认证/授权(权限的工作) 能够杀死并重新启动工作流 模块化和可插拔的插件机制 项目工作区 工作流和任务的日志记录和审计 2.1 安装和部署Azkaban Web服务器azkaban-web-server-2.5.0.tar.gz Azkaban执行服务器 azkaban-executor-server-2.5.0.tar.gz 下载地址:http://azkaban.github.io/downloads.html MySQL目前azkaban只支持 mysql,需安装mysql服务器 安装 将安装文件上传到集群,最好上传到安装 hive、sqoop的机器上,方便命令的执行 在当前用户目录下新建 azkabantools目录,用于存放源安装文件.新建azkaban目录,用于存放azkaban运行程序 azkaban web服务器安装 解压azkaban-web-server-2.5.0.tar.gz 命令: tar –zxvf azkaban-web-server-2.5.0.tar.gz 将解压后的azkaban-web-server-2.5.0 移动到 azkaban目录中,并重新命名 webserver azkaban 执行服器安装 解压azkaban-executor-server-2.5.0.tar.gz 命令:tar –zxvf azkaban-executor-server-2.5.0.tar.gz 将解压后的azkaban-executor-server-2.5.0 移动到 azkaban目录中,并重新命名 executor azkaban脚本导入 解压: azkaban-sql-script-2.5.0.tar.gz 命令:tar –zxvf azkaban-sql-script-2.5.0.tar.gz 将解压后的mysql 脚本,导入到mysql中: 进入mysql mysql&gt; create database azkaban; mysql&gt; use azkaban; Database changed mysql&gt; source /home/hadoop/azkaban-2.5.0/create-all-sql-2.5.0.sql; 由于Azkaban的web管理页面需要HTTPS访问，所以需要配置HTTPS服务 SSL配置 参考地址: http://docs.codehaus.org/display/JETTY/How+to+configure+SSL 命令: keytool -keystore keystore -alias jetty -genkey -keyalg RSA 将在当前目录生成 keystore 证书文件,将keystore 拷贝到 azkaban web服务器根目录中.如:cp keystore azkaban/server 启动Web服务 ./bin/azkaban-web-start.sh 启动Executor ./bin/azkaban-executor-start.sh 2.2 Azkaban实战 Azkaba内置的任务类型支持command、java 2.2.1 Command类型单一job示例 创建job描述文件 vi command.job 123#command.jobtype=command command=echo &apos;hello&apos; 将job资源文件打包成zip文件 zip command.job 通过azkaban的web管理平台创建project并上传job压缩包 创建project 上传zip任务包 执行job 2.2.2 Command类型多Job工作流Flow 创建有依赖关系的多个job描述 第一个job：foo.job 123# foo.jobtype=commandcommand=echo foo 第二个job：bar.job依赖foo.job 1234# bar.jobtype=commanddependencies=foocommand=echo bar 将所有job资源文件打到一个zip包中 在azkaban的web管理界面创建工程并上传zip包 启动工作流flow 2.2.3 HDFS操作任务 创建job描述文件 123# fs.jobtype=commandcommand=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop fs -mkdir /azaz 将job资源文件打包成zip文件 通过azkaban的web管理平台创建project并上传job压缩包 启动执行该job 2.2.3 MAPREDUCE任务 Mr任务依然可以使用command的job类型来执行 创建job描述文件，及mr程序jar包（示例中直接使用hadoop自带的example jar） 123# mrwc.jobtype=commandcommand=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop jar hadoop-mapreduce-examples-2.6.1.jar wordcount /wordcount/input /wordcount/azout 将所有job资源文件打到一个zip包中 在azkaban的web管理界面创建工程并上传zip包 启动job 2.2.4 HIVE脚本任务 创建job描述文件和hive脚本 Hive脚本： test.sql 123456use default;drop table aztest;create table aztest(id int,name string) row format delimited fields terminated by ',';load data inpath '/aztest/hiveinput' into table aztest;create table azres as select * from aztest;insert overwrite directory '/aztest/hiveoutput' select count(1) from aztest; Job描述文件：hivef.job 123# hivef.jobtype=commandcommand=/home/hadoop/apps/hive/bin/hive -f 'test.sql' 将所有job资源文件打到一个zip包中 在azkaban的web管理界面创建工程并上传zip包 启动job","tags":[{"name":"大数据","slug":"大数据","permalink":"https://seawaylee.github.io/tags/大数据/"}]},{"title":"大数据架构辅助系统（一）- Flume学习笔记","date":"2017-09-28T15:29:59.000Z","path":"2017/09/28/大数据/辅助系统/Flume/","text":"在一个完整的大数据处理系统中，除了hdfs+mapreduce+hive组成分析系统的核心之外，还需要数据采集、结果数据导出、任务调度等不可或缺的辅助系统，而这些辅助工具在hadoop生态体系中都有便捷的开源框架，如图所示： 1 Flume介绍1.1 概述 Flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。 Flume可以采集文件，socket数据包等各种形式源数据，又可以将采集到的数据输出到HDFS、hbase、hive、kafka等众多外部存储系统中 一般的采集需求，通过对flume的简单配置即可实现 Flume针对特殊场景也具备良好的自定义扩展能力，因此，flume可以适用于大部分的日常数据采集场景 1.2 运行机制 Flume分布式系统中最核心的角色是agent，flume采集系统就是由一个个agent所连接起来形成。 每一个agent相当于一个数据传递员，内部有三个组件： Source：采集源，用于跟数据源对接，以获取数据 Sink：下沉地，采集数据的传送目的地，用于往下一级agent传递数据或者往最终存储系统传递数据 Channel：angent内部的数据传输通道，用于从source将数据传递到sink 1.3 Flume采集系统结构图简单结构 - 单个agent采集数据 复杂结构 - 多级agent之间串联 2 Flume实战2.1 安装 Flume的安装非常简单，只需要解压即可，当然，前提是已有hadoop环境 安装 上传安装包到数据源所在节点上 解压 tar -zxvf apache-flume-1.6.0-bin.tar.gz 进入flume的目录，修改conf下的flume-env.sh，在里面配置JAVA_HOME 根据数据采集的需求配置采集方案，描述在配置文件中(文件名可任意自定义) 指定采集方案配置文件，在相应的节点上启动flume agent 2.2 部署测试先用一个最简单的例子来测试一下程序环境是否正常 在flume的conf目录下新建一个文件 vi netcat-logger.conf 123456789101112131415161718192021# 定义这个agent中各组件的名字a1.sources = r1a1.sinks = k1a1.channels = c1# 描述和配置source组件：r1a1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# 描述和配置sink组件：k1a1.sinks.k1.type = logger# 描述和配置channel组件，此处使用是内存缓存的方式a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# 描述和配置source channel sink之间的连接关系a1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动agent采集数据 1bin/flume-ng agent -c conf -f conf/netcat-logger.conf -n a1 -Dflume.root.logger=INFO,console * -c conf 指定flume自身的配置文件所在目录 * -f conf/netcat-logger.con 指定我们所描述的采集方案 * -n a1 指定我们这个agent的名字 测试 先要往agent采集监听的端口上发送数据，让agent有数据可采 随便在一个能跟agent节点联网的机器上 telnet anget-hostname port （telnet localhost 44444） 采集目录到HDFS 采集需求：某服务器的某特定目录下，会不断产生新的文件，每当有新文件出现，就需要把文件采集到HDFS中去 根据需求，首先定义以下3大要素 采集源，即source——监控文件目录 : spooldir 下沉目标，即sink——HDFS文件系统 : hdfs sink source和sink之间的传递通道——channel，可用file channel 也可以用内存channel 配置文件编写 123456789101112131415161718192021222324252627282930313233343536373839#定义三大组件的名称agent1.sources = source1agent1.sinks = sink1agent1.channels = channel1# 配置source组件agent1.sources.source1.type = spooldiragent1.sources.source1.spoolDir = /home/hadoop/logs/agent1.sources.source1.fileHeader = false#配置拦截器agent1.sources.source1.interceptors = i1agent1.sources.source1.interceptors.i1.type = hostagent1.sources.source1.interceptors.i1.hostHeader = hostname# 配置sink组件agent1.sinks.sink1.type = hdfsagent1.sinks.sink1.hdfs.path =hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%Magent1.sinks.sink1.hdfs.filePrefix = access_logagent1.sinks.sink1.hdfs.maxOpenFiles = 5000agent1.sinks.sink1.hdfs.batchSize= 100agent1.sinks.sink1.hdfs.fileType = DataStreamagent1.sinks.sink1.hdfs.writeFormat =Textagent1.sinks.sink1.hdfs.rollSize = 102400agent1.sinks.sink1.hdfs.rollCount = 1000000agent1.sinks.sink1.hdfs.rollInterval = 60#agent1.sinks.sink1.hdfs.round = true#agent1.sinks.sink1.hdfs.roundValue = 10#agent1.sinks.sink1.hdfs.roundUnit = minuteagent1.sinks.sink1.hdfs.useLocalTimeStamp = true# Use a channel which buffers events in memoryagent1.channels.channel1.type = memoryagent1.channels.channel1.keep-alive = 120agent1.channels.channel1.capacity = 500000agent1.channels.channel1.transactionCapacity = 600# Bind the source and sink to the channelagent1.sources.source1.channels = channel1agent1.sinks.sink1.channel = channel1 Channel参数解释： capacity：默认该通道中最大的可以存储的event数量 trasactionCapacity：每次最大可以从source中拿到或者送到sink中的event数量 keep-alive：event添加到通道中或者移出的允许时间 采集文件到HDFS 采集需求：比如业务系统使用log4j生成的日志，日志内容不断增加，需要把追加到日志文件中的数据实时采集到hdfs 根据需求，首先定义以下3大要素 采集源，即source——监控文件内容更新 : exec ‘tail -F file’ 下沉目标，即sink——HDFS文件系统 : hdfs sink Source和sink之间的传递通道——channel，可用file channel 也可以用 内存channel 配置文件编写 12345678910111213141516171819202122232425262728293031323334353637383940agent1.sources = source1agent1.sinks = sink1agent1.channels = channel1# Describe/configure tail -F source1agent1.sources.source1.type = execagent1.sources.source1.command = tail -F /home/hadoop/logs/access_logagent1.sources.source1.channels = channel1#configure host for sourceagent1.sources.source1.interceptors = i1agent1.sources.source1.interceptors.i1.type = hostagent1.sources.source1.interceptors.i1.hostHeader = hostname# Describe sink1agent1.sinks.sink1.type = hdfs#a1.sinks.k1.channel = c1agent1.sinks.sink1.hdfs.path =hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%Magent1.sinks.sink1.hdfs.filePrefix = access_logagent1.sinks.sink1.hdfs.maxOpenFiles = 5000agent1.sinks.sink1.hdfs.batchSize= 100agent1.sinks.sink1.hdfs.fileType = DataStreamagent1.sinks.sink1.hdfs.writeFormat =Textagent1.sinks.sink1.hdfs.rollSize = 102400agent1.sinks.sink1.hdfs.rollCount = 1000000agent1.sinks.sink1.hdfs.rollInterval = 60agent1.sinks.sink1.hdfs.round = trueagent1.sinks.sink1.hdfs.roundValue = 10agent1.sinks.sink1.hdfs.roundUnit = minuteagent1.sinks.sink1.hdfs.useLocalTimeStamp = true# Use a channel which buffers events in memoryagent1.channels.channel1.type = memoryagent1.channels.channel1.keep-alive = 120agent1.channels.channel1.capacity = 500000agent1.channels.channel1.transactionCapacity = 600# Bind the source and sink to the channelagent1.sources.source1.channels = channel1agent1.sinks.sink1.channel = channel1 2.3 更多source和sink组件Flume支持众多的source和sink类型，详细手册可参考官方文档","tags":[{"name":"大数据","slug":"大数据","permalink":"https://seawaylee.github.io/tags/大数据/"}]},{"title":"Effective Python - 学习笔记","date":"2017-09-05T05:20:54.000Z","path":"2017/09/05/Python基础/Effective Python - 学习笔记/","text":"1 Thinking In Pythonic1.1 Knowing Bytes、Str、Unicode 12345678def str_and_unicode(): &quot;&quot;&quot; In Python3: Str.encode() -&gt; Bytes, Bytes.decode() -&gt; Str In Python2: Unicode.encode() -&gt; Str, Str.decode() -&gt; Unicode &quot;&quot;&quot; my_unicode = u&apos;你好&apos; my_str = my_unicode.encode(&apos;utf-8&apos;) print my_str 1.2 Using List Generator Instead Of Map &amp; Filter1234567891011121314151617def list_generator(): a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] sqr_list_gen = [x ** 2 for x in a] sqr_lambda = map(lambda x: x ** 2, a) sqr_list_gen_with_condition = [x ** 2 for x in a if x % 2 == 0] sqr_map_filter = map(lambda x: x ** 2, filter(lambda x: x % 2 == 0, a)) print sqr_list_gen print sqr_lambda print sqr_list_gen_with_condition print sqr_map_filter print \"============== Pretty Split Line ==============\" chile_ranks = &#123;'tom': 1, 'helen': 2, 'harry_porter': 3&#125; rank_dict = &#123;rank: name for name, rank in chile_ranks.items()&#125; # Dict Generator print rank_dict print \"============== Pretty Split Line ==============\" chile_len_set = &#123;len(name) for name in rank_dict.values()&#125; print chile_len_set 1.3 Don’t Using List Generator More Than Twice With One Time1234567891011121314151617181920212223242526def more_list_generator(): matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] # flat = [e for row in matrix for e in row] flat = [[x for x in row] for row in matrix] print flat sqr = [[e ** 2 for e in row] for row in matrix] print sqr my_lists = [ [[1, 2, 3], [4, 5, 6], [7, 8, 9]] ] # Write Multiple Lines While Using Multiple List Generators flat = [x for s1 in my_lists for s2 in s1 for x in s2] print flat # Multiple Conditions While Creating List a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] b = [x for x in a if x &gt; 4 if x % 2 == 0] print b # Find The Elements Whose RowSum &gt;= 10 And Element % 3 == 0 filtered = [[x for x in row if x % 3 == 0] for row in matrix if sum(row) &gt;= 10] # filtered = [e for row in matrix if sum(row) &gt;= 10 for e in row if e % 3 == 0] print filtered 1.4 Generator Expression123456def generator_expression(): l = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] l_it = (x for x in l) sqr_it = ((x, x ** 2) for x in l_it) for x in sqr_it: print x 1.5 Enumerate1234567891011121314151617def enum_instead_of_range(): from random import randint # Range Is Useful while Handle Integers random_bits = 0 for i in range(64): if randint(0, 1): random_bits |= 1 &lt;&lt; i print \"============== Pretty Split Line ==============\" flavor_list = ['coco', 'coke', 'KungPaoKiTen', 'Banana'] for flavor_index in range(len(flavor_list)): print '%d : %s is delicious' % (flavor_index + 1, flavor_list[flavor_index]) print \"============== Pretty Split Line ==============\" for i, flavor in enumerate(flavor_list, 1): print '%d : %s is delicious' % (i, flavor) 1.6 Zip123456def iterator_multiple_collections_by_zip(): names = ['Niko', 'Tom', 'Helen'] ages = [21, 15, 25] sizes = &#123;'A': 35, 'B': 60, 'C': 22&#125; for name, age, size, in zip(names, ages, sizes): print name, age, size 1.7 For Else12345678def for_else(): num_list = [x for x in range(100)] for n in num_list: print n if n == 20: break else: print 'for never break.' 1.8 Try Except Else Finally123456789101112131415def try_except_else_finally(): f = open('/Users/lixiwei-mac/Documents/IdeaProjects/PythonStudy/effective_python/test.txt', 'r') try: data = f.read() if len(data) &lt; 100: raise Exception(\"Shit ! This fucking file is too short\") except Exception as e: raise ValueError(e) else: print 'There is no exception happend.' print data return data finally: print 'closing file handle' f.close() 2 Function2.1 Closure12345678910111213141516171819202122232425262728293031from itertools import islicedef sort_priority(values, group): def helper(x): if x in group: return 0, x return 1, x values.sort(key=helper) def sort_priority_v2(values, group): found = [False] # 位于此作用于的列表、元组、字典可以被内部函数搜索并修改 def helper(x): # python3中可以使用nonlocal来在此处声明found，以表示查找并修改外层作用域的found if x in group: found[0] = True if found: return 0, x return 1, x values.sort(key=helper) return found def do_sort_proority(): nums = [2, 5, 6, 5, 4, 7, 45, 54, 8] group = &#123;2, 4, 45, 8&#125; found = sort_priority_v2(nums, group) print nums print found 2.2 Generator12345678910111213141516def get_first_letter(text): if text: yield 0 for index, letter in enumerate(text): if letter == ' ': yield index + 1def do_get_first_letter(): # for index in get_first_letter('Niko Belic Is A Good Boy'): # print index # print list(get_first_letter('Niko Belic Is A Good Boy')) index_iterator = get_first_letter('Niko Belic Is A Good Boy') print list(islice(index_iterator, 0, 3)) 2.3 Iterate In Params1234567891011121314151617181920212223def normalize(numbers): total = sum(numbers) # return a new iterator result = [] for value in numbers: # return a new iterator percent = 100 * value / total result.append(percent) return resultclass ReadVisits: def __init__(self, numbers): self.numbers = numbers def __iter__(self): for value in self.numbers: yield valuedef do_nomalize(): numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] visits = ReadVisits(numbers) print iter(visits) is iter(visits) 3 Property3.1 Using property instead of get and set12345678910111213141516171819202122class Register(object): def __init__(self, name, age): self._name = name self.age = age self.logo = self._name + \",\" + str(self.age) @property def name(self): return self._name @name.setter def name(self, name): \"\"\"You can do something more than just setting property in this method\"\"\" self._name = name # Do not use self.name = name, it will raise max recursive exception self.logo = self._name + \",\" + str(self.age)if __name__ == '__main__': r = Register('Niko', 18) r.name = 'Belic' print r.logo 4 Concurrency4.1 用subprocess模块来管理子进程1234567891011121314151617181920212223242526272829303132333435363738394041424344import subprocessimport threadingfrom Queue import Queuefrom time import time, sleepfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutordef sub_process_test1(): \"\"\" 使用Popen构造器来启动进程 communicate方法读取紫禁城的输出信息，并等待其终止 \"\"\" proc = subprocess.Popen(['echo', 'Hello NikoBelic'], stdout=subprocess.PIPE) out, err = proc.communicate() print out.decode('utf-8')def sub_process_test2(): \"\"\" 使用poll查询子进程状态 \"\"\" proc = subprocess.Popen(['sleep', '0.1']) while proc.poll() is None: print 'Working' print 'Exit status', proc.poll()def run_sleep(period): proc = subprocess.Popen(['sleep', str(period)]) return procdef sub_process_test3(): \"\"\" 测试并行的性能，10个单个耗时1秒钟的任务，串行需要10秒，并行只需要1秒\"\"\" start = time() procs = [] for _ in range(10): proc = run_sleep(1) procs.append(proc) for proc in procs: proc.communicate() end = time() print 'Finished in %.3f seconds' % (end - start) 4.2 可以用现成来执行阻塞式IO，但不要用它做平行计算123456789101112131415161718192021222324252627282930313233def factorize(number): for i in range(1, number + 1): if number % i == 0: yield idef do_factorize(): numbers = [12431241, 42352452, 4574543, 1231224] start = time() for number in numbers: list(factorize(number)) end = time() print 'It tooke %.3f seconds' % (end - start)def parallel_do_factorize(): \"\"\" 改成并行模式发现时间还长了，因为是Python的GIL缘故，导致CPU不能真正的并行 \"\"\" numbers = [12431241, 42352452, 4574543, 1231224] start = time() threads = [] def run(number): list(factorize(number)) for number in numbers: thread = threading.Thread(target=run, args=(number,)) thread.start() threads.append(thread) for thread in threads: thread.join() end = time() print 'It tooke %.3f seconds' % (end - start) 4.3 在线程中使用Lock来防止数据竞争1234567891011121314151617181920212223242526272829303132333435363738394041424344class Counter(object): def __init__(self): self.count = 0 def increment(self, offset): \"\"\" A += B 会拆分三步进行计算，在多线程切换的情况下，会出现新值被旧值替换的问题 \"\"\" self.count += offsetdef worker(how_many, counter, split_num): for _ in range(how_many / split_num): counter.increment(1)def run_threads(func, how_many, counter): threads = [] split_num = 5 for _ in range(split_num): args = (how_many, counter, split_num) thread = threading.Thread(target=func, args=args) thread.start() threads.append(thread) for thread in threads: thread.join()def do_incr(): how_many = 10 ** 5 # counter = Counter() counter = LockCounter() run_threads(worker, how_many, counter) print 'should be %d , found %d' % (how_many, counter.count) # should be 100000 , found 67760class LockCounter(object): def __init__(self): self.lock = threading.Lock() self.count = 0 def increment(self, offset): with self.lock: self.count += offset 4.4 用Queue来协调各线程之间的工作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162from collections import deque, namedtupleclass MyQueue(object): \"\"\" 自定义队列模型 \"\"\" def __init__(self): self.items = deque() self.lock = threading.Lock() def put(self, item): with self.lock: self.items.append(item) def get(self): with self.lock: return self.items.popleft()class Worker(threading.Thread): \"\"\" 消费者线程 \"\"\" def __init__(self, func, in_queue, out_queue): super(Worker, self).__init__() self.func = func self.in_queue = in_queue self.out_queue = out_queue self.polled_count = 0 self.work_done = 0 def run(self): while True: self.polled_count += 1 try: self.in_queue.get() except IndexError: sleep(0.01) # No work to do else: result = self.func() self.out_queue.put(result) self.work_done += 1def download(): print '正在执行 download 任务...'def resize(): print '正在执行 resize 任务 ...'def upload(): print '正在执行 upload 任务...'def do_work_with_my_queue(): \"\"\" 这种消费者生产者模型有4个缺点： 1. 当消费者无法从队列中获取数据时，会导致CPU空转，浪费资源 2. Worker的run方法会一直执行其循环，即使应该退出了也不会停止 3. 为了判断所有任务是否已经处理完毕，需要一直循环检查 4. 如果某个步骤停滞，将导致内存溢出、程序崩溃 \"\"\" download_queue = MyQueue() resize_queue = MyQueue() upload_queue = MyQueue() done_queue = MyQueue() threads = [ Worker(download, download_queue, resize_queue), Worker(resize, resize_queue, upload_queue), Worker(upload, upload_queue, done_queue) ] for thread in threads: thread.start() download_num = 1000 for _ in range(download_num): download_queue.put(object()) while len(done_queue.items) &lt; download_num: continue processed = len(done_queue.items) polled = sum(t.polled_count for t in threads) print 'Processed', processed, 'items after polling', polled, 'times' # Processed 1000 items after polling 3013 timesdef test_queue(): queue = Queue(1) def consumer(): print 'Consumer waiting' queue.get() # will blocked if queue is empty print 'Consumer done' def producer(): print 'Producer putting' queue.put(object()) # will blocked if queue is full print 'Producer done' for _ in range(10): threading.Thread(target=producer).start() print 'Will consume the items' for _ in range(10): threading.Thread(target=consumer).start() queue.task_done()class ClosableQueue(Queue): \"\"\" 使用Queue来解决以上问题 \"\"\" SENTINEL = object() def close(self): self.put(self.SENTINEL) def __iter__(self): while True: item = self.get() try: if item is self.SENTINEL: return yield item finally: self.task_done()class ConsumerThread(threading.Thread): def __init__(self, queue): super(ConsumerThread, self).__init__() self.queue = queue def run(self): while True: self.queue.get() print threading.currentThread().getName() + \" 消费了 \" sleep(1)class ProducerThread(threading.Thread): def __init__(self, queue): super(ProducerThread, self).__init__() self.queue = queue def run(self): while True: self.queue.put('1') print threading.currentThread().getName() + \" 生产了 \" sleep(1)def do_consume_produce(): queue = ClosableQueue() ProducerThread(queue).start() ConsumerThread(queue).start() 4.5 考虑用携程来并发地运行多个函数1234567891011121314151617181920212223242526272829def my_coroutine(): while True: received = yield print 'received', receiveddef do_test_coroutine(): it = my_coroutine() next(it) it.send('First') it.send('Second')def minimize(): current = yield while True: value = yield current # 实际上这里会拆分为两步： 返回current，等待下一个send参数 current = min(value, current)def do_test_minize(): it = minimize() next(it) print it.send(10) print it.send(4) print it.send(22) print it.send(-1) 4.6 考虑用concurrent.futures来实现真正的平行计算12345678910111213141516171819def gcd(pair): a, b = pair low = min(a, b) for i in range(low, 0, -1): if a % i == 0 and b % i == 0: return idef do_calc_gcd(): numbers = [(4325435, 3453463), (756756734, 43263546), (4564565, 234523546), (46575467, 23421345)] # numbers = [(4325435, 3453463)] start = time() # results = list(map(gcd, numbers)) # 单线程 7s # pool = ThreadPoolExecutor(max_workers=4) # 线程池方法 12s pool = ProcessPoolExecutor(max_workers=4) # 进程池方法 4s result = list(pool.map(gcd, numbers)) end = time() print 'Took %.3f seconds' % (end - start)","tags":[{"name":"Python基础","slug":"Python基础","permalink":"https://seawaylee.github.io/tags/Python基础/"}]},{"title":"Mysql快速重装与配置","date":"2017-09-04T11:12:41.000Z","path":"2017/09/04/Mysql/Mysql快速重装与配置/","text":"1 彻底删除旧版 yum remove mysql mysql-server mysql-libs mysql-server find / -name mysql 将找到的相关东西delete掉； rpm -qa|grep mysql (查询出来的东东yum remove掉) 2 安装、启动Mysql yum install mariadb-server -y systemctl start mariadb.service systemctl enable mariadb.service mysql_secure_installation 初始化配置 mysql 3 开启远程访问 方法一 本地登入mysql，更改 “mysql” 数据库里的 “user” 表里的 “host” 项，将”localhost”改为”%” mysql -u root -proot mysql&gt;use mysql; mysql&gt;update user set host = ‘%’ where user = ‘root’; mysql&gt;select host, user from user; 方法二 直接授权(推荐) 从任何主机上使用root用户，密码：youpassword（你的root密码）连接到mysql服务器： mysql -uroot -proot mysql&gt;GRANT ALL PRIVILEGES ON . TO ‘root’@’%’ IDENTIFIED BY ‘youpassword’ WITH GRANT OPTION;","tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://seawaylee.github.io/tags/Mysql/"}]},{"title":"PythonWeb - 一次关于Celery的性能调试","date":"2017-08-27T07:34:38.000Z","path":"2017/08/27/PythonWeb/PythonWeb - 一次关于Celery的性能调试/","text":"TODO","tags":[{"name":"PythonWeb","slug":"PythonWeb","permalink":"https://seawaylee.github.io/tags/PythonWeb/"},{"name":"Celery","slug":"Celery","permalink":"https://seawaylee.github.io/tags/Celery/"}]},{"title":"PythonWeb - 学习笔记（一）- Flask","date":"2017-08-27T07:33:13.000Z","path":"2017/08/27/PythonWeb/PythonWeb - 学习笔记（一）- Flask/","text":"本部分基于图书《Flask Web开发》学习总结，后续将继续深入学习扩展知识。 学习计划 Flask框架学习与实战 Celery任务调度框架的深入学习与研究 基于WebSocket的网络通讯研究 高并发PyWeb框架Tornado Chapter 1 安装1.安装环境为了防止学习过程中安装的libs干扰了工作环境，建议使用virtualenv或Anaconda来创建虚拟Python环境，与工作环境隔离。 virtualenv与Anaconda对比 Anaconda支持多个python版本，可以创建py2、py3的环境，但是virtualenv只能创建目前系统正在使用的python版本来创建虚拟环境。 Anaconda非常强大，有GUI操作界面，在学习机器学习时安装依赖非常方便。virtualenv就是单纯的创建了一个虚拟环境，很简单。 安装virtualenv sudo easy_install virtualenv mkdir ~/py_envs &amp;&amp; cd ~/py_envs &amp;&amp; virtualenv flask_env source flask_env/bin/activate deactivate (退出虚拟环境) 安装Flask pip install flask Chapter 2 程序的基本结构2.1 一个完整的程序12345678910111213141516171819202122from flask import Flaskapp = Flask(__name__)@app.route(\"/\")def index(): return \"&lt;h1&gt;Home Page222sadasdasd&lt;/h1&gt;\"@app.route(\"/user\")def user(): return \"&lt;h1&gt;User Controller&lt;/h1&gt;\"@app.route(\"/user/&lt;path:name&gt;\")def hello(name): return \"&lt;h1&gt;Hello %s&lt;/h1&gt;\" % nameif __name__ == '__main__': app.run(debug=True) 2.2 请求-响应2.2.1 上下文 var_name context remark current_app 程序上下文 当前激活程序的程序实例 g 程序上下文 处理请求时的临时存储变量 request 请求上下文 请求对象，封装了客户端的HTTP请求中的内容 session 请求上下文 用户会话，用于存储请求之间需要“记住”的内容 12345678910from flask import current_appfrom hello import appif __name__ == '__main__': print current_app.name # RuntimeError: 没激活程序上下文就调用会出现错误 app_ctx = app.app_context() app_ctx.push() print 'current_app.name:',current_app.name app_ctx.pop() OUTPUTcurrent_app.name: hello 2.2.2 请求调度1print 'app.url_map:\\n', app.url_map OUTPUT 1234Map([&lt;Rule &apos;/user&apos; (HEAD, OPTIONS, GET) -&gt; user&gt;, &lt;Rule &apos;/&apos; (HEAD, OPTIONS, GET) -&gt; index&gt;, &lt;Rule &apos;/static/&lt;filename&gt;&apos; (HEAD, OPTIONS, GET) -&gt; static&gt;, &lt;Rule &apos;/user/&lt;name&gt;&apos; (HEAD, OPTIONS, GET) -&gt; hello&gt;]) 2.2.3 请求钩子 before_first_request: 第一个请求之前运行 before_request: 每次请求之前运行 after_request: 如果没有未处理的异常抛出，在每次请求后运行 teardown_request: 即使有未处理的异常抛出，也在每次请求之后运行 2.3.4 响应make_response(&quot;&lt;h1&gt;Hello %s&lt;/h1&gt;&quot; % name, 200) 等价于 return &quot;&lt;h1&gt;Hello %s&lt;/h1&gt;&quot; % name, 200make_response的三个参数：返回字符串、状态码、HEAD 123456@app.route(\"/user/&lt;path:name&gt;\")def hello(name): print 'cookie:', request.cookies.get(\"my_cookie\") response = make_response(\"&lt;h1&gt;Hello %s&lt;/h1&gt;\" % name, 200) response.set_cookie('my_cookie', '100') return response OUTPUT1cookie: None OUTPUT2cookie: 100 2.3 Flask扩展Flask被设计为可扩展形式，开发者可以自由选择合适的程序包。下面以Flask-Sript给Flask提供命令行扩展为例。 pip install flask-script 安装 添加命令行解析功能 1234567891011from flask import Flask, make_response, requestfrom flask_script import Managerapp = Flask(__name__)manager = Manager(app)......if __name__ == '__main__': # app.run(debug=True) manager.run() 执行main函数看到命令行提示 12345678910/Users/lixiwei-mac/app/anaconda/envs/py27/bin/python2.7 /Users/lixiwei-mac/Documents/IdeaProjects/flask-learning/hello.pyusage: hello.py [-?] &#123;shell,runserver&#125; ...positional arguments: &#123;shell,runserver&#125; shell Runs a Python shell inside Flask application context. runserver Runs the Flask development server i.e. app.run()optional arguments: -?, --help show this help message and exit 在命令行执行 python hello.py runserver 启动flask应用(app.run()) 12345(py27) NikoBelic@bogon:~/Documents/IdeaProjects/flask-learning$ python hello.py runserver * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)before_requestcookie: 100127.0.0.1 - - [27/Aug/2017 19:01:45] &quot;GET /user/Niko HTTP/1.1&quot; 404 - runserver 的参数 123456789101112131415161718192021222324252627(py27) NikoBelic@bogon:~/Documents/IdeaProjects/flask-learning$ python hello.py runserver --helpusage: hello.py runserver [-?] [-h HOST] [-p PORT] [--threaded] [--processes PROCESSES] [--passthrough-errors] [-d] [-D] [-r] [-R]Runs the Flask development server i.e. app.run()optional arguments: -?, --help show this help message and exit -h HOST, --host HOST -p PORT, --port PORT --threaded --processes PROCESSES --passthrough-errors -d, --debug enable the Werkzeug debugger (DO NOT use in production code) -D, --no-debug disable the Werkzeug debugger -r, --reload monitor Python files for changes (not 100&#123;&apos;const&apos;: True, &apos;help&apos;: &apos;monitor Python files for changes (not 100% safe for production use)&apos;, &apos;option_strings&apos;: [&apos;-r&apos;, &apos;--reload&apos;], &apos;dest&apos;: &apos;use_reloader&apos;, &apos;required&apos;: False, &apos;nargs&apos;: 0, &apos;choices&apos;: None, &apos;default&apos;: None, &apos;prog&apos;: &apos;hello.py runserver&apos;, &apos;container&apos;: &lt;argparse._ArgumentGroup object at 0x1027bb510&gt;, &apos;type&apos;: None, &apos;metavar&apos;: None&#125;afe for production use) -R, --no-reload do not monitor Python files for changes 其中 –host 非常有用，可以指定当前web服务只可以被指定的IP来源进行访问。 –host 127.0.0.1 web服务只接受来自本机的请求 –host 0.0.0.0 web服务可以接收同局域网中的其他PC的请求 Chapter 3 模板Flask内置Jinjia模板引擎，用于渲染视图，将数据与展示分离。jinjia2官方文档jinjia2中文翻译文档 3.1 Jinjia2模板引擎渲染 默认情况下，flask会在templates文件夹中寻找模板 创建templates文件夹 创建user.html 12345678910&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Flask-Learning&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;Hello &#123;&#123;name&#125;&#125;&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 后台接口代码 1234@app.route(\"/user/&lt;path:name&gt;\")def hello(name): print 'cookie:', request.cookies.get(\"my_cookie\") return render_template('user.html', name=name) 请求该接口，返回渲染后的HTML内容 变量 jinjia能识别所有类型的变量，例如 字典、列表、对象等。 1234&lt;p&gt;a value from dictionary &#123;&#123;mydict['key']&#125;&#125;&lt;/p&gt;&lt;p&gt;a value from list &#123;&#123;mylist&#125;&#125;&lt;/p&gt;&lt;p&gt;a value from list with a variable index &#123;&#123;mylist[myindex]&#125;&#125;&lt;/p&gt;&lt;p&gt;a value from an object's method &#123;&#123;myobj.dosth() | upper&#125;&#125;&lt;/p&gt; 123456789101112131415@app.route(\"/\")def index(): mylist = [12, 3, 4, 5, 65] myindex = 2 mydict = &#123;'key': 'Fuck'&#125; class myObj(): def __init__(self): pass def dosth(self): return \"Doing Something.\" myobj = myObj() return render_template('index.html', mylist=mylist, myindex=myindex, mydict=mydict, myobj=myobj) 可以使用过滤器修改变量· safe 渲染时值不转义（不要在不可信的值上使用safe过滤器，例如用户表单锁输入的内容） capitalize 手写字母转换成大写，其他小写 lower 转小写 upper 转大写 title 每个单词首字母大写 trim 去除首尾空格 striptags 渲染之前把值中所有的HTML标签都删掉 控制结构 1234567891011121314151617181920212223242526272829&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Flask-Learning&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;!--If-Else条件判断--&gt;&lt;h1&gt; &#123;% if name%&#125; Hello &#123;&#123;name&#125;&#125; &#123;% else %&#125; Hello Stranger &#123;%endif%&#125;&lt;/h1&gt;&lt;!--宏定义，类似于python中的函数--&gt;&#123;% macro render_comment(comment)%&#125;&lt;li&gt;&#123;&#123;comment&#125;&#125;&lt;/li&gt;&#123;%endmacro%&#125;&lt;!--For循环遍历--&gt;&lt;ul&gt; &#123;% for comment in comments%&#125; &#123;&#123;render_comment(comment)&#125;&#125; &#123;% endfor%&#125;&lt;/ul&gt;&lt;/body&gt;&lt;/html&gt; 1234@app.route(\"/user/&lt;path:name&gt;\")def hello(name): comments = ['Hello', 'That is ridiculous'] return render_template('user.html', name=name,comments=comments) 导入模板 macros.html 1234&lt;!--宏定义，类似于python中的函数--&gt;&#123;% macro render_comment(comment)%&#125;&lt;li&gt;&#123;&#123;comment&#125;&#125;&lt;/li&gt;&#123;%endmacro%&#125; user.html 1234567&lt;!--导入模板--&gt;&#123;%import 'macros.html' as macros%&#125;&lt;ul&gt; &#123;% for comment in comments%&#125; &#123;&#123;render_comment(comment)&#125;&#125; &#123;% endfor%&#125;&lt;/ul&gt; 继承 base.html 1234567891011121314151617&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &#123;% block head%&#125; &lt;title&gt;&#123;%block title%&#125;&#123;%endblock%&#125; - My Application&lt;/title&gt; &#123;% endblock%&#125;&lt;/head&gt;&lt;body&gt;&#123;%block body%&#125;&#123;%endblock%&#125;&lt;p&gt;a value from dictionary &#123;&#123;mydict['key']&#125;&#125;&lt;/p&gt;&lt;p&gt;a value from list &#123;&#123;mylist&#125;&#125;&lt;/p&gt;&lt;p&gt;a value from list with a variable index &#123;&#123;mylist[myindex]&#125;&#125;&lt;/p&gt;&lt;p&gt;a value from an object's method &#123;&#123;myobj.dosth() | upper&#125;&#125;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; index.html 12345678910&#123;% extends 'base.html'%&#125;&#123;%block title %&#125; Index &#123;%endblock%&#125;&#123;%block head%&#125;&#123;&#123;super()&#125;&#125;&lt;style&gt;&lt;/style&gt;&#123;%endblock%&#125;&#123;%block body%&#125;&lt;h1&gt;This is home page&lt;/h1&gt;&#123;%endblock%&#125; 3.2 集成Flask-BootstrapBootstrap的基模板(bootstrap/base.html)提供了一个网页框架，引入了Bootstrap中所有的CSS和JavaScript文件。 12from flask_bootstrap import Bootstrapbootstrap = Bootstrap(app) base.html 123456789101112131415161718192021222324252627282930&#123;% extends \"bootstrap/base.html\" %&#125;&#123;% block title %&#125;Flasky&#123;% endblock %&#125;&#123;% block navbar %&#125;&lt;div class=\"navbar navbar-inverse\" role=\"navigation\"&gt; &lt;div class=\"container\"&gt; &lt;div class=\"navbar-header\"&gt; &lt;button type=\"button\" class=\"navbar-toggle\" data-toggle=\"collapse\" data-target=\".navbar-collapse\"&gt; &lt;span class=\"sr-only\"&gt;Toggle navigation&lt;/span&gt; &lt;span class=\"icon-bar\"&gt;&lt;/span&gt; &lt;span class=\"icon-bar\"&gt;&lt;/span&gt; &lt;span class=\"icon-bar\"&gt;&lt;/span&gt; &lt;/button&gt; &lt;a class=\"navbar-brand\" href=\"/\"&gt;Flasky&lt;/a&gt; &lt;/div&gt; &lt;div class=\"navbar-collapse collapse\"&gt; &lt;ul class=\"nav navbar-nav\"&gt; &lt;li&gt;&lt;a href=\"/\"&gt;Home&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&#123;% endblock %&#125;&#123;% block content %&#125;&lt;div class=\"container\"&gt; &#123;% block page_content %&#125;&#123;% endblock %&#125;&lt;/div&gt;&#123;% endblock %&#125; user.html 123456789&#123;% extends \"base.html\" %&#125;&#123;% block title %&#125;Flasky&#123;% endblock %&#125;&#123;% block page_content %&#125;&lt;div class=\"page-header\"&gt; &lt;h1&gt;Hello, &#123;&#123; name &#125;&#125;!&lt;/h1&gt;&lt;/div&gt;&#123;% endblock %&#125; 3.3 自定义错误页面12345678@app.errorhandler(404)def page_not_found(e): return render_template('404.html', e=e), 404@app.errorhandler(500)def internal_server_error(e): return render_template('500.html', e=e), 500 404.html 12345&#123;% extends 'base.html'%&#125;&#123;% block page_content%&#125;&lt;h3&gt;&#123;&#123;e&#125;&#125;&lt;/h3&gt;&#123;%endblock%&#125; 3.4 链接在模板中使用url_for来创建链接地址，而不是写死 base.html 1234567891011121314151617&#123;% block content %&#125;&lt;div class=\"container\"&gt; &#123;% block page_content %&#125;&#123;% endblock %&#125;&lt;/div&gt;&lt;div class=\"container\"&gt; &lt;h2&gt; &#123;% block url%&#125; &#123;&#123;url_for('index')&#125;&#125; &#123;% endblock %&#125; &lt;/h2&gt;&lt;/div&gt;&#123;% endblock %&#125; 注意上面有坑: {% block content%} 是页面自定义内容，它对应的{% endblock %} 一定要放到所有需要展示内容的最后，否则不会被页面读取到。 user.html 123&#123;% block url %&#125; &#123;&#123;url_for('user',_external=True,name=name)&#125;&#125;&#123;% endblock %&#125; _external表示使用相对/绝对路径，一般使用相对路径。 3.5 静态文件Jinjia默认情况下会到static文件夹中搜索静态文件&lt;img src=&quot;{{url_for('static',filename='head.jpeg')}}&quot;/&gt; Chapter 4 Web表单知识点： 跨站请求伪造保护（CSRF） 继承Form表单的用户自定义表单类 标准字段（XXXField） 验证函数 把表单渲染成HTML {{form.name()}} bootstrap的辅助函数 wtf.quick_form() 在视图函数中处理表单 form.name.data POST重定向GET模式（防止刷新浏览器出现重复踢脚表单提示） redirect的使用 url_for(‘fun_name’) 的参数fun_name不是路由地址而是试图函数的名称 Flash消息 试图函数中 使用flash(msg)创建消息 模板中使用get_flashed_messages()获取消息列表 12345678910111213141516171819202122232425262728app.config['SECRET_KEY'] = 'hard to guess string'class NameForm(Form): name = StringField('what is your name?', validators=[Required()]) submit = SubmitField('Submit') @app.route(\"/user/&lt;path:name&gt;\", methods=['GET', 'POST'])def hello(name): print 'cookie:', request.cookies.get(\"my_cookie\") response = make_response(\"&lt;h1&gt;Hello %s&lt;/h1&gt;\" % name, 404) response.set_cookie('my_cookie', '100') comments = ['Hello', 'That is ridiculous'] name = None form = NameForm() if form.validate_on_submit(): name = form.name.data if session.get('name') != name: flash('You have changed your name!') session['name'] = name form.name.data = '' return redirect(url_for('hello', name=name)) print 'name', name return render_template('user.html', name=session.get('name'), comments=comments, form=form) user.html 123456789101112131415&lt;div&gt; &#123;% import 'bootstrap/wtf.html' as wtf%&#125; &#123;% for msg in get_flashed_messages()%&#125; &lt;div class=\"alert alert-warning\"&gt; &lt;button type=\"button\" class=\"close\" data-dismiss=\"alter\"&gt; &amp;times;&lt;/button&gt; &#123;&#123;msg&#125;&#125; &lt;/div&gt; &#123;% endfor %&#125; &#123;&#123;wtf.quick_form(form)&#125;&#125; &lt;!-- &lt;form method=\"POST\"&gt; &#123;&#123;form.hidden_tag()&#125;&#125; &#123;&#123;form.name.label&#125;&#125; &#123;&#123;form.name()&#125;&#125; &#123;&#123;form.submit()&#125;&#125; &lt;/form&gt;--&gt;&lt;/div&gt; Chapter 5 数据库","tags":[{"name":"PythonWeb","slug":"PythonWeb","permalink":"https://seawaylee.github.io/tags/PythonWeb/"},{"name":"Flask","slug":"Flask","permalink":"https://seawaylee.github.io/tags/Flask/"}]},{"title":"C4-栈和队列","date":"2017-08-07T18:39:21.000Z","path":"2017/08/08/面试/算法/C4-栈和队列/","text":"栈和队列1 概念栈和队列的基本性质 栈：先进后出 队列：先进先出 栈和队列在实现结构上可以有数组和链表两种形式 数组结构容易实现 链表结构复杂，因为有很多指针操作 栈结构的基本操作 pop操作 top或peek操作 push操作 size操作 队列基本操作 与栈不同的是，push操作为在队头加入元素，而pop操作是从队列尾部弹出一个元素。 其他 栈和队列的基本操作，都是时间复杂度为O(1)的。 双端队列的首尾都可以亚茹和弹出元素 优先级队列根绝元素的优先级值，决定元素的弹出顺序 优先级队列的结构为堆结构，并不是线性结构 2 相关算法DFS 一棵二叉树的深度优先遍历，可以用栈实现，遍历的顺序其实就是节点入栈的顺序 BFS 一棵二叉树的广度优先遍历，可以用队列实现，遍历的顺序其实就是节点的入队列顺序 递归 平时使用的递归函数实际上用到了系统提供的函数栈 递归函数的处理过程，可以看做是函数进栈出栈的过程 所有用递归函数可以做的过程都一定可以用非递归的方式实现 3 例题例题1 例题2 解法 例题3 解法 例题4 解法 例题5 解法 方法1： 例题6 解法","tags":[{"name":"算法","slug":"算法","permalink":"https://seawaylee.github.io/tags/算法/"}]},{"title":"C3-字符串相关算法","date":"2017-08-06T18:50:19.000Z","path":"2017/08/07/面试/算法/C3-字符串相关算法/","text":"1 字符串面试题的特点 广泛性 字符串可以看做字符类型的数组，与数组排序、查找、调整有关 很多其他类型的面试题可以看做字符串类型的面试题 使用Java实现字符串类型的题目时，要掌握StringBuffer、StringBuilder、toCharArray方法。 需要掌握的概念 回文 子串（连续） 子序列（不连续） 前缀树（Trie树） 后缀树和后缀数组 匹配 字典序 需要掌握的操作 与数组有关的操作：增删改查 字符的替换 字符串的旋转 2 字符串题目的常见类型 规则判断 判断字符串是否符合整数规则 判断字符串是否符合浮点数规则 判断字符串时候符合回文字符串规则 数字运算 int和long类型表达整数范围有限，所以经常用字符串实现大整数。 与大整数相关的加减乘除操作，需要模拟笔算的过程。 与数组操作有关的类型 数组有关的调整、排序等操作需要掌握 快速排序的划分过程需要掌握和改写 字符计数 哈希表 固定长度的数组 C/C++(256长度),Java（65535长度） 滑动窗口问题、寻找无重复字符子串问题、计算变位词问题 动态规划 最长公共子串 最长公共子序列 最长回文子串 最长回文子序列 搜索类型 宽度优先搜索 深度优先搜索 高级算法与数据结构解决的问题 Manacher算法解决最长回文子串问题 KMP算法解决字符串匹配问题 前缀树结构 后缀树和后缀数组 通常面试中很少出现 3 例题例题1 普通解法 二叉树遍历 + 匹配问题 考察t1中以每个节点为头的子数是否与t2一致 假设t1树节点数为N，t2树节点数为M，最差时间复杂度为O(M*N) 最优解法 时间复杂度为O(M+N) t1序列化-&gt;字符串str1，t2序列化-&gt;字符串str2 使用KMP算法判断str1中是否含有str2 如果str1中包含str2，说明t1中有与t2相同的子树 否则说明没有 代码实现 1234567891011121314151617181920212223242526272829303132public boolean chkIdentical(TreeNode t1, TreeNode t2)&#123; StringBuilder sb1 = new StringBuilder(); serializeTree(t1, sb1); StringBuilder sb2 = new StringBuilder(); serializeTree(t2, sb2); if (sb1.toString().indexOf(sb2.toString()) != -1) &#123; return true; &#125; return false;&#125;public void serializeTree(TreeNode t, StringBuilder sb)&#123; if (t == null) &#123; sb.append(\"#!\"); return; &#125; sb.append(t.val).append(\"!\"); if (t.left != null) serializeTree(t.left, sb); else sb.append(\"#!\"); if (t.right != null) serializeTree(t.right, sb); else sb.append(\"#!\"); &#125; 例题2 普通方法 使用哈希表做字符统计 str1 -&gt; 字符统计 -&gt; hash1 str2 -&gt; 字符统计 -&gt; hash2 比对hash1与hash2的记录是否一致 其他方法 可以使用固定长度的数组来代理哈希表结构，时间复杂度为O(N)，额外空间复杂度为O(N) 代码实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 对于两个字符串A和B，如果A和B中出现的字符种类相同且每种字符出现的次数相同，则A和B互为变形词，请设计一个高效算法，检查两给定串是否互为变形词。 * &lt;p&gt; * 给定两个字符串A和B及他们的长度，请返回一个bool值，代表他们是否互为变形词。 * &lt;p&gt; * 测试样例： * \"abc\",3,\"bca\",3 * 返回：true * * @author NikoBelic * @create 2017/8/6 18:01 */public class ChkTransfrom&#123; public boolean chkTransform(String a, int lenA, String b, int lenB) &#123; if (a.equals(b)) &#123; return true; &#125; if (lenA != lenB) &#123; return false; &#125; Map&lt;Character, Integer&gt; hashA = new HashMap&lt;&gt;(); Map&lt;Character, Integer&gt; hashB = new HashMap&lt;&gt;(); for (char c : a.toCharArray()) &#123; hashA.put(c, hashA.get(c) == null ? 1 : hashA.get(c) + 1); &#125; for (char c : b.toCharArray()) &#123; hashB.put(c, hashB.get(c) == null ? 1 : hashB.get(c) + 1); &#125; for (Map.Entry&lt;Character, Integer&gt; entry : hashA.entrySet()) &#123; if (hashB.get(entry.getKey()) == entry.getValue()) continue; return false; &#125; return true; &#125; public static void main(String[] args) &#123; System.out.println(new ChkTransfrom().chkTransform(\"DAQT\", 4, \"QDAT\", 4)); &#125;&#125; 例题3 最优解时间复杂度为O(N) 判断s1与s2是否长度相等 不等直接返回false 如果长度相等，生成s1+s1的大字符串 使用KMP算法判断大字符串中是否包含s2 如果包含则说明s1与s2互为旋转词 例题4 实现将字符串局部所有字符逆序的函数f 第i个字符和第n-i个字符交换顺序即可实现 利用f将字符串所有字符逆序 找到逆序后的字符串中每一个单词的区域，利用f将每一个单词区域逆序 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 对于一个字符串，请设计一个算法，只在字符串的单词间做逆序调整，也就是说，字符串由一些由空格分隔的部分组成，你需要将这些部分逆序。 * &lt;p&gt; * 给定一个原字符串A和他的长度，请返回逆序后的字符串。 * &lt;p&gt; * 测试样例： * \"dog loves pig\",13 * 返回：\"pig loves dog\" * * @author NikoBelic * @create 2017/8/6 20:44 */public class ReverseSentence&#123; public String reverseSentence(String a, int n) &#123; String reversedStr = reverseChars(a.toCharArray()); String reversedWords = \"\"; for (String word : reversedStr.split(\" \")) &#123; reversedWords += reverseChars(word.toCharArray()); reversedWords += \" \"; &#125; if (reversedWords.length() &gt; 1) &#123; return reversedWords.substring(0, reversedWords.length() - 1); &#125; return reversedWords; &#125; private String reverseChars(char[] chars) &#123; char c; for (int i = 0; i &lt; chars.length / 2; i++) &#123; c = chars[i]; chars[i] = chars[chars.length - i - 1]; chars[chars.length - i - 1] = c; &#125; return String.valueOf(chars); &#125; public static void main(String[] args) &#123; ReverseSentence reverseSentence = new ReverseSentence(); String s = \"Niko Belic Is A Good Boy.\"; System.out.println(reverseSentence.reverseSentence(s, s.length())); &#125;&#125; 例题5 先将str[0,i]逆序 再将str[i+1,n-1]逆序 将整个str逆序 代码实现 12345678910111213141516171819202122232425262728public class StringTranslation&#123; public String stringTranslation(String a, int n, int len) &#123; String reversedStr = reverseStr(a.toCharArray()); return reverseStr(reversedStr.substring(0, n - len).toCharArray()) + reverseStr(reversedStr.substring(n - len).toCharArray()); &#125; private String reverseStr(char[] a) &#123; char c; for (int i = 0; i &lt; a.length / 2; i++) &#123; c = a[i]; a[i] = a[a.length - i - 1]; a[a.length - i - 1] = c; &#125; return String.valueOf(a); &#125; public static void main(String[] args) &#123; StringTranslation stringTranslation = new StringTranslation(); String s = \"ABCDE\"; System.out.println(stringTranslation.stringTranslation(s, s.length(), 3)); &#125;&#125; 例题6 最优时间复杂度O(N*logN)，其实质是一种排序的实现 代码实现 12345678910111213141516171819202122public String findSmallest(String[] strs, int n) &#123; String tmp; for (int i = 0; i &lt; n; i++) &#123; for (int j = i + 1; j &lt; n; j++) &#123; if ((strs[i] + strs[j]).compareTo(strs[j] + strs[i]) &gt; 0) &#123; tmp = strs[i]; strs[i] = strs[j]; strs[j] = tmp; &#125; &#125; &#125; StringBuilder sb = new StringBuilder(); for (String str : strs) &#123; sb.append(str); &#125; return sb.toString(); &#125; 例题7 代码实现 1234567891011121314151617181920212223242526272829303132public class Replacement&#123; public String repalceSpace(String iniString, int length) &#123; if (iniString == null || iniString.equals(\"\")) return \"\"; int spaceCount = iniString.split(\" \").length - 1; char[] initChars = iniString.toCharArray(); char[] resChars = new char[iniString.length() + spaceCount * 2]; int j = resChars.length - 1; for (int i = length - 1; i &gt;= 0; i--) &#123; if (initChars[i] != ' ') &#123; resChars[j--] = initChars[i]; &#125; else &#123; resChars[j--] = '0'; resChars[j--] = '2'; resChars[j--] = '%'; &#125; &#125; return String.valueOf(resChars); &#125; public static void main(String[] args) &#123; Replacement r = new Replacement(); String s = \"Niko Belic Is A Good Boy\"; System.out.println(r.repalceSpace(s, s.length())); &#125;&#125; 例题8 代码实现 123456789101112131415161718192021public boolean chkParenthesis(String a, int n)&#123; int num = 0; for (char c : a.toCharArray()) &#123; if (c == '(') &#123; num++; &#125; else &#123; num--; &#125; if (num &lt; 0) &#123; return false; &#125; &#125; if (num == 0) return true; return false;&#125; 例题9 解法 代码实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class DistinctSubString&#123; public int longestSubstring(String A, int n) &#123; char[] chas = A.toCharArray(); int len = -1; int[] map = new int[255]; int pre = -1; int cur = 0; Arrays.fill(map, -1); for (int i = 0; i &lt; n; i++) &#123; pre = Math.max(pre, map[chas[i]]); cur = i - pre; // ? len = Math.max(len, cur); map[chas[i]] = i; &#125; return len; &#125; public int l2(String A, int size) &#123; int i, j; char[] str = A.toCharArray(); int longest = 0; int[] visit = new int[255]; for (i = 0; i &lt; size; ++i) &#123; Arrays.fill(visit, 0); visit[str[i]] = 1; for (j = i + 1; j &lt; size; ++j) &#123; if (visit[str[j]] == 0) &#123; visit[str[j]] = 1; &#125; else &#123; if (j - i &gt; longest) &#123; longest = j - i; &#125; break; &#125; &#125; if ((j == size) &amp;&amp; (j - i &gt; longest)) &#123; longest = j - i; &#125; &#125; return longest; &#125; public static void main(String[] args) &#123; DistinctSubString d = new DistinctSubString(); //System.out.println(d.longestSubstring(\"ABCDA\", 5)); System.out.println(d.l2(\"spnvw\", 5)); &#125;&#125;","tags":[{"name":"算法","slug":"算法","permalink":"https://seawaylee.github.io/tags/算法/"}]},{"title":"C2-排序算法总结","date":"2017-08-01T17:07:35.000Z","path":"2017/08/02/面试/算法/C2-排序算法总结/","text":"1 对比分析图 均按从小到大排列 k代表数值中的”数位”个数 n代表数据规模 m代表数据的最大值减最小值 稳定性：稳定排序算法会让原本有相等键值的纪录维持相对次序。也就是如果一个排序算法是稳定的，当有两个相等键值的纪录R和S，且在原本的列表中R出现在S之前，在排序过的列表中R也将会是在S之前。 2 算法原理及实现2.1 冒泡排序概述 冒泡排序通过重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来，直到没有再需要交换的元素为止（对n个项目需要O(n^2)的比较次数）。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 实现步骤 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对每一对相邻元素做同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 性能 最差时间复杂度 O(n^2) 最优时间复杂度 O(n) 平均时间复杂度 O(n^2) 最差空间复杂度 总共O(n)，需要辅助空间O(1) 代码实现 1234567891011121314151617181920212223242526/** * 优化版冒泡排序 * 思路:每次冒泡都会将无序部分最大的数移动到无序部分的最后一个位置 * @author NikoBelic * @create 16/01/2017 17:54 */public static int[] sort(int[] array)&#123; boolean swap = false; for (int i = 0; i &lt; array.length - 1; i++) &#123; swap = false; for (int j = 0; j &lt; array.length - i - 1; j++) &#123; if (array[j] &gt; array[j + 1]) &#123; int t = array[j]; array[j] = array[j + 1]; array[j + 1] = t; swap = true; // 如果某一轮冒泡发现没有任何位置的交换,那么说明这个数组已经是有序的,无需再进行下一轮冒泡 &#125; &#125; if (!swap) break; &#125; return array; 2.2 简单选择排序概述 设所排序序列的记录个数为n，i 取 1,2,…,n-1 。从所有n-i+1个记录（Ri,Ri+1,…,Rn）中找出排序码最小（或最大）的记录，与第i个记录交换。执行n-1趟 后就完成了记录序列的排序。 以排序数组｛3，2，1，4，6，5｝为例 简单选择排序性能 在简单选择排序过程中，所需移动记录的次数比较少。最好情况下，即待排序记录初始状态就已经是正序排列了，则不需要移动记录。 最坏情况下，即待排序记录初始状态是按第一条记录最大，之后的记录从小到大顺序排列，则需要移动记录的次数最多为3（n-1）。 简单选择排序过程中需要进行的比较次数与初始状态下待排序的记录序列的排列情况无关。当i=1时，需进行n-1次比较；当i=2时，需进行n-2次比较；依次类推，共需要进行的比较次数是(n-1)+(n-2)+…+2+1=n(n-1)/2，即进行比较操作的时间复杂度为O(n^2)，进行移动操作的时间复杂度为O(n)。 简单选择排序是不稳定排序。 12345678910111213141516171819202122232425262728/** * 简单选择排序 * 思路:每次循环都将无序数组的第一个元素和无序数组中的最小元素互换位置 * @author NikoBelic * @create 17/01/2017 13:29 */public static int[] sort(int[] array)&#123; int index; int min; for (int i = 0; i &lt; array.length; i++) &#123; index = i; min = array[i]; // 找到最小值 for (int j = i; j &lt; array.length; j++) &#123; if (array[j] &lt; min) &#123; min = array[j]; index = j; &#125; &#125; int t = array[i]; array[i] = array[index]; array[index] = t; &#125; return array; 2.3 插入排序概述 将一个数据插入到已经排好序的有序数据中，从而得到一个新的、个数加一的有序数据，算法适用于少量数据的排序，是稳定的排序方法。 插入排序又分为 直接插入排序 和 折半插入排序。 直接插入排序 把待排序的纪录按其关键码值的大小逐个插入到一个已经排好序的有序序列中，直到所有的纪录插入完为止，得到一个新的有序序列。 代码实现 123456789101112131415161718192021222324252627/*** 普通插入排序 空间复杂度O(1) 时间复杂度O(n^2)* 思路:从第2个元素开始,与其左边的有序数组进行逐一比较,找到其需要正确插入的位置* 将该位置右边的数组向右移动一位,然后将当前元素插入进去** @Author NikoBelic* @Date 16/01/2017 09:36*/public static int[] sort(int[] array) &#123; int key; // Insert value int preIndex; // index that need to be move for (int i = 1; i &lt; array.length; i++) &#123; key = array[i]; // get the insert value preIndex = i - 1; // move elements which less than key while (preIndex &gt;= 0 &amp;&amp; key &lt; array[preIndex]) &#123; array[preIndex + 1] = array[preIndex]; preIndex--; &#125; // put the key in the correct position array[preIndex + 1] = key; &#125; return array; &#125; 效率分析 空间复杂度O(1) 平均时间复杂度O(n^2) 最差情况：反序，需要移动n*(n-1)/2个元素 ，运行时间为O(n^2)。 最好情况：正序，不需要移动元素，运行时间为O(n)． 折半插入排序 直接插入排序中要把插入元素与已有序序列元素依次进行比较，效率非常低。 折半插入排序,使用使用折半查找的方式寻找插入点的位置, 可以减少比较的次数,但移动的次数不变, 时间复杂度和空间复杂度和直接插入排序一样，在元素较多的情况下能提高查找性能。 代码实现 12345678910111213141516171819202122232425262728293031/** * 折半插入排序 空间、时间复杂度不变,较少了比较次数 * * @Author NikoBelic * @Date 16/01/2017 09:37 */public static int[] binarySort(int[] array)&#123; //从数组的第二个位置开始遍历值 for(int i = 1; i &lt; array.length; i++) &#123; int key = array[i]; //暂存要插入的值 int pre = 0; //有序序列开始和结尾下标申明 int last = i - 1; // 折半查找出插入位置 a[pre] while(pre &lt;= last) &#123; int mid = (pre + last) / 2; if(key &lt; array[mid]) &#123; last = mid - 1; &#125; else &#123; pre = mid + 1; &#125; &#125; //a[i]已经取出来存放在key中，把下标从pre + 1到 i-1的元素依次后移 for(int j = i; j &gt;= pre + 1; j--) &#123; array[j] = array[j - 1]; &#125; //把值插入空白位置 array[pre] = key; &#125; return array;&#125; 直接插入排序是，比较一个后移一个折半插入排序是，先找到位置，然后一起移动； 2.4 快速排序概述 快速排序（Quicksort）是对冒泡排序的一种改进，又称划分交换排序（partition-exchange sort。快速排序使用分治法（Divide and conquer）策略来把一个序列（list）分为两个子序列（sub-lists）。 步骤 从数列中挑出一个元素，称为”基准”（pivot） 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序 网上的通俗讲解 快速排序由于排序效率在同为O(N*logN)的几种排序方法中效率较高，因此经常被采用，再加上快速排序思想—-分治法也确实实用，因此很多软件公司的笔试面试，包括像腾讯，微软等知名IT公司都喜欢考这个，还有大大小的程序方面的考试如软考，考研中也常常出现快速排序的身影。 总的说来，要直接默写出快速排序还是有一定难度的，因为本人就自己的理解对快速排序作了下白话解释，希望对大家理解有帮助，达到快速排序，快速搞定。 快速排序是C.R.A.Hoare于1962年提出的一种划分交换排序。它采用了一种分治的策略，通常称其为分治法(Divide-and-ConquerMethod)。 该方法的基本思想是： 先从数列中取出一个数作为基准数。 分区过程，将比这个数大的数全放到它的右边，小于或等于它的数全放到它的左边。 再对左右区间重复第二步，直到各区间只有一个数。 虽然快速排序称为分治法，但分治法这三个字显然无法很好的概括快速排序的全部步骤。因此我的对快速排序作了进一步的说明：挖坑填数+分治法： 先来看实例吧，定义下面再给出（最好能用自己的话来总结定义，这样对实现代码会有帮助）。 以一个数组作为示例，取区间第一个数为基准数。 初始时，i = 0; j = 9; X = a[i] = 72 由于已经将a[0]中的数保存到X中，可以理解成在数组a[0]上挖了个坑，可以将其它数据填充到这来。 从j开始向前找一个比X小或等于X的数。当j=8，符合条件，将a[8]挖出再填到上一个坑a[0]中。a[0]=a[8]; i++; 这样一个坑a[0]就被搞定了，但又形成了一个新坑a[8]，这怎么办了？简单，再找数字来填a[8]这个坑。这次从i开始向后找一个大于X的数，当i=3，符合条件，将a[3]挖出再填到上一个坑中a[8]=a[3]; j–; 数组变为： i = 3; j = 7; X=72再重复上面的步骤，先从后向前找，再从前向后找。从j开始向前找，当j=5，符合条件，将a[5]挖出填到上一个坑中，a[3] = a[5]; i++;从i开始向后找，当i=5时，由于i==j退出。此时，i = j = 5，而a[5]刚好又是上次挖的坑，因此将X填入a[5]。 数组变为： 可以看出a[5]前面的数字都小于它，a[5]后面的数字都大于它。因此再对a[0…4]和a[6…9]这二个子区间重复上述步骤就可以了。 对挖坑填数进行总结 i =L; j = R; 将基准数挖出形成第一个坑a[i]。 j–由后向前找比它小的数，找到后挖出此数填前一个坑a[i]中。 i++由前向后找比它大的数，找到后也挖出此数填到前一个坑a[j]中。 再重复执行2，3二步，直到i==j，将基准数填入a[i]中。 实现 1234567891011121314151617181920212223242526272829public static void quickSort(int[] a, int left, int right) &#123; if (left &lt; right) &#123; int i = left, j = right; // 以中间数为基准 //SortUtil.swap(a, left, (left + right) / 2); // 以随机数为基准 SortUtil.swap(a, left, SortUtil.getRandomIndex(left, right)); int x = a[left]; // a[left] 就是被挖出的第一个坑 while (i &lt; j) &#123; // 从右向左找小于x的数来填a[i] while (i &lt; j &amp;&amp; a[j] &gt; x) j--; if (i &lt; j) a[i++] = a[j]; // 将a[j]填到a[i]，a[j]形成了新的坑 // 从左向右找大于x的数来填a[j] while (i &lt; j &amp;&amp; a[i] &lt; x) i++; if (i &lt; j) a[j--] = a[i];// 将[i]天到a[j],a[i]形成了新的坑 &#125; a[i] = x;// 退出时，i等于j，将x填到这个坑 quickSort(a, left, i - 1); quickSort(a, i + 1, right); &#125; &#125; 3 桶排序 时间复杂度O(n),思想都是来自于桶排序，桶排序不是一种具体的排序算法，而是一种思想。 3.1 计数排序 分别创建100-300号的桶，将待排序数按对应的桶号存储，全部存储完毕后，从100号-300号桶逐一倒出数值，即完成排序。 3.2 基数排序 将待排序数列分别按照，个位、十位、百位 添加到不同的桶中并倒出，三次装入与倒出后的数列即为有序数列。 4 排序笔试面试题 已知一个几乎有序的数组，几乎有序是值如果把数组排好顺序的话，每个元素移动的距离不超过k，并且k相对于数组长度来说很小。请问选择什么方法对其排序比较好？ 冒泡、选择都不好，因为他们是严格的O( n^2 )算法 插入排序还不错，其排序过程与原始顺序有关，时间复杂度不会超过O(N*K) 快排、归并不好 答案是改进后的堆排序 由于元素移动距离不会超过k，i=0,所以建立a[i,k-1]的小根堆，将最小值换到a[i],i++ 构建a[i,k+i]的小根堆，将当前堆最小值换到a[i],i++ 堆顶的弹出顺序其实就是最终排序结果 每得到一个数的时间复杂度为O(logK),总共N个数，时间复杂度就是O(N*logK) 判断数组中是否有重复值。必须保证额外的空间复杂度为O(1) 如果没有空间复杂度的限制，用哈希表实现最好 有空间复杂度要求，需要先排序，然后判断（遍历一次） 答案是改出一个非递归版本的堆排序，堆排序递归空间复杂度是O(logN) 把两个有序数组合并为一个数组，第一个数组空间正好可以容纳两个数组的元素。 从后往前遍历数组A和B 荷兰国旗问题。只包含0，2，3的整数数组进行排序，要求使用交换、原地排序，而不是利用计数进行排序。 与快排的划分过程类似 如果当前数为1，那么就与0区的后一个数进行交换，并且0区向后扩充1 如果当前数是2，那么当前数与2区前一个数进行交换，并且2区向前扩充1 注意2区域的前一个数是我们没有遍历过的数，所以index不能加1 给定一个2维数组，在2维数组中，每一行每一列都是有序的，给定一个数k，判断2维数组中是否包含这个数。 如果二维数组规模为 m*n，那么最优时间复杂度为m+n 从2维数组右上角开始找 如果当前数大于k，舍弃当前列，向左移动 如果当前数小于k，舍弃行，向下移动 在查找过程中如果找到该数则返回true，若最终角标都越界了还没找到，则返回false 给定一个数组，求解数组中需要排序的最短子数组的长度。比如[1,5,4,3,2,6,7]，返回4，因为只有[5,4,3,2]需要排序 最优解：时间复杂度O(n),空间复杂度O(1) 首先从左向右遍历数组，过程中用max来标记过程中数值的最大值，并记录max&gt;curr_val 最后出现的to_index 从右往左遍历数组，过程中用min来标记过程中数值最小的值，并记录min&lt;curr_val最后出现的位置from_index [from_index,to_index]就是需要排序的最短子数组 给定一个无序数组a，返回如果排序之后，相邻两数的最大差值。 最优解O(n),空间O(n) 思想来自于桶排序 遍历数组，找到min和max，将[min,max]分为 n 个等量区间 同一个桶中的数差不会超过当前桶区间，所以不用考虑。 考虑每一个桶的最小值 和 上一个桶中的最大值，记录最大差值，即为答案","tags":[{"name":"算法","slug":"算法","permalink":"https://seawaylee.github.io/tags/算法/"}]},{"title":"Hive学习笔记（一）- 详解Hive","date":"2017-07-02T15:35:33.000Z","path":"2017/07/02/大数据/Hive/Hive学习笔记（一）- 详解Hive/","text":"1 Hive基础知识1.1 简介什么是HiveHive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。 为什么使用Hive 直接使用Hadoop所面临的问题 学习成本高 一般项目周期要求短 MapReduce实现复杂的查询呢逻辑开发难度较大 为什么要使用Hive 操作接口采用类SQL语法，可以快速开发 避免编写MR，减少学习成本 扩展功能很方便 Hive的特点 可扩展： Hive可以自由的扩展集群规模，一般情况下不需要重启服务 延展性： Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数 容错： 良好的容错性，节点出现问题SQL仍然可以完成执行 1.2 架构 JobTracker是Hadoop1.x中的ResouceManager + AppMasterTaskTracker相当于NodeManager + YarnChild 1.2.1 基本组成 用户接口：CLI、JDBC/ODBC、WebGUI CLI是SHELL命令行 JDBC、ODBC是Hive的Java实现，与传统的JDBC类似 WebGUI是通过浏览器访问Hive 元数据存储：Mysql、Derby等 Hive将元数据存储在数据库中。Hive中的元数据包括表名、列、分区及其属性、表的属性（是否为外部表等）、表数据所在目录等。 解释器、编译器、优化器、执行器 完成HQL查询语句 词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS中，并在随后由MapReduce调用执行。 1.3 Hive与Hadoop的关系Hive利用HDFS存储数据，利用MapReduce查询数据 1.4 Hive与传统数据库对比Hive具有SQL数据库的外表，但应用应用场景完全不同，Hive只适合用来做批量的数据统计分析 Hive RDBMS 查询语言 HQL SQL 数据存储 HDFS LOCAL FS 执行 MapReduce Executor 执行延迟 高 低 处理数据规模 大 小 索引 位图索引 复杂索引 1.5 Hive的数据存储 Hive中所有的数据都存储在HDFS中，没有专门的数据存储格式（可支持Text，SequenceFile，ParquetFile，RCFILE等） 只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符，Hive就可以解析数据。 Hive中包含以下数据模型：DB、Table、External Table、Partition、Bucket DB：在HDFS中表现为 ${hive.metastore.warehouse.dir} 目录下的文件夹 Table：在HDFS中表现所属DB目录下的文件夹 External Table：外部表，与Table类似，不过其数据存放的位置可以在任意指定路径。 Table表：删除表后，HDFS上的文件都删除了 External外部表：删除表后，HDFS上的文件没有删除，只是把表删除了 Partition：在HDFS中表现为Table目录下的子目录 Bucket：桶，在HDFS中表现为同一个表目录下根据Hash散列之后的多个文件，会根据不同的文件把数据放到不同的文件夹中 1.6 Hive的安装的部署1.6.1 准备工作 安装mysql 下载、解压Hive 修改hive-site.xml，配置mysql 12345678910111213141516171819202122232425&lt;configuration&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;&lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;&lt;value&gt;MYSQL的密码&lt;/value&gt;&lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt; 1.6.2 使用Hive Hive的Shell交互模式 bin/hive 启动hive （与mysql操作方式一样） Hive Thrift服务 在安装有hive的主机上后台启动 nohup ./hiveserver2 1&gt; hiverserver.log 2&gt;&amp;1 &amp; 在任意子hadoop子节点上用beeline连接hiveserver 方式1 ./beeline !connect jdbc:hive2://hadoop1:10000 NikoBelic 方式2 ./beeline -u jdbc:hive2://hadoop1:10000 -n NikoBelic 使用sql语句疯狂输出吧 2 Hive基本操作2.1 DDL操作2.1.1 创建表建表语法 123456789CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path] 说明： CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。 EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。 PARTITIONED 表示根据某一个key(不在create table里面)对数据进行分区，体现在HDFS上就是 table目录下有n个不同的分区文件夹(country=China,country=USA) ROW FORMAT DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]| SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)]用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive通过 SerDe 确定表的具体的列的数据。 STORED ASSEQUENCEFILE|TEXTFILE|RCFILE如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。 CLUSTERED BY对于每一个表（table）或者分区， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。把表（或者分区）组织成桶（Bucket）有两个理由： （1）获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。 （2）使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。 实例 123456789show databases;use hive_test_db;create table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string)clustered by(Sno) sorted by(Sno DESC)into 4 bucketsrow format delimitedfields terminated by ','; 设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数set hive.enforce.bucketing = true;set mapreduce.job.reduces=4; 2.1.2 修改表 增加/删除分区 12345ALTER TABLE table_name ADD [IF NOT EXISTS] partition_spec [ LOCATION 'location1' ] partition_spec [ LOCATION 'location2' ] ...partition_spec:: PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)ALTER TABLE table_name DROP partition_spec, partition_spec,... 实例 1alter table student_p add partition(part='a') partition(part='b'); 重命名表 1ALTER TABLE table_name RENAME TO new_table_name 增加/更新列 1ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。 1ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] 2.1.3 显示命令123456show tablesshow databasesshow partitionsshow functionsdesc extended t_name;desc formatted table_name; 2.2 DML操作2.2.1 Load12LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] 1load data local inpath '/Users/lixiwei-mac/app/data/hive_tmp/metrics.data' into table metrics; Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。 filepath： 相对路径，例如：project/data1 绝对路径，例如：/user/hive/project/data1 包含模式的完整 URI，列如： hdfs://namenode:9000/user/hive/project/data1 LOCAL关键字 如果指定了 LOCAL， load 命令会去查找本地文件系统中的 filepath。 如果没有指定 LOCAL 关键字，则根据inpath中的uri查找文件 OVERWRITE 关键字 如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。 如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。 2.2.2 Insert1INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement Multiple inserts: 123FROM from_statement INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 [INSERT OVERWRITE TABLE tablename2 [PARTITION ...] select_statement2] ... Dynamic partition inserts: 1INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement 1insert into table metrics_buck select * from metrics distribute by (type); 导出表数据 1INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ... 2.2.3 Select12345678SELECT [ALL | DISTINCT] select_expr, select_expr, ... FROM table_reference[WHERE where_condition] [GROUP BY col_list [HAVING condition]] [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list] ] [LIMIT number] 注： order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。 sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。 distribute by根据distribute by指定的内容将数据分到同一个reducer。 Cluster by 除了具有Distribute by的功能外，还会对该字段进行排序。因此，常常认为cluster by = distribute by + sort by 2.3 Hive Join1234join_table: table_reference JOIN table_factor [join_condition] | table_reference &#123;LEFT|RIGHT|FULL&#125; [OUTER] JOIN table_reference join_condition | table_reference LEFT SEMI JOIN table_reference join_condition Hive 支持等值连接（equality joins）、外连接（outer joins）和（left/right joins）。Hive 不支持非等值的连接，因为非等值连接非常难转化到 map/reduce 任务。另外，Hive 支持多于 2 个表的连接。 写 join 查询时，需要注意几个关键点： 1、只支持等值join例如： 123SELECT a.* FROM a JOIN b ON (a.id = b.id) SELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department) 是正确的，然而: 1SELECT a.* FROM a JOIN b ON (a.id&gt;b.id) 是错误的。 2、可以 join 多于 2 个表。例如 12SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2) 如果join中多个表的 join key 是同一个，则 join 会被转化为单个 map/reduce 任务，例如： 123SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1) 被转化为单个 map/reduce 任务，因为 join 中只使用了 b.key1 作为 join key。 12SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2) 而这一 join 被转化为 2 个 map/reduce 任务。因为 b.key1 用于第一次 join 条件，而 b.key2 用于第二次 join。 3、join 时，每次 map/reduce 任务的逻辑： reducer 会缓存 join 序列中除了最后一个表的所有表的记录，再通过最后一个表将结果序列化到文件系统。这一实现有助于在 reduce 端减少内存的使用量。实践中，应该把最大的那个表写在最后（否则会因为缓存浪费大量内存）。例如： 12SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1) 所有表都使用同一个 join key（使用 1 次 map/reduce 任务计算）。Reduce 端会缓存 a 表和 b 表的记录，然后每次取得一个 c 表的记录就计算一次 join 结果，类似的还有： 12SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2) 这里用了 2 次 map/reduce 任务。第一次缓存 a 表，用 b 表序列化；第二次缓存第一次 map/reduce 任务的结果，然后用 c 表序列化。 4、LEFT，RIGHT 和 FULL OUTER 关键字用于处理 join 中空记录的情况例如： 12SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key) 对应所有 a 表中的记录都有一条记录输出。输出的结果应该是 a.val, b.val，当 a.key=b.key 时，而当 b.key 中找不到等值的 a.key 记录时也会输出: a.val, NULL 所以 a 表中的所有记录都被保留了； “a RIGHT OUTER JOIN b”会保留所有 b 表的记录。 Join 发生在 WHERE 子句之前。如果你想限制 join 的输出，应该在 WHERE 子句中写过滤条件——或是在 join 子句中写。这里面一个容易混淆的问题是表分区的情况： 123SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key) WHERE a.ds='2009-07-07' AND b.ds='2009-07-07' 会 join a 表到 b 表（OUTER JOIN），列出 a.val 和 b.val 的记录。WHERE 从句中可以使用其他列作为过滤条件。但是，如前所述，如果 b 表中找不到对应 a 表的记录，b 表的所有列都会列出 NULL，包括 ds 列。也就是说，join 会过滤 b 表中不能找到匹配 a 表 join key 的所有记录。这样的话，LEFT OUTER 就使得查询结果与 WHERE 子句无关了。解决的办法是在 OUTER JOIN 时使用以下语法： 1234SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key AND b.ds='2009-07-07' AND a.ds='2009-07-07') 这一查询的结果是预先在 join 阶段过滤过的，所以不会存在上述问题。这一逻辑也可以应用于 RIGHT 和 FULL 类型的 join 中。 Join 是不能交换位置的。无论是 LEFT 还是 RIGHT join，都是左连接的。 1234SELECT a.val1, a.val2, b.val, c.val FROM a JOIN b ON (a.key = b.key) LEFT OUTER JOIN c ON (a.key = c.key) 先 join a 表到 b 表，丢弃掉所有 join key 中不匹配的记录，然后用这一中间结果和 c 表做 join。这一表述有一个不太明显的问题，就是当一个 key 在 a 表和 c 表都存在，但是 b 表中不存在的时候：整个记录在第一次 join，即 a JOIN b 的时候都被丢掉了（包括a.val1，a.val2和a.key），然后我们再和 c 表 join 的时候，如果 c.key 与 a.key 或 b.key 相等，就会得到这样的结果：NULL, NULL, NULL, c.val 2.4 HQL小结123show databases;show tables;desc test; 2.4.1 分桶表示例 创建分桶表 12345678drop table stu_buck;create table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string)clustered by(Sno) sorted by(Sno DESC)into 4 bucketsrow format delimitedfields terminated by ','; 设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数 set hive.enforce.bucketing = true; set mapreduce.job.reduces=4; insert overwrite table student_buck select * from student cluster by(Sno) sort by(Sage); 报错,cluster 和 sort 不能共存 往创建的分通表插入数据(插入数据需要是已分桶, 且排序的) 可以使用distribute by(sno) sort by(sno asc) 或是排序和分桶的字段相同的时候使用Cluster by(字段) 注意使用cluster by 就等同于分桶+排序(sort) 12345678insert into table stu_buckselect Sno,Sname,Sex,Sage,Sdept from student distribute by(Sno) sort by(Sno asc);insert overwrite table stu_buckselect * from student distribute by(Sno) sort by(Sno asc);insert overwrite table stu_buckselect * from student cluster by(Sno); 2.4.2 保存select查询结果的几种方式： 将查询结果保存到一张新的hive表中 123create table t_tmpasselect * from t_p; 将查询结果保存到一张已经存在的hive表中 12insert into table t_tmpselect * from t_p; 将查询结果保存到指定的文件目录（可以是本地，也可以是hdfs） 12345insert overwrite local directory '/home/hadoop/test'select * from t_p;insert overwrite directory '/aaa/test'select * from t_p; 2.4.3 关于hive中的各种join 准备数据 1,a 2,b 3,c 4,d 7,y 8,u 2,bb 3,cc 7,yy 9,pp 建表： 12345create table a(id int,name string)row format delimited fields terminated by ',';create table b(id int,name string)row format delimited fields terminated by ','; 导入数据 12load data local inpath '/home/hadoop/a.txt' into table a;load data local inpath '/home/hadoop/b.txt' into table b; 实验 inner join 12345678select * from a inner join b on a.id=b.id;+-------+---------+-------+---------+--+| a.id | a.name | b.id | b.name |+-------+---------+-------+---------+--+| 2 | b | 2 | bb || 3 | c | 3 | cc || 7 | y | 7 | yy |+-------+---------+-------+---------+--+ left join 1234567891011select * from a left join b on a.id=b.id; +-------+---------+-------+---------+--+ | a.id | a.name | b.id | b.name | +-------+---------+-------+---------+--+ | 1 | a | NULL | NULL | | 2 | b | 2 | bb | | 3 | c | 3 | cc | | 4 | d | NULL | NULL | | 7 | y | 7 | yy | | 8 | u | NULL | NULL | +-------+---------+-------+---------+--+ right join select * from a right join b on a.id=b.id; outer join 123456789101112select * from a full outer join b on a.id=b.id;+-------+---------+-------+---------+--+| a.id | a.name | b.id | b.name |+-------+---------+-------+---------+--+| 1 | a | NULL | NULL || 2 | b | 2 | bb || 3 | c | 3 | cc || 4 | d | NULL | NULL || 7 | y | 7 | yy || 8 | u | NULL | NULL || NULL | NULL | 9 | pp |+-------+---------+-------+---------+--+ left semi join 12345678select * from a left semi join b on a.id = b.id;+-------+---------+--+| a.id | a.name |+-------+---------+--+| 2 | b || 3 | c || 7 | y |+-------+---------+--+ 2.4.4 其他 多重插入： 12345from studentinsert into table student_p partition(part='a')select * where Sno&lt;95011;insert into table student_p partition(part='a')select * where Sno&lt;95011; 导出数据到本地 12insert overwrite local directory '/home/hadoop/student.txt'select * from student; UDF案例 123456789create table rat_json(line string) row format delimited;load data local inpath '/home/hadoop/rating.json' into table rat_json;drop table if exists t_rating;create table t_rating(movieid string,rate int,timestring string,uid string)row format delimited fields terminated by '\\t';insert overwrite table t_ratingselect split(parsejson(line),'\\t')[0]as movieid,split(parsejson(line),'\\t')[1] as rate,split(parsejson(line),'\\t')[2] as timestring,split(parsejson(line),'\\t')[3] as uid from rat_json limit 10; 内置jason函数 1select get_json_object(line,'$.movie') as moive,get_json_object(line,'$.rate') as rate from rat_json limit 10; transform案例 先加载rating.json文件到hive的一个原始表 rat_json 12create table rat_json(line string) row format delimited;load data local inpath '/home/hadoop/rating.json' into table rat_json; 需要解析json数据成四个字段，插入一张新的表 t_rating 12insert overwrite table t_ratingselect get_json_object(line,'$.movie') as moive,get_json_object(line,'$.rate') as rate from rat_json; 使用transform+python的方式去转换unixtime为weekday 先编辑一个python脚本文件 vi weekday_mapper.py 12345678910#!/bin/pythonimport sysimport datetimefor line in sys.stdin: line = line.strip() movieid, rating, unixtime,userid = line.split('\\t') weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday() print '\\t'.join([movieid, rating, str(weekday),userid]) 保存文件 然后，将文件加入hive的classpath： hive&gt;add FILE /home/hadoop/weekday_mapper.py; hive&gt;create TABLE u_data_new as 1234567SELECT TRANSFORM (movieid, rate, timestring,uid) USING 'python weekday_mapper.py' AS (movieid, rate, weekday,uid)FROM t_rating;select distinct(weekday) from u_data_new limit 10; 3 Hive Shell参数3.1 Hive命令行语法结构 hive [-hiveconf x=y] [&lt;-i filename&gt;] [&lt;-f filename&gt;|&lt;-e query-string&gt;] [-S] 说明： -i 从文件初始化HQL。-e从命令行执行指定的HQL-f 执行HQL脚本-v 输出执行的HQL语句到控制台-p connect to Hive Server on port number-hiveconf x=y Use this to set hive/hadoop configuration variables. 3.2 Hive参数配置方式3.2.1 Hive参数大全https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties 开发Hive应用时，不可避免地需要设定Hive的参数。设定Hive的参数可以调优HQL代码的执行效率，或帮助定位问题。然而实践中经常遇到的一个问题是，为什么设定的参数没有起作用？这通常是错误的设定方式导致的。 对于一般参数，有以下三种设定方式： 配置文件 命令行参数 参数声明 3.2.2 配置文件Hive的配置文件包括 用户自定义配置文件：$HIVE_CONF_DIR/hive-site.xml 默认配置文件：$HIVE_CONF_DIR/hive-default.xml 用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。 3.2.3 命令行参数启动Hive（客户端或Server方式）时，可以在命令行添加-hiveconf param=value来设定参数，例如：bin/hive -hiveconf hive.root.logger=INFO,console这一设定对本次启动的Session（对于Server方式启动，则是所有请求的Sessions）有效。 3.2.4 参数声明可以在HQL中使用SET关键字设定参数，例如： 1set mapred.reduce.tasks=100; 这一设定的作用域也是session级的。 上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在Session建立以前已经完成了。 4 Hive函数创建测试表 vim dual.data 只写一个空格 创建表 use school; create table dual(id int); load data local inpath &#39;/Users/lixiwei-mac/app/data/hive_tmp/dual.data&#39; into table dual; 导入数据 测试 select substr(&#39;NikoBelic&#39;,0,4) from dual; 12345+-------+--+| _c0 |+-------+--+| Niko |+-------+--+ 4.1 内置运算符内容较多，见《Hive官方文档》 4.2 内置函数内容较多，见《Hive官方文档》 4.3 Hive自定义函数和Transform当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。 4.3.1 自定义函数类别 UDF 作用于单个数据行，产生一个数据行作为输出。（数学函数，字符串函数） UDAF（用户定义聚集函数）：接收多个输入数据行，并产生一个输出数据行。（count，max） 4.3.2 UDF开发实例 先开发一个java类，继承UDF，并重载evaluate方法 12345678910package cn.itcast.bigdata.udfimport org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.Text;public final class Lower extends UDF&#123; public Text evaluate(final Text s)&#123; if(s==null)&#123;return null;&#125; return new Text(s.toString().toLowerCase()); &#125;&#125; 打成jar包上传到服务器 将jar包添加到hive的classpath hive&gt;add JAR /home/hadoop/udf.jar; 创建临时函数与开发好的java class关联 即可在hql中使用自定义的函数strip Select strip(name),age from t_test; 创建临时函数与开发好的java class关联 Hive&gt;create temporary function toprovince as &#39;cn.itcast.bigdata.udf.ToProvince&#39;; 即可在hql中使用自定义的函数strip Select strip(name),age from t_test; 4.3.3 Transform实现 Hive的 TRANSFORM 关键字提供了在SQL中调用自写脚本的功能 适合实现Hive中没有的功能又不想写UDF的情况 12345678910111213141516CREATE TABLE u_data_new ( movieid INT, rating INT, weekday INT, userid INT)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\\t';add FILE weekday_mapper.py;INSERT OVERWRITE TABLE u_data_newSELECT TRANSFORM (movieid, rating, unixtime,userid) USING 'python weekday_mapper.py' AS (movieid, rating, weekday,userid)FROM u_data; 使用示例1：下面这句sql就是借用了weekday_mapper.py对数据进行了处理. 123456789#!/bin/pythonimport sysimport datetimefor line in sys.stdin: line = line.strip() movieid, rating, unixtime,userid = line.split('\\t') weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday() print '\\t'.join([movieid, rating, str(weekday),userid]) 其中weekday_mapper.py内容如下 123456789#!/bin/pythonimport sysimport datetimefor line in sys.stdin: line = line.strip() movieid, rating, unixtime,userid = line.split('\\t') weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday() print '\\t'.join([movieid, rating, str(weekday),userid]) 使用示例2：下面的例子则是使用了shell的cat命令来处理数据 1FROM invites a INSERT OVERWRITE TABLE events SELECT TRANSFORM(a.foo, a.bar) AS (oof, rab) USING '/bin/cat' WHERE a.ds &gt; '2008-08-09'; 5 遇到的问题问题 UDF 载入Jar成功 但添加function时ClassNotFound。 如果确定create function xx as ‘ClassPackageName’ 中的ClassPackageName没有输入错误，那么可能是导出的Jar出现了问题。 Idea编辑器下，导出jar时不要选择 from moudles 而要选择empty,并创建META-INF，不要引入依赖jar包 退出Hive服务重新连接并导入即可 创建UDF方法出现异常 Unsupported major.minor version 52.0 Hadoop的JDK版本和jar的导出版本不一致，52.0是jdk8的版本号 修改Hadoop的hadoop-env.sh，将jdk7的路径修改为jdk8即可","tags":[{"name":"大数据","slug":"大数据","permalink":"https://seawaylee.github.io/tags/大数据/"},{"name":"Hive","slug":"Hive","permalink":"https://seawaylee.github.io/tags/Hive/"}]},{"title":"Maven_SSH_Oracle项目搭建全流程","date":"2017-06-27T15:02:44.000Z","path":"2017/06/27/JavaWeb/Maven_SSH_Oracle项目搭建全流程/","text":"[TOC] 系统搭建github: https://github.com/seawaylee/maven-ssh-quickstart 1 SSH框架搭建1.1 SVN创建项目1234567cd /svnsudo svnadmin create nwrd2017 # 创建下面sudo chown -R www-data:www-data /svn # 修改权限cd /svn/authsudo vim dav_svn.authz # 为项目配置用户及权限sudo htpasswd dav_svn.passwd $&#123;username&#125; # 为新用户设置密码sudo service apache2 restart # 重启项目 访问 https://${SVN_IP}/svn/${project_name} 查看项目结构 1.2 SSH框架搭建 - Maven项目结构 1.2.1 创建MavenWeb项目 1.2.2 引入SSH相关jar包123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.edu.ncut&lt;/groupId&gt; &lt;artifactId&gt;NWRDServer&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;NWRDServer&lt;/name&gt; &lt;url&gt;http://localhost:8080/NWRDServer&lt;/url&gt; &lt;properties&gt; &lt;spring.version&gt;4.3.9.RELEASE&lt;/spring.version&gt; &lt;hibernate.version&gt;5.1.0.Final&lt;/hibernate.version&gt; &lt;ojdbc.version&gt;11.1.0.7.0&lt;/ojdbc.version&gt; &lt;!--下面这两个是springAOP需要用到--&gt; &lt;aspectjweaver.version&gt;1.7.2&lt;/aspectjweaver.version&gt; &lt;persistence-api.version&gt;2.1&lt;/persistence-api.version&gt; &lt;junit.version&gt;4.12&lt;/junit.version&gt; &lt;druid.version&gt;1.0.4&lt;/druid.version&gt; &lt;jedis.version&gt;2.9.0&lt;/jedis.version&gt; &lt;slf4j.version&gt;1.7.7&lt;/slf4j.version&gt; &lt;log4j.version&gt;1.2.17&lt;/log4j.version&gt; &lt;javaee-api.version&gt;7.0&lt;/javaee-api.version&gt; &lt;jstl.version&gt;1.2&lt;/jstl.version&gt; &lt;jsp-api.version&gt;2.0&lt;/jsp-api.version&gt; &lt;servlet-api.version&gt;2.4&lt;/servlet-api.version&gt; &lt;commons-fileupload.version&gt;1.3.1&lt;/commons-fileupload.version&gt; &lt;commons-io.version&gt;2.4&lt;/commons-io.version&gt; &lt;commons-lang3.version&gt;3.3.2&lt;/commons-lang3.version&gt; &lt;commons-email.version&gt;1.3.2&lt;/commons-email.version&gt; &lt;commons-beanutils.version&gt;1.9.2&lt;/commons-beanutils.version&gt; &lt;commons-pool2.version&gt;2.4.2&lt;/commons-pool2.version&gt; &lt;httpclient.version&gt;4.3.3&lt;/httpclient.version&gt; &lt;jackson-databind.version&gt;2.8.7&lt;/jackson-databind.version&gt; &lt;fastjson.version&gt;1.1.43&lt;/fastjson.version&gt; &lt;!--架包版本变量 end--&gt; &lt;!--插件版本变量 start--&gt; &lt;tomcat7-maven-plugin.version&gt;2.2&lt;/tomcat7-maven-plugin.version&gt; &lt;maven-compiler-plugin.version&gt;3.1&lt;/maven-compiler-plugin.version&gt; &lt;maven-war-plugin.version&gt;2.3&lt;/maven-war-plugin.version&gt; &lt;maven-resources-plugin.version&gt;2.6&lt;/maven-resources-plugin.version&gt; &lt;maven-install-plugin.version&gt;2.4&lt;/maven-install-plugin.version&gt; &lt;maven-clean-plugin.version&gt;2.5&lt;/maven-clean-plugin.version&gt; &lt;maven-antrun-plugin.version&gt;1.7&lt;/maven-antrun-plugin.version&gt; &lt;maven-dependency-plugin.version&gt;2.5.1&lt;/maven-dependency-plugin.version&gt; &lt;maven-source-plugin.version&gt;2.2.1&lt;/maven-source-plugin.version&gt; &lt;!--插件版本变量 end--&gt; &lt;!--其他变量 start--&gt; &lt;war-name.version&gt;NWRDServer&lt;/war-name.version&gt; &lt;jdk.version&gt;1.7&lt;/jdk.version&gt; &lt;tomcat-port.version&gt;8080&lt;/tomcat-port.version&gt; &lt;tomcat-uri-encoding.version&gt;UTF-8&lt;/tomcat-uri-encoding.version&gt; &lt;tomcat-path.version&gt;/&lt;/tomcat-path.version&gt; &lt;!--其他变量 end--&gt; &lt;!--这个配置是为了解决下面两个警告--&gt; &lt;!--Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!--&gt; &lt;!--File encoding has not been set, using platform encoding UTF-8, i.e. build is platform dependent!--&gt; &lt;!--指定Maven用什么编码来读取源码及文档--&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;!--指定Maven用什么编码来呈现站点的HTML文件--&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!--Spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-messaging&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-oxm&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--这个jar 文件包含Spring MVC 框架相关的所有类。包括框架的Servlets，Web MVC框架，控制器和视图支持。当然，如果你的应用使用了独立的MVC 框架，则无需这个JAR 文件里的任何类。 外部依赖spring-web--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--这个jar 文件包含支持UI模版（Velocity，FreeMarker，JasperReports），邮件服务，脚本服务(JRuby)，缓存Cache（EHCache），任务计划Scheduling（uartz）方面的类。 外部依赖spring-context--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;$&#123;aspectjweaver.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.persistence&lt;/groupId&gt; &lt;artifactId&gt;persistence-api&lt;/artifactId&gt; &lt;version&gt;$&#123;persistence-api.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Hibernate --&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-core&lt;/artifactId&gt; &lt;version&gt;$&#123;hibernate.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-ehcache&lt;/artifactId&gt; &lt;version&gt;$&#123;hibernate.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 二级缓存ehcache --&gt; &lt;dependency&gt; &lt;groupId&gt;net.sf.ehcache&lt;/groupId&gt; &lt;artifactId&gt;ehcache&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;!--这个组件具体可以看这里介绍:http://www.oschina.net/p/druid--&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;$&#123;druid.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- JSTL标签类 --&gt; &lt;dependency&gt; &lt;groupId&gt;jstl&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;$&#123;jstl.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;$&#123;servlet-api.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;$&#123;jsp-api.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- java ee jar 包 --&gt; &lt;dependency&gt; &lt;groupId&gt;javax&lt;/groupId&gt; &lt;artifactId&gt;javaee-api&lt;/artifactId&gt; &lt;version&gt;$&#123;javaee-api.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!--单元测试--&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;$&#123;junit.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- 日志文件管理包 start --&gt; &lt;!--下面这三个是配套使用：http://blog.csdn.net/woshiwxw765/article/details/7624556--&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 日志文件管理包 end --&gt; &lt;!--JSON处理--&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;$&#123;jackson-databind.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 上传组件包 --&gt; &lt;dependency&gt; &lt;groupId&gt;commons-fileupload&lt;/groupId&gt; &lt;artifactId&gt;commons-fileupload&lt;/artifactId&gt; &lt;version&gt;$&#123;commons-fileupload.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;$&#123;commons-io.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--apache工具包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;$&#123;commons-lang3.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;$&#123;commons-pool2.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;oracle&lt;/groupId&gt; &lt;artifactId&gt;ojdbc6&lt;/artifactId&gt; &lt;version&gt;$&#123;ojdbc.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;NWRDServer&lt;/finalName&gt; &lt;resources&gt; &lt;!--表示把java目录下的有关xml文件,properties文件编译/打包的时候放在resource目录下--&gt; &lt;resource&gt; &lt;directory&gt;$&#123;basedir&#125;/src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;$&#123;basedir&#125;/src/main/resources&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;plugins&gt; &lt;!-- Compiler 插件, 设定JDK版本 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;maven-compiler-plugin.version&#125;&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;$&#123;jdk.version&#125;&lt;/source&gt; &lt;target&gt;$&#123;jdk.version&#125;&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;showWarnings&gt;true&lt;/showWarnings&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 注意各jar包之间的依赖、版本号 oracle驱动由于授权问题，无法从maven仓库直接导入，可以先从官网下载ojdbc6，然后手动存储到maven-repository OJDBC6下载地址 手动安装mvn install:install-file -DgroupId=com.oracle -DartifactId=ojdbc14 -Dversion=11.1.0.7.0 -Dpackaging=jar -Dfile=ojdbc.jar拷贝到mvn-repositorycp ~/app/repository/oracle/11.1.0.7.0/ojdbc14-11.1.0.7.0.jar 1.2.3 创建相关配置文件web.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;web-app xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://java.sun.com/xml/ns/javaee\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd\" version=\"3.0\"&gt; &lt;display-name&gt;NWRDServer&lt;/display-name&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;!-- Spring和Hibernate的配置文件 --&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath*:spring/applicationContext*.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;!--配置JVM环境变量来解决log4j文件输出位置不正确问题--&gt; &lt;context-param&gt; &lt;param-name&gt;webAppRootKey&lt;/param-name&gt; &lt;param-value&gt;webApp.root&lt;/param-value&gt; &lt;/context-param&gt; &lt;context-param&gt; &lt;param-name&gt;log4jConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:log4j.properties&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.util.Log4jConfigListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- Spring监听器 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- 防止Spring内存溢出监听器 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.util.IntrospectorCleanupListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- 配置SESSION超时，单位是分钟 --&gt; &lt;session-config&gt; &lt;session-timeout&gt;15&lt;/session-timeout&gt; &lt;/session-config&gt; &lt;!-- ############################################ filter start ############################################ --&gt; &lt;!-- 编码过滤器 --&gt; &lt;filter&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;async-supported&gt;true&lt;/async-supported&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;!-- druid 数据源，用于采集 web-jdbc 关联监控的数据 --&gt; &lt;!-- 具体参考官网：https://github.com/alibaba/druid/wiki/%E9%85%8D%E7%BD%AE_%E9%85%8D%E7%BD%AEWebStatFilter--&gt; &lt;filter&gt; &lt;filter-name&gt;DruidWebStatFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.alibaba.druid.support.http.WebStatFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;exclusions&lt;/param-name&gt; &lt;param-value&gt;*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/*&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;profileEnable&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;DruidWebStatFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;!-- ############################################ filter end ############################################ --&gt; &lt;!-- ############################################ servlet start ############################################ --&gt; &lt;!-- Spring MVC servlet --&gt; &lt;servlet&gt; &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring/spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;async-supported&gt;true&lt;/async-supported&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt; &lt;!-- 此处可以可以配置成 *.do ，对应struts的后缀习惯 --&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;/index.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;!--展示Druid的统计信息--&gt; &lt;!--具体可以看官网信息：https://github.com/alibaba/druid/wiki/%E9%85%8D%E7%BD%AE_StatViewServlet%E9%85%8D%E7%BD%AE--&gt; &lt;servlet&gt; &lt;servlet-name&gt;DruidStatView&lt;/servlet-name&gt; &lt;servlet-class&gt;com.alibaba.druid.support.http.StatViewServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;DruidStatView&lt;/servlet-name&gt; &lt;!--访问路径eg：http://localhost:8080/druid/index.html --&gt; &lt;url-pattern&gt;/druid/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;!-- ############################################ servlet end ############################################ --&gt;&lt;/web-app&gt; applicationContext.xml 123456789101112131415161718192021222324252627282930&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-4.3.xsdhttp://www.springframework.org/schema/contexthttp://www.springframework.org/schema/context/spring-context-4.3.xsd\"&gt; &lt;!--上面的xsd最好和当前使用的Spring版本号一致,如果换了Spring版本,这个最好也跟着改--&gt; &lt;!-- 引入属性文件 放在最开头 ,在使用spring之前就引入,里面的变量才能被引用--&gt; &lt;context:property-placeholder location=\"classpath*:properties/*.properties\"/&gt; &lt;!-- 引入属性文件也可以用这种写法 &lt;bean id=\"propertyConfigurer\" class=\"org.springframework.beans.factory.config.PropertyPlaceholderConfigurer\"&gt; &lt;property name=\"location\" value=\"classpath:config.properties\" /&gt; &lt;/bean&gt; --&gt; &lt;!-- 自动扫描(需要自动注入的类，对于那些类上有注解：@Repository、@Service、@Controller、@Component都进行注入) --&gt; &lt;!--因为 Spring MVC 是 Spring 的子容器，所以我们在 Spring MVC 的配置中再指定扫描 Controller 的注解，这里是父容器的配置地方--&gt; &lt;!--注解注入的相关材料可以看：--&gt; &lt;!--http://blog.csdn.net/u012763117/article/details/17253849--&gt; &lt;!--http://casheen.iteye.com/blog/295348--&gt; &lt;!--http://blog.csdn.net/zhang854429783/article/details/6785574--&gt; &lt;context:component-scan base-package=\"cn.edu.ncut.nwrd.dao,cn.edu.ncut.nwrd.service,cn.edu.ncut.nwrd.common,cn.edu.ncut.nwrd.test\"/&gt;&lt;/beans&gt; applicationContext-dataSource.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102&lt;beans xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.springframework.org/schema/beans\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;!-- 引入properties文件 --&gt; &lt;context:property-placeholder location=\"classpath*:/properties/config.properties\"/&gt; &lt;!-- 引入jdbc配置文件 --&gt; &lt;bean id=\"propertyConfigurer\" class=\"org.springframework.beans.factory.config.PropertyPlaceholderConfigurer\"&gt; &lt;property name=\"locations\"&gt; &lt;list&gt; &lt;value&gt;classpath*:/properties/config.properties&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 持久层异常转换为Sping类型异常--&gt; &lt;bean id=\"persistenceExceptionTranslationPostProcessor\" class=\"org.springframework.dao.annotation.PersistenceExceptionTranslationPostProcessor\"&gt;&lt;/bean&gt; &lt;!-- 使用阿里的druid配置数据源 start--&gt; &lt;!--具体查看官网信息：https://github.com/alibaba/druid/wiki/%E9%85%8D%E7%BD%AE_DruidDataSource%E5%8F%82%E8%80%83%E9%85%8D%E7%BD%AE--&gt; &lt;bean id=\"dataSource\" class=\"com.alibaba.druid.pool.DruidDataSource\" init-method=\"init\" destroy-method=\"close\"&gt; &lt;!--这三个变量读取config.properties的--&gt; &lt;property name=\"driverClassName\" value=\"oracle.jdbc.driver.OracleDriver\"/&gt; &lt;property name=\"url\" value=\"jdbc:oracle:thin:@$&#123;jdbc.host&#125;:$&#123;jdbc.port&#125;:$&#123;jdbc.database&#125;\"/&gt; &lt;property name=\"username\" value=\"$&#123;jdbc.user&#125;\"/&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\"/&gt; &lt;!-- 初始化连接大小 --&gt; &lt;property name=\"initialSize\" value=\"1\"/&gt; &lt;!-- 初始化连接池最大使用连接数量 --&gt; &lt;property name=\"maxActive\" value=\"20\"/&gt; &lt;!-- 初始化连接池最小空闲 --&gt; &lt;property name=\"minIdle\" value=\"1\"/&gt; &lt;!-- 获取连接最大等待时间，单位毫秒--&gt; &lt;property name=\"maxWait\" value=\"60000\"/&gt; &lt;!-- 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒 --&gt; &lt;property name=\"timeBetweenEvictionRunsMillis\" value=\"60000\"/&gt; &lt;!-- 配置一个连接在池中最小生存的时间，单位是毫秒 --&gt; &lt;property name=\"minEvictableIdleTimeMillis\" value=\"25200000\"/&gt; &lt;!-- 打开PSCache，并且指定每个连接上PSCache的大小 --&gt; &lt;!--如果用Oracle，则把poolPreparedStatements配置为true，mysql可以配置为false。分库分表较多的数据库，建议配置为false。--&gt; &lt;property name=\"poolPreparedStatements\" value=\"false\"/&gt; &lt;property name=\"maxPoolPreparedStatementPerConnectionSize\" value=\"20\"/&gt; &lt;property name=\"validationQuery\" value=\"$&#123;validation_query&#125;\"/&gt; &lt;property name=\"testWhileIdle\" value=\"true\"/&gt; &lt;property name=\"testOnBorrow\" value=\"false\"/&gt; &lt;property name=\"testOnReturn\" value=\"false\"/&gt; &lt;!--当程序存在缺陷时，申请的连接忘记关闭，这时候，就存在连接泄漏了。Druid提供了RemoveAbandanded相关配置，用来关闭长时间不使用的连接--&gt; &lt;!--配置removeAbandoned对性能会有一些影响，建议怀疑存在泄漏之后再打开。在上面的配置中，如果连接超过30分钟未关闭，就会被强行回收，并且日志记录连接申请时的调用堆栈。--&gt; &lt;!--具体查看官网信息：https://github.com/alibaba/druid/wiki/%E8%BF%9E%E6%8E%A5%E6%B3%84%E6%BC%8F%E7%9B%91%E6%B5%8B--&gt; &lt;!-- 打开removeAbandoned功能 --&gt; &lt;property name=\"removeAbandoned\" value=\"true\"/&gt; &lt;!-- 1800秒，也就是30分钟 --&gt; &lt;property name=\"removeAbandonedTimeout\" value=\"1800\"/&gt; &lt;!-- 关闭abanded连接时输出错误日志 --&gt; &lt;property name=\"logAbandoned\" value=\"true\"/&gt; &lt;!-- 配置监控统计拦截的filters--&gt; &lt;!--官网信息：https://github.com/alibaba/druid/wiki/%E9%85%8D%E7%BD%AE_StatFilter--&gt; &lt;!--mergeSql可以合并输出的sql，方便查看，但是在mybatis框架中使用这个则无法监控sql,需要用stat--&gt; &lt;!--&lt;property name=\"filters\" value=\"mergeSql,log4j\"/&gt;--&gt; &lt;!--&lt;property name=\"filters\" value=\"mergeSql,wall\"/&gt;--&gt; &lt;!--&lt;property name=\"filters\" value=\"stat\"/&gt;--&gt; &lt;!--&lt;property name=\"filters\" value=\"mergeSql\"/&gt;--&gt; &lt;property name=\"filters\" value=\"stat,log4j\"/&gt;&lt;/bean&gt; &lt;!-- 使用阿里的druid配置数据源 end--&gt; &lt;bean id=\"sessionFactory\" class=\"org.springframework.orm.hibernate5.LocalSessionFactoryBean\"&gt; &lt;property name=\"dataSource\"&gt; &lt;ref bean=\"dataSource\"/&gt; &lt;/property&gt; &lt;property name=\"hibernateProperties\"&gt; &lt;props&gt; &lt;prop key=\"hibernate.dialect\"&gt;$&#123;jdbc.hibernate.dialect&#125;&lt;/prop&gt; &lt;prop key=\"hibernate.show_sql\"&gt;true&lt;/prop&gt; &lt;prop key=\"javax.persistence.validation.mode\"&gt;none&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;property name=\"packagesToScan\" value=\"cn.edu.ncut.nwrd.**.model.**\"/&gt; &lt;/bean&gt; &lt;bean id=\"txManager\" class=\"org.springframework.orm.hibernate5.HibernateTransactionManager\"&gt; &lt;property name=\"sessionFactory\" ref=\"sessionFactory\"/&gt; &lt;/bean&gt; &lt;!--使用基于注解方式配置事务 --&gt; &lt;tx:annotation-driven transaction-manager=\"txManager\"&gt;&lt;/tx:annotation-driven&gt;&lt;/beans&gt; applicationContext-transaction.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xsi:schemaLocation=\"http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-4.3.xsdhttp://www.springframework.org/schema/txhttp://www.springframework.org/schema/tx/spring-tx-4.3.xsdhttp://www.springframework.org/schema/aophttp://www.springframework.org/schema/aop/spring-aop-4.3.xsd\"&gt; &lt;!--上面的xsd最好和当前使用的Spring版本号一致,如果换了Spring版本,这个最好也跟着改--&gt; &lt;!-- Druid 和 Spring 关联监控配置 start--&gt; &lt;!-- 具体可以查阅官网：https://github.com/alibaba/druid/wiki/%E9%85%8D%E7%BD%AE_Druid%E5%92%8CSpring%E5%85%B3%E8%81%94%E7%9B%91%E6%8E%A7%E9%85%8D%E7%BD%AE--&gt; &lt;bean id=\"druid-stat-interceptor\" class=\"com.alibaba.druid.support.spring.stat.DruidStatInterceptor\" /&gt; &lt;bean id=\"druid-stat-pointcut\" class=\"org.springframework.aop.support.JdkRegexpMethodPointcut\" scope=\"prototype\"&gt; &lt;property name=\"patterns\"&gt; &lt;list&gt; &lt;value&gt;cn.edu.ncut.service.*&lt;/value&gt; &lt;!--如果使用的是 hibernate 则这里也要扫描路径，但是 mybatis 不需要--&gt; &lt;!--&lt;value&gt;com.youmeek.ssm.module.*.dao.*&lt;/value&gt;--&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;aop:config proxy-target-class=\"true\"&gt; &lt;!-- pointcut-ref=\"druid-stat-pointcut\" 这个报红没事--&gt; &lt;aop:advisor advice-ref=\"druid-stat-interceptor\" pointcut-ref=\"druid-stat-pointcut\" /&gt; &lt;/aop:config&gt; &lt;!-- Druid 和 Spring 关联监控配置 end--&gt; &lt;!-- (事务管理器)transaction manager, use JtaTransactionManager for global tx --&gt; &lt;!--http://www.mybatis.org/spring/zh/transactions.html--&gt; &lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt; &lt;/bean&gt; &lt;!-- 一种方式:注解方式配置事物,扫描@Transactional注解的类定义事务，配置上service实现类(下面还有一个方法名拦截方式,两个同时配置也是可以使用的，但是不建议两者一起使用) --&gt; &lt;!--&lt;tx:annotation-driven transaction-manager=\"transactionManager\" proxy-target-class=\"true\"/&gt;--&gt; &lt;!-- 一种方式:拦截器方式配置事物start 配置了该方式之后,在方法里面使用注解方式配置事务也是没有作用的 --&gt; &lt;tx:advice id=\"transactionAdvice\" transaction-manager=\"transactionManager\"&gt; &lt;tx:attributes&gt; &lt;!--以这些单词开头的方法自动加入事务--&gt; &lt;!--更多参数和使用方法可以参考:--&gt; &lt;!--http://wuhenjia.blog.163.com/blog/static/93469449201123010594395--&gt; &lt;!--http://baobao707.iteye.com/blog/415446--&gt; &lt;!--http://jinnianshilongnian.iteye.com/blog/1442376--&gt; &lt;!--http://winters1224.blog.51cto.com/3021203/807500--&gt; &lt;!--如果是方法中直接抛顶层Exception异常的话,propagation=\"REQUIRED\"是不顶用的,还需要配置rollback-for属性--&gt; &lt;!--&lt;tx:method name=\"delete*\" propagation=\"REQUIRED\" read-only=\"false\" rollback-for=\"java.lang.Exception\" no-rollback-for=\"java.lang.RuntimeException\"/&gt;--&gt; &lt;!--&lt;tx:method name=\"insert*\" propagation=\"REQUIRED\" read-only=\"false\" rollback-for=\"java.lang.RuntimeException\" /&gt;--&gt; &lt;!--&lt;tx:method name=\"update*\" propagation=\"REQUIRED\" read-only=\"false\" rollback-for=\"java.lang.Exception\" /&gt; --&gt; &lt;tx:method name=\"add*\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"save*\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"insert*\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"register*\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"update*\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"modify*\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"edit*\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"batch*\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"delete*\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"remove*\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"time*\" propagation=\"REQUIRED\"/&gt;&lt;!--定时器方法--&gt; &lt;tx:method name=\"repair\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"deleteAndRepair\" propagation=\"REQUIRED\"/&gt; &lt;!--以这些单词开头的方法不加入事务--&gt; &lt;tx:method name=\"get*\" propagation=\"SUPPORTS\" read-only=\"true\"/&gt; &lt;tx:method name=\"find*\" propagation=\"SUPPORTS\" read-only=\"true\"/&gt; &lt;tx:method name=\"select*\" propagation=\"SUPPORTS\" read-only=\"true\"/&gt; &lt;tx:method name=\"load*\" propagation=\"SUPPORTS\" read-only=\"true\"/&gt; &lt;tx:method name=\"search*\" propagation=\"SUPPORTS\" read-only=\"true\"/&gt; &lt;tx:method name=\"datagrid*\" propagation=\"SUPPORTS\" read-only=\"true\"/&gt; &lt;tx:method name=\"*\" propagation=\"SUPPORTS\"/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;aop:config&gt; &lt;!--把这个拦截器配置到com.youmeek.ssh.service下(包括子包)下的以impl目录下类的,任意方法--&gt; &lt;!-- execution的语法表示:在impl包中定义的任意方法的执行，更多方式可以参考： http://lavasoft.blog.51cto.com/62575/172292/ http://blog.csdn.net/partner4java/article/details/7015946 --&gt; &lt;aop:pointcut id=\"transactionPointcut\" expression=\"execution(* cn.edu.ncut.service.impl.*.*(..) )\"/&gt; &lt;!--&lt;aop:pointcut id=\"transactionPointcut\" expression=\"execution(* *..*Service*.*(..))\"/&gt;--&gt; &lt;aop:advisor pointcut-ref=\"transactionPointcut\" advice-ref=\"transactionAdvice\"/&gt; &lt;/aop:config&gt; &lt;!--一种方式:拦截器方式配置事物end--&gt;&lt;/beans&gt; spring-mvc.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:mvc=\"http://www.springframework.org/schema/mvc\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd\"&gt; &lt;!-- 自动扫描该包，使SpringMVC认为包下用了@controller注解的类是控制器 --&gt; &lt;context:component-scan base-package=\"cn.edu.ncut.nwrd.controller\"/&gt; &lt;!-- 配置注解驱动 --&gt; &lt;mvc:annotation-driven/&gt; &lt;!--静态资源映射--&gt; &lt;!-- http://perfy315.iteye.com/blog/2008763 mapping=\"/js/**\" 表示当浏览器有静态资源请求的时候，并且请求url路径中带有：/js/，则这个资源跑到webapp目录下的/WEB-INF/statics/js/去找 比如我们在 JSP 中引入一个 js 文件：src=\"$&#123;webRoot&#125;/js/jQuery-core/jquery-1.6.1.min.js --&gt; &lt;!--&lt;mvc:resources mapping=\"/css/**\" location=\"/WEB-INF/statics/css/\"/&gt;--&gt; &lt;!--&lt;mvc:resources mapping=\"/js/**\" location=\"/WEB-INF/statics/js/\"/&gt;--&gt; &lt;!--&lt;mvc:resources mapping=\"/images/**\" location=\"/WEB-INF/statics/images/\"/&gt;--&gt; &lt;!-- 对模型视图名称的解析，即在模型视图名称添加前后缀(如果最后一个还是表示文件夹,则最后的斜杠不要漏了) 使用JSP--&gt; &lt;!-- 默认的视图解析器 在上边的解析错误时使用 (默认使用html)- --&gt; &lt;bean id=\"defaultViewResolver\" class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"viewClass\" value=\"org.springframework.web.servlet.view.JstlView\"/&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/view/\"/&gt;&lt;!--设置JSP文件的目录位置--&gt; &lt;property name=\"suffix\" value=\".jsp\"/&gt; &lt;/bean&gt; &lt;!-- 文件上传 start 配置文件上传，如果没有使用文件上传可以不用配置，当然如果不配，那么配置文件中也不必引入上传组件包 --&gt; &lt;bean id=\"multipartResolver\" class=\"org.springframework.web.multipart.commons.CommonsMultipartResolver\"&gt; &lt;!-- 默认编码 --&gt; &lt;property name=\"defaultEncoding\" value=\"UTF-8\"/&gt; &lt;!-- 文件大小最大值 上传文件大小限制为10M，10*1024*1024 --&gt; &lt;property name=\"maxUploadSize\" value=\"10485760\"/&gt; &lt;!-- 内存中的最大值 --&gt; &lt;property name=\"maxInMemorySize\" value=\"4096\"/&gt; &lt;/bean&gt; &lt;!--文件上传 end--&gt;&lt;/beans&gt; log4j.properties 1234567891011121314151617181920#本属性文件只能放在 resources 根目录下#同时使用两种记录,一种控制台,一种文件方式（文件大小到达指定尺寸的时候产生一个新的文件）#log4j.rootLogger=trace,appenderNameConsole,appenderNameRollingFile#只输出到控制台，不输出到文件，级别：all &gt; trace &gt; debug &gt; info &gt; warn &gt; error#log4j.rootLogger=info,appenderNameConsole,appenderNameRollingFilelog4j.rootLogger=info,appenderNameConsole#控制台输出log4j.appender.appenderNameConsole=org.apache.log4j.ConsoleAppenderlog4j.appender.appenderNameConsole.Target=System.outlog4j.appender.appenderNameConsole.layout=org.apache.log4j.PatternLayoutlog4j.appender.appenderNameConsole.layout.ConversionPattern=[%d&#123;yyyy-MM-dd HH:mm:ss&#125;] --- [%p] --- [%F:%L] --- [%m] --- %n#这个用来输出mybatis执行sql语句.其中 com.youmeek.ssm.manage.mapper 表示mapper.xml中的namespace,这里只是前缀表示所有这个前缀下的都输出,也可以写完整namespace.#======================================================#输出日志到硬盘，文件大小到达指定尺寸的时候产生一个新的文件#log4j.appender.appenderNameRollingFile=org.apache.log4j.RollingFileAppender#log4j.appender.appenderNameRollingFile.File=$&#123;webApp.root&#125;/WEB-INF/logs/dmes.log#log4j.appender.appenderNameRollingFile.MaxFileSize=50MB#log4j.appender.appenderNameRollingFile.Threshold=ALL#log4j.appender.appenderNameRollingFile.layout=org.apache.log4j.PatternLayout#log4j.appender.appenderNameRollingFile.layout.ConversionPattern=[%d&#123;yyyy-MM-dd HH:mm:ss&#125;] --- [%p] --- [%F:%L] --- [%m] --- %n config.properties 1234567jdbc.host=马赛克jdbc.port=马赛克jdbc.database=马赛克jdbc.user=马赛克jdbc.password=马赛克jdbc.hibernate.dialect=org.hibernate.dialect.Oracle10gDialectvalidation_query=SELECT 1 FROM DUAL 1.3 创建数据库表123456 CREATE TABLE \"NWRD\".\"TB_USER\" ( \"ID\" NUMBER(10,0) NOT NULL ENABLE, \"NAME\" VARCHAR2(200 BYTE), \"BIRTHDAY\" DATE, \"CREATE_TIME\" TIMESTAMP (6) 1.4 创建MVC模型1.4.1 使用Idea的Hibernate逆向工程 右键项目 Add Frameworks Support 勾选Hibernate ProjectStructure -&gt; Facets -&gt; 添加HibernateConfiguration View -&gt; Tool Windows -&gt; Persistence 生成Model 注意绿色部位 需要手动修改为 java.util.Date Idea的数据库客户端插件值得推荐使用 1.4.2 Model修改Model的表名、添加主键序列绑定、添加toStirng方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109package cn.edu.ncut.nwrd.model;import cn.edu.ncut.nwrd.model.base.BaseModel;import javax.persistence.*;import java.sql.Timestamp;import java.util.Date;/** * @author NikoBelic * @create 2017/6/27 18:16 */@Entity@Table(name = \"TB_USER\", schema = \"NWRD\", catalog = \"\")public class UserObj extends BaseModel&#123; private Integer id; private String name; private Date birthday; private Timestamp createTime; @Id @Column(name = \"ID\") @SequenceGenerator(name = \"TB_USER_PK_GENERATOR\", sequenceName = \"SEQ_USER\",allocationSize=1) @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = \"TB_USER_PK_GENERATOR\") public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; @Basic @Column(name = \"NAME\") public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Basic @Column(name = \"BIRTHDAY\") public Date getBirthday() &#123; return birthday; &#125; public void setBirthday(Date birthday) &#123; this.birthday = birthday; &#125; @Basic @Column(name = \"CREATE_TIME\") public Timestamp getCreateTime() &#123; return createTime; &#125; public void setCreateTime(Timestamp createTime) &#123; this.createTime = createTime; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; UserObj userObj = (UserObj) o; if (id != null ? !id.equals(userObj.id) : userObj.id != null) return false; if (name != null ? !name.equals(userObj.name) : userObj.name != null) return false; if (birthday != null ? !birthday.equals(userObj.birthday) : userObj.birthday != null) return false; if (createTime != null ? !createTime.equals(userObj.createTime) : userObj.createTime != null) return false; return true; &#125; @Override public int hashCode() &#123; int result = id != null ? id.hashCode() : 0; result = 31 * result + (name != null ? name.hashCode() : 0); result = 31 * result + (birthday != null ? birthday.hashCode() : 0); result = 31 * result + (createTime != null ? createTime.hashCode() : 0); return result; &#125; @Override public String toString() &#123; return \"UserObj&#123;\" + \"id=\" + id + \", name='\" + name + '\\'' + \", birthday=\" + birthday + \", createTime=\" + createTime + '&#125;'; &#125;&#125; BaseModel 12345678910111213141516171819202122package cn.edu.ncut.nwrd.model.base;import org.apache.log4j.Logger;import java.io.Serializable;public abstract class BaseModel implements Serializable&#123; private static final long serialVersionUID = 2387253001746505718L; private final static Logger logger = Logger.getLogger(BaseModel.class); // date formats protected static final String DATE_FORMAT = \"yyyy-MM-dd\"; protected static final String TIME_FORMAT = \"HH:mm:ss\"; protected static final String DATE_TIME_FORMAT = \"yyyy-MM-dd'T'HH:mm:ss\"; protected static final String TIMESTAMP_FORMAT = \"yyyy-MM-dd HH:mm:ss.S\";&#125; 1.4.3 DaoBaseDao 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148package cn.edu.ncut.nwrd.common.basedao;import java.io.Serializable;import java.util.LinkedHashMap;import java.util.List;import java.util.Map;public interface Dao&lt;T extends Serializable&gt; &#123; T find(Serializable id); void save(T entity); void update(T entity); void merge(T entity); /** * 更新时首先进行查询，实体应包含主键，实验于不把字段改为NULL的更新 * * @param entity */ void updateWithQuery(T entity); void delete(Serializable id); long getCount(); long getCount(String where, Map&lt;String, Object&gt; params); /** * 根据条件查询，获得所有记录 */ List&lt;T&gt; findAll(); /** * 根据条件查询，获得记录集 * * @param where * 查询条件语句 * @param params * 查询条件参数 * @param orderby * 排序属性 value asc/desc */ List&lt;T&gt; findAll(String where, Map&lt;String, ?&gt; params, LinkedHashMap&lt;String, String&gt; orderby); /** * 根据条件查询，获得记录集 查询时对数据加行锁 * * @param where * 查询条件语句 * @param params * 查询条件参数 * @param orderby * 排序属性 value asc/desc */ List&lt;T&gt; findAllWithLock(String where, Map&lt;String, ?&gt; params, LinkedHashMap&lt;String, String&gt; orderby); /** * 根据条件查询，获得记录集 * * @param firstResult * 第一条记录号,为-1时不进行筛选 * @param maxResult * 返回记录最大数,为-1时不进行筛选 * @param where * 查询条件语句 * @param params * 查询条件参数 * @param orderby * 排序属性 value asc/desc */ List&lt;T&gt; findAll(int firstResult, int maxResult, String where, Map&lt;String, Object&gt; params, String orderby); /** * 根据条件查询，获得记录集,只查询fields的字段 * * @param firstResult * 第一条记录号,为-1时不进行筛选 * @param maxResult * 返回记录最大数,为-1时不进行筛选 * @param where * 查询条件语句 * @param params * 查询条件参数 * @param orderby * order by 属性 asc/desc * @param fields * 需要查询的字段 */ QueryResult&lt;Map&lt;String, Object&gt;&gt; getScrollData(int firstResult, int maxResult, String where, Map&lt;String, Object&gt; params, String orderby, String[] fields); /** * 根据条件查询，获得记录集 * * @param firstResult * 第一条记录号,为-1时不进行筛选 * @param maxResult * 返回记录最大数,为-1时不进行筛选 * @param where * 查询条件语句 * @param params * 查询条件参数 * @param orderby * order by 属性 asc/desc */ QueryResult&lt;T&gt; getScrollData(int firstResult, int maxResult, String where, Map&lt;String, Object&gt; params, String orderby); /** * 根据条件查询，获得记录集 * * @param firstResult * 第一条记录号,为-1时不进行筛选 * @param maxResult * 返回记录最大数,为-1时不进行筛选 * @param where * 查询条件语句 * @param params * 查询条件参数 * @param orderby * 排序属性 value asc/desc */ QueryResult&lt;T&gt; getScrollData(int firstResult, int maxResult, String where, Map&lt;String, Object&gt; params, LinkedHashMap&lt;String, String&gt; orderby); /** * 根据条件查询，获得记录集 * * @param firstResult * 第一条记录号 * @param maxResult * 返回记录最大数 * @param orderby * 排序属性 value asc/desc */ QueryResult&lt;T&gt; getScrollData(int firstResult, int maxResult, LinkedHashMap&lt;String, String&gt; orderby); /** * 根据条件查询，获得记录集 * * @param where * 查询条件语句 * @param params * 查询条件参数 */ QueryResult&lt;T&gt; getScrollData(String where, Map&lt;String, Object&gt; params); QueryResult&lt;T&gt; getScrollData();&#125; DaoSupport 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307package cn.edu.ncut.nwrd.common.basedao;import org.hibernate.LockMode;import org.hibernate.Query;import org.hibernate.SessionFactory;import org.springframework.transaction.annotation.Transactional;import javax.annotation.Resource;import javax.persistence.Id;import java.io.Serializable;import java.lang.reflect.Field;import java.lang.reflect.Method;import java.lang.reflect.ParameterizedType;import java.lang.reflect.Type;import java.util.LinkedHashMap;import java.util.List;import java.util.Map;@Transactionalpublic abstract class DaoSupport&lt;T extends Serializable&gt; implements Dao&lt;T&gt; &#123; @Resource protected SessionFactory sessionFactory; protected Class&lt;?&gt; entityClass; protected String entityName; protected Method getIdMethod; public DaoSupport() &#123; if (!\"BaseDaoImpl\".equals(this.getClass().getSimpleName())) &#123; entityClass = getEntityClass(); entityName = getEntityName(); getIdMethod = findGetIdMethod(); &#125; &#125; @SuppressWarnings(\"unchecked\") public Class&lt;T&gt; getEntityClass() &#123; Type parentType = getClass().getGenericSuperclass(); if (parentType instanceof ParameterizedType) &#123; ParameterizedType ptype = (ParameterizedType) parentType; return (Class&lt;T&gt;) ptype.getActualTypeArguments()[0]; &#125; return null; &#125; public String getEntityName() &#123; return this.entityClass.getSimpleName(); &#125; @SuppressWarnings(\"unchecked\") @Transactional(readOnly = true) public T find(Serializable id) &#123; return (T) sessionFactory.getCurrentSession().get(entityClass, id); &#125; public void save(T entity) &#123; sessionFactory.getCurrentSession().persist(entity); &#125; public void update(T entity) &#123; sessionFactory.getCurrentSession().update(entity); &#125; public void merge(T entity) &#123; sessionFactory.getCurrentSession().merge(entity); &#125; public void updateWithQuery(T entity) &#123; try &#123; Serializable id = (Serializable) getIdMethod.invoke(entity); T entitycur = find(id); Field[] fields = entityClass.getDeclaredFields(); for (Field field : fields) &#123; String filedName = field.getName(); String ufilename = filedName.substring(0, 1).toUpperCase() + filedName.substring(1); String getfiledMethodName = \"get\" + ufilename; String setfiledMethodName = \"set\" + ufilename; try &#123; Method getMethod = entityClass .getMethod(getfiledMethodName); Object f = getMethod.invoke(entity); if (f != null) &#123; Class&lt;?&gt; type = getMethod.getReturnType(); Method setMethod = entityClass.getMethod( setfiledMethodName, type); setMethod.invoke(entitycur, f); &#125; &#125; catch (NoSuchMethodException e) &#123; &#125; &#125; this.update(entitycur); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public void delete(Serializable id) &#123; sessionFactory.getCurrentSession().delete( sessionFactory.getCurrentSession().load(entityClass, id)); &#125; @Transactional(readOnly = true) public long getCount() &#123; return (Long) sessionFactory.getCurrentSession() .createQuery(\"select count(o) from \" + entityName + \" o\") .iterate().next(); &#125; @Transactional(readOnly = true) public long getCount(String where, Map&lt;String, Object&gt; params) &#123; String whereql = where != null &amp;&amp; !\"\".equals(where.trim()) ? \" where \" + where : \"\"; Query query = sessionFactory.getCurrentSession().createQuery( \"select count(o) from \" + entityName + \" o \" + whereql); if (where != null &amp;&amp; !\"\".equals(where.trim())) &#123; setParameter(query, params); &#125; return (Long) query.iterate().next(); &#125; @SuppressWarnings(\"unchecked\") @Transactional(readOnly = true) public List&lt;T&gt; findAll(String where, Map&lt;String, ?&gt; params, LinkedHashMap&lt;String, String&gt; orderby) &#123; String whereql = where != null &amp;&amp; !\"\".equals(where.trim()) ? \" where \" + where : \"\"; Query query = sessionFactory.getCurrentSession().createQuery( \"from \" + entityName + \" o \" + whereql + buildOrderby(orderby)); if (where != null &amp;&amp; !\"\".equals(where.trim())) &#123; setParameter(query, params); &#125; return query.list(); &#125; @SuppressWarnings(\"unchecked\") @Transactional(readOnly = true) public List&lt;T&gt; findAll(int firstResult,int maxResult,String where, Map&lt;String, Object&gt; params, String orderby) &#123; String whereql = where != null &amp;&amp; !\"\".equals(where.trim()) ? \" where \" + where : \"\"; orderby = orderby == null ? \"\" : orderby; Query query = sessionFactory.getCurrentSession().createQuery( \"from \" + entityName + \" o \" + whereql + orderby); if (firstResult != -1 &amp;&amp; maxResult != -1) query.setFirstResult(firstResult).setMaxResults(maxResult); if (where != null &amp;&amp; !\"\".equals(where.trim())) &#123; setParameter(query, params); &#125; return query.list(); &#125; @SuppressWarnings(\"unchecked\") @Transactional(readOnly = true) public List&lt;T&gt; findAllWithLock(String where, Map&lt;String, ?&gt; params, LinkedHashMap&lt;String, String&gt; orderby) &#123; String whereql = where != null &amp;&amp; !\"\".equals(where.trim()) ? \" where \" + where : \"\"; Query query = sessionFactory.getCurrentSession().createQuery( \"from \" + entityName + \" o \" + whereql + buildOrderby(orderby)).setLockMode(\"o\", LockMode.UPGRADE); if (where != null &amp;&amp; !\"\".equals(where.trim())) &#123; setParameter(query, params); &#125; return query.list(); &#125; public List&lt;T&gt; findAll() &#123; return findAll(null, null, null); &#125; @SuppressWarnings(\"unchecked\") @Transactional(readOnly = true) public QueryResult&lt;T&gt; getScrollData(int firstResult, int maxResult, String where, Map&lt;String, Object&gt; params, String orderby) &#123; String whereql = where != null &amp;&amp; !\"\".equals(where.trim()) ? \" where \" + where : \"\"; orderby = orderby == null ? \"\" : orderby; Query query = sessionFactory.getCurrentSession().createQuery( \"from \" + entityName + \" o \" + whereql + orderby); if (firstResult != -1 &amp;&amp; maxResult != -1) query.setFirstResult(firstResult).setMaxResults(maxResult); if (where != null &amp;&amp; !\"\".equals(where.trim())) &#123; setParameter(query, params); &#125; QueryResult&lt;T&gt; qr = new QueryResult&lt;T&gt;(); qr.setResultlist(query.list()); query = sessionFactory.getCurrentSession().createQuery( \"select count(o) from \" + entityName + \" o \" + whereql); if (where != null &amp;&amp; !\"\".equals(where.trim())) &#123; setParameter(query, params); &#125; qr.setRecordtotal((Long) query.iterate().next()); return qr; &#125; @SuppressWarnings(\"unchecked\") @Transactional(readOnly = true) public QueryResult&lt;Map&lt;String, Object&gt;&gt; getScrollData(int firstResult, int maxResult, String where, Map&lt;String, Object&gt; params, String orderby, String[] fields) &#123; String whereql = where != null &amp;&amp; !\"\".equals(where.trim()) ? \" where \" + where : \"\"; String strFields = buildFields(fields); orderby = orderby == null ? \"\" : orderby; Query query = sessionFactory.getCurrentSession().createQuery( \"select \" + strFields + \" from \" + entityName + \" o \" + whereql + orderby); if (firstResult != -1 &amp;&amp; maxResult != -1) query.setFirstResult(firstResult).setMaxResults(maxResult); if (where != null &amp;&amp; !\"\".equals(where.trim())) &#123; setParameter(query, params); &#125; QueryResult&lt;Map&lt;String, Object&gt;&gt; qr = new QueryResult&lt;Map&lt;String, Object&gt;&gt;(); qr.setResultlist(query.list()); query = sessionFactory.getCurrentSession().createQuery( \"select count(o) from \" + entityName + \" o \" + whereql); if (where != null &amp;&amp; !\"\".equals(where.trim())) &#123; setParameter(query, params); &#125; qr.setRecordtotal((Long) query.iterate().next()); return qr; &#125; public QueryResult&lt;T&gt; getScrollData(int firstResult, int maxResult, String where, Map&lt;String, Object&gt; params, LinkedHashMap&lt;String, String&gt; orderby) &#123; return getScrollData(firstResult, maxResult, where, params, buildOrderby(orderby)); &#125; public QueryResult&lt;T&gt; getScrollData(String where, Map&lt;String, Object&gt; params) &#123; return getScrollData(-1, -1, where, params, \"\"); &#125; public QueryResult&lt;T&gt; getScrollData(int firstResult, int maxResult, LinkedHashMap&lt;String, String&gt; orderby) &#123; return getScrollData(firstResult, maxResult, \"\", null, orderby); &#125; /** * 获得记录集 */ public QueryResult&lt;T&gt; getScrollData() &#123; return getScrollData(-1, -1, \"\", null, \"\"); &#125; /** /** * 构建排序语句 * * @param orderby * key排序属性 value asc/desc * @return */ protected static String buildOrderby(LinkedHashMap&lt;String, String&gt; orderby) &#123; StringBuilder sb = new StringBuilder(); if (orderby != null &amp;&amp; !orderby.isEmpty()) &#123; sb.append(\" order by \"); for (Map.Entry&lt;String, String&gt; entry : orderby.entrySet()) &#123; sb.append(\"o.\").append(entry.getKey()).append(\" \") .append(entry.getValue()).append(\",\"); &#125; sb.deleteCharAt(sb.length() - 1); &#125; return sb.toString(); &#125; protected static void setParameter(Query query, Map&lt;String, ?&gt; params) &#123; if (params == null || params.size() == 0) return; query.setProperties(params); &#125; protected static String buildFields(String[] fields) &#123; StringBuilder sb = new StringBuilder(\"new map(\"); for (String field : fields) &#123; if (field.lastIndexOf(\".\") &gt;= 0) sb.append(field.toLowerCase()).append(\" as \") .append(field.substring(field.lastIndexOf(\".\")+1)) .append(\",\"); else sb.append(field.toLowerCase()).append(\" as \").append(field) .append(\",\"); &#125; sb.deleteCharAt(sb.length() - 1); sb.append(\")\"); return sb.toString(); &#125; protected Method findGetIdMethod() &#123; Method[] methods = entityClass.getMethods(); for (Method method : methods) &#123; if (method.isAnnotationPresent(Id.class)) &#123; return method; &#125; &#125; return null; &#125; &#125; QueryResult 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package cn.edu.ncut.nwrd.common.basedao;import org.apache.log4j.Logger;import org.dom4j.Element;import java.lang.reflect.Method;import java.util.List;import java.util.Map;public class QueryResult&lt;T&gt; &#123; private final static Logger logger = Logger .getLogger(QueryResult.class); private List&lt;T&gt; resultlist; private long recordtotal; public void setResultlist(List&lt;T&gt; resultlist) &#123; this.resultlist = resultlist; &#125; public List&lt;T&gt; getResultlist() &#123; return resultlist; &#125; public void setRecordtotal(long recordtotal) &#123; this.recordtotal = recordtotal; &#125; public long getRecordtotal() &#123; return recordtotal; &#125; @SuppressWarnings(\"unchecked\") private void addData(String[] fields, Map&lt;String, Object&gt; fieldMap, Element element, T record) throws Exception &#123; Class&lt;T&gt; cla = (Class&lt;T&gt;) resultlist.get(0).getClass(); for (String field : fields) &#123; Object fieldItem = null; if (fieldMap != null) &#123; fieldItem = fieldMap.get(field); if (fieldItem != null &amp;&amp; String.class.isAssignableFrom(fieldItem .getClass())) &#123; if (\"hide\".equals(fieldItem)) continue; &#125; &#125; Object obj; if (Map.class.isAssignableFrom(cla)) &#123; if(field.lastIndexOf(\".\")&gt;=0) field=field.substring(field.lastIndexOf(\".\")+1); obj = ((Map&lt;String, Object&gt;) record).get(field); &#125; else &#123; String methodName = \"get\" + field.substring(0, 1).toUpperCase() + field.substring(1).toLowerCase(); Method m = cla.getMethod(methodName); obj = m.invoke(record); &#125; String text = buildData(field, fieldItem, obj, record); element.addElement(field).addText(text); &#125; &#125; @SuppressWarnings(\"unchecked\") private String buildData(String field, Object fieldItem, Object data, T record) &#123; if(field.lastIndexOf(\".\")&gt;=0) field=field.substring(field.lastIndexOf(\".\")+1); if (data == null) return \"\"; String text = \"\"; if (fieldItem != null)&#123; if(Map.class.isAssignableFrom(fieldItem.getClass()))&#123; Map&lt;Object, String&gt; map = (Map&lt;Object, String&gt;) fieldItem; text = map.get(data); &#125;else if((Method.class.isAssignableFrom(fieldItem.getClass())))&#123; Method method = (Method) fieldItem; try &#123; text = (String) method.invoke(null, data, record); &#125; catch (Exception e) &#123; logger.error(e); &#125; &#125; &#125; else &#123; text = data.toString(); &#125; return text; &#125;&#125; 12345678package cn.edu.ncut.nwrd.dao;import cn.edu.ncut.nwrd.common.basedao.Dao;import cn.edu.ncut.nwrd.model.UserObj;public interface UserDao extends Dao&lt;UserObj&gt;&#123;&#125; 12345678910111213141516package cn.edu.ncut.nwrd.dao;import cn.edu.ncut.nwrd.common.basedao.DaoSupport;import cn.edu.ncut.nwrd.model.UserObj;import org.apache.log4j.Logger;import org.springframework.stereotype.Repository;/** * @author NikoBelic * @create 2017/6/27 15:12 */@Repository(\"userDao\")public class UserDaoImpl extends DaoSupport&lt;UserObj&gt; implements UserDao&#123; private final static Logger logger = Logger.getLogger(UserDaoImpl.class);&#125; 1.4.4 Service1234567891011121314package cn.edu.ncut.nwrd.service;import cn.edu.ncut.nwrd.model.UserObj;import java.util.List;/** * @author NikoBelic * @create 2017/6/27 21:56 */public interface UserService&#123; List&lt;UserObj&gt; getAllUser();&#125; 123456789101112131415161718192021222324252627package cn.edu.ncut.nwrd.service;import cn.edu.ncut.nwrd.dao.UserDao;import cn.edu.ncut.nwrd.model.UserObj;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.List;/** * @author NikoBelic * @create 2017/6/27 21:57 */@Servicepublic class UserServiceImpl implements UserService&#123; @Autowired private UserDao userDao; @Override public List&lt;UserObj&gt; getAllUser() &#123; List&lt;UserObj&gt; userList = userDao.findAll(); return userList; &#125;&#125; 1.4.5 Controller1234567891011121314151617181920212223242526272829303132package cn.edu.ncut.nwrd.controller;import cn.edu.ncut.nwrd.service.UserService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.servlet.ModelAndView;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import java.util.HashMap;/** * @author NikoBelic * @create 2017/6/27 21:59 */@Controller@RequestMapping(\"/user\")public class UserController&#123; @Autowired private UserService userService; @RequestMapping(value = \"/getAllUser\") public Object getAllUser(HttpServletRequest request, HttpServletResponse response) &#123; HashMap&lt;String, Object&gt; result = new HashMap&lt;&gt;(); result.put(\"userList\", userService.getAllUser()); return new ModelAndView(\"/user/list\", result); &#125;&#125; 1.4.6 视图1234567891011121314151617181920212223242526272829303132&lt;%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\" pageEncoding=\"UTF-8\"%&gt;&lt;%@ include file=\"../commons/taglibs.jsp\" %&gt;&lt;%@ taglib tagdir=\"/WEB-INF/tags/simpletable\" prefix=\"simpletable\"%&gt;&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;用户列表&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;td&gt;编号&lt;/td&gt; &lt;td&gt;用户名&lt;/td&gt; &lt;td&gt;出生日期&lt;/td&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;c:forEach items=\"$&#123;userList&#125;\" var=\"user\"&gt; &lt;tr&gt; &lt;td&gt;$&#123;user.id&#125;&lt;/td&gt; &lt;td&gt;$&#123;user.name&#125;&lt;/td&gt; &lt;td&gt;$&#123;user.birthday&#125;&lt;/td&gt; &lt;/tr&gt; &lt;/c:forEach&gt; &lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 1.4.7 渲染结果 1.4.8 测试Spring+Hibernate12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package cn.edu.ncut.nwrd.test;import cn.edu.ncut.nwrd.dao.LogDao;import cn.edu.ncut.nwrd.dao.UserDao;import cn.edu.ncut.nwrd.model.LogObj;import cn.edu.ncut.nwrd.model.UserObj;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import javax.annotation.Resource;import java.sql.Timestamp;import java.util.Date;import java.util.List;/** * @author NikoBelic * @create 2017/6/27 15:13 */@ContextConfiguration(locations = \"classpath:spring/*.xml\")@RunWith(SpringJUnit4ClassRunner.class)public class TestSSH&#123; @Resource private UserDao userDao; @Resource private LogDao logDao; @Test public void testAddUser() &#123; testGetAll(); UserObj user = new UserObj(); user.setName(\"YJM\"); user.setBirthday(new Date()); user.setCreateTime(new Timestamp(System.currentTimeMillis() / 1000)); userDao.save(user); testGetAll(); &#125; @Test public void testGetUser() &#123; UserObj userObj = userDao.find(5); System.out.println(\"userObj = \" + userObj); &#125; @Test public void testUpdateUser() &#123; testGetAll(); UserObj userObj = userDao.find(5); if (userObj != null) &#123; userObj.setName(\"新全国作品登记\"); userDao.update(userObj); &#125; testGetAll(); &#125; @Test public void testDeleteUser() &#123; testGetAll(); userDao.delete(3); testGetAll(); &#125; @Test public void testGetAll() &#123; System.out.println(\"查询\"); List&lt;UserObj&gt; userList = userDao.findAll(); for (UserObj userDO : userList) &#123; System.out.println(\"userObj = \" + userDO); &#125; &#125; @Test public void testAddLog() &#123; LogObj log = new LogObj(); log.setMsg(\"测试日志\"); logDao.save(log); &#125;&#125; 2 Oracle数据库实例搭建 Server创建实例 Client连接实例 创建临时表空间、表空间 创建用户 给用户分配表空间 测试连接 3 问题总结 Oracle11g区分大小写，创建表和字段时我使用的是小写，但是Hibernate访问Oracle时会将SQL中的表名、字段名转换为大写，导致出现 “未找到表或视图” 解决：将Oracle表名、字段名都改为大写 Maven 无法引入 Oracle驱动问题。上面已经说了解决方法。 创建包文件夹时，不小心创建成了 python package 导致import后面的包名都标红，查了半天才发现是创建错了包类型。","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"}]},{"title":"Scrapy学习笔记（一）- 快速入门","date":"2017-06-24T02:41:04.000Z","path":"2017/06/24/爬虫/python/Scrapy学习笔记（一）- 快速入门/","text":"1 概览1.1 安装pip install scrapy=1.4.0 1.2 测试Demovim quotes_spider.py 12345678910111213141516171819import scrapyclass QuotesSpider(scrapy.Spider): name = \"quotes\" start_urls = [ 'http://quotes.toscrape.com/tag/humor/', ] def parse(self, response): for quote in response.css('div.quote'): yield &#123; 'text': quote.css('span.text::text').extract_first(), 'author': quote.xpath('span/small/text()').extract_first(), &#125; next_page = response.css('li.next a::attr(\"href\")').extract_first() if next_page is not None: yield response.follow(next_page, self.parse) scrapy runspider quotes_spider.py -o quotes.json 查看输出结果 12345678910111213141516171819202122232425262728292017-06-23 23:23:58 [scrapy.core.engine] INFO: Spider opened2017-06-23 23:23:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)2017-06-23 23:23:58 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:60232017-06-23 23:23:59 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/tag/humor/&gt; (referer: None)2017-06-23 23:23:59 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/tag/humor/&gt;&#123;&apos;text&apos;: &apos;“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”&apos;, &apos;author&apos;: &apos;Jane Austen&apos;&#125;2017-06-23 23:23:59 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/tag/humor/&gt;&#123;&apos;text&apos;: &apos;“A day without sunshine is like, you know, night.”&apos;, &apos;author&apos;: &apos;Steve Martin&apos;&#125;2017-06-23 23:23:59 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/tag/humor/&gt;&#123;&apos;text&apos;: &apos;“Anyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.”&apos;, &apos;author&apos;: &apos;Garrison Keillor&apos;&#125;2017-06-23 23:23:59 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/tag/humor/&gt;&#123;&apos;text&apos;: &apos;“Beauty is in the eye of the beholder and it may be necessary from time to time to give a stupid or misinformed beholder a black eye.”&apos;, &apos;author&apos;: &apos;Jim Henson&apos;&#125;2017-06-23 23:23:59 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/tag/humor/&gt;&#123;&apos;text&apos;: &quot;“All you need is love. But a little chocolate now and then doesn&apos;t hurt.”&quot;, &apos;author&apos;: &apos;Charles M. Schulz&apos;&#125;2017-06-23 23:23:59 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/tag/humor/&gt;&#123;&apos;text&apos;: &quot;“Remember, we&apos;re madly in love, so it&apos;s all right to kiss me anytime you feel like it.”&quot;, &apos;author&apos;: &apos;Suzanne Collins&apos;&#125;2017-06-23 23:23:59 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/tag/humor/&gt;&#123;&apos;text&apos;: &apos;“Some people never go crazy. What truly horrible lives they must lead.”&apos;, &apos;author&apos;: &apos;Charles Bukowski&apos;&#125;2017-06-23 23:23:59 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/tag/humor/&gt;&#123;&apos;text&apos;: &apos;“The trouble with having an open mind, of course, is that people will insist on coming along and trying to put things in it.”&apos;, &apos;author&apos;: &apos;Terry Pratchett&apos;&#125;2017-06-23 23:23:59 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/tag/humor/&gt;&#123;&apos;text&apos;: &apos;“Think left and think right and think low and think high. Oh, the thinks you can think up if only you try!”&apos;, &apos;author&apos;: &apos;Dr. Seuss&apos;&#125;2017-06-23 23:23:59 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/tag/humor/&gt;&#123;&apos;text&apos;: &apos;“The reason I talk to myself is because I’m the only one whose answers I accept.”&apos;, &apos;author&apos;: &apos;George Carlin&apos;&#125;2017-06-23 23:24:00 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/tag/humor/page/2/&gt; (referer: http://quotes.toscrape.com/tag/humor/)2017-06-23 23:24:00 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/tag/humor/page/2/&gt;&#123;&apos;text&apos;: &apos;“I am free of all prejudice. I hate everyone equally. ”&apos;, &apos;author&apos;: &apos;W.C. Fields&apos;&#125;2017-06-23 23:24:00 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://quotes.toscrape.com/tag/humor/page/2/&gt;&#123;&apos;text&apos;: &quot;“A lady&apos;s imagination is very rapid; it jumps from admiration to love, from love to matrimony in a moment.”&quot;, &apos;author&apos;: &apos;Jane Austen&apos;&#125; 2 入门2.1 创建项目scrapy startproject tutorial结构如下 1234567891011121314tutorial/ scrapy.cfg # deploy configuration file tutorial/ # project&apos;s Python module, you&apos;ll import your code from here __init__.py items.py # project items definition file pipelines.py # project pipelines file settings.py # project settings file spiders/ # a directory where you&apos;ll later put your spiders __init__.py 2.2 第一个爬虫2.3.1 定义爬虫爬虫类必须是scrapy.Spider的子类，并且定义初始爬取页面在tutorial/spiders下创建爬虫类 1234567891011121314151617181920import scrapyclass QuotesSpider(scrapy.Spider): name = \"quotes\" def start_requests(self): urls = [ 'http://quotes.toscrape.com/page/1/', 'http://quotes.toscrape.com/page/2/', ] for url in urls: yield scrapy.Request(url=url, callback=self.parse) def parse(self, response): page = response.url.split(\"/\")[-2] filename = 'quotes-%s.html' % page with open(filename, 'wb') as f: f.write(response.body) self.log('Saved file %s' % filename) name: 爬虫的名称，在项目中必须是唯一的。 start_requests(): 返回可遍历的Requests，爬虫将从这里进行爬取。 parse(): 对请求的结果数据进行解析。 2.3.2 运行爬虫scrapy crawl quotes 结果 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657582017-06-24 00:02:16 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: tutorial)2017-06-24 00:02:16 [scrapy.utils.log] INFO: Overridden settings: &#123;&apos;NEWSPIDER_MODULE&apos;: &apos;tutorial.spiders&apos;, &apos;SPIDER_MODULES&apos;: [&apos;tutorial.spiders&apos;], &apos;ROBOTSTXT_OBEY&apos;: True, &apos;BOT_NAME&apos;: &apos;tutorial&apos;&#125;2017-06-24 00:02:16 [scrapy.middleware] INFO: Enabled extensions:[&apos;scrapy.extensions.memusage.MemoryUsage&apos;, &apos;scrapy.extensions.logstats.LogStats&apos;, &apos;scrapy.extensions.telnet.TelnetConsole&apos;, &apos;scrapy.extensions.corestats.CoreStats&apos;]2017-06-24 00:02:16 [scrapy.middleware] INFO: Enabled downloader middlewares:[&apos;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&apos;, &apos;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&apos;, &apos;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&apos;, &apos;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&apos;, &apos;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&apos;, &apos;scrapy.downloadermiddlewares.retry.RetryMiddleware&apos;, &apos;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&apos;, &apos;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&apos;, &apos;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&apos;, &apos;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&apos;, &apos;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&apos;, &apos;scrapy.downloadermiddlewares.stats.DownloaderStats&apos;]2017-06-24 00:02:16 [scrapy.middleware] INFO: Enabled spider middlewares:[&apos;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&apos;, &apos;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&apos;, &apos;scrapy.spidermiddlewares.referer.RefererMiddleware&apos;, &apos;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&apos;, &apos;scrapy.spidermiddlewares.depth.DepthMiddleware&apos;]2017-06-24 00:02:16 [scrapy.middleware] INFO: Enabled item pipelines:[]2017-06-24 00:02:16 [scrapy.core.engine] INFO: Spider opened2017-06-24 00:02:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)2017-06-24 00:02:16 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:60232017-06-24 00:02:17 [scrapy.core.engine] DEBUG: Crawled (404) &lt;GET http://quotes.toscrape.com/robots.txt&gt; (referer: None)2017-06-24 00:02:17 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/1/&gt; (referer: None)2017-06-24 00:02:17 [quotes] DEBUG: Saved file quotes-1.html2017-06-24 00:02:17 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/2/&gt; (referer: None)2017-06-24 00:02:17 [quotes] DEBUG: Saved file quotes-2.html2017-06-24 00:02:17 [scrapy.core.engine] INFO: Closing spider (finished)2017-06-24 00:02:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:&#123;&apos;downloader/request_bytes&apos;: 675, &apos;downloader/request_count&apos;: 3, &apos;downloader/request_method_count/GET&apos;: 3, &apos;downloader/response_bytes&apos;: 5976, &apos;downloader/response_count&apos;: 3, &apos;downloader/response_status_count/200&apos;: 2, &apos;downloader/response_status_count/404&apos;: 1, &apos;finish_reason&apos;: &apos;finished&apos;, &apos;finish_time&apos;: datetime.datetime(2017, 6, 23, 16, 2, 17, 551927), &apos;log_count/DEBUG&apos;: 6, &apos;log_count/INFO&apos;: 7, &apos;memusage/max&apos;: 51879936, &apos;memusage/startup&apos;: 51875840, &apos;response_received_count&apos;: 3, &apos;scheduler/dequeued&apos;: 2, &apos;scheduler/dequeued/memory&apos;: 2, &apos;scheduler/enqueued&apos;: 2, &apos;scheduler/enqueued/memory&apos;: 2, &apos;start_time&apos;: datetime.datetime(2017, 6, 23, 16, 2, 16, 322185)&#125;2017-06-24 00:02:17 [scrapy.core.engine] INFO: Spider closed (finished) 现在当前目录下应该出现了两个文件 quotes-1.html quotes-2.html 2.3.3 抽取数据使用 scrapy shell &#39;http://quotes.toscrape.com/page/1/&#39; 通过shell来学习如何抽取我们想要的数据。 CSS选择器 1234567891011121314&gt;&gt;&gt; response.css(&apos;title&apos;)[&lt;Selector xpath=u&apos;descendant-or-self::title&apos; data=u&apos;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&apos;&gt;]&gt;&gt;&gt; response.css(&apos;title&apos;).extract()[u&apos;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&apos;]&gt;&gt;&gt; response.css(&apos;title::text&apos;).extract()[u&apos;Quotes to Scrape&apos;]&gt;&gt;&gt; response.css(&apos;title::text&apos;).extract_first()u&apos;Quotes to Scrape&apos;&gt;&gt;&gt; response.css(&apos;title::text&apos;)[0].extract()u&apos;Quotes to Scrape&apos;&gt;&gt;&gt; response.css(&apos;title::text&apos;).extract()[0]u&apos;Quotes to Scrape&apos;&gt;&gt;&gt; view(response)True 使用extract_first抽取数据可以避免角标越界，抽取不到会返回None。 XPath 123456&gt;&gt;&gt; response.xpath(&quot;//title&quot;)[&lt;Selector xpath=&apos;//title&apos; data=u&apos;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&apos;&gt;]&gt;&gt;&gt; response.xpath(&quot;//title/text()&quot;)[&lt;Selector xpath=&apos;//title/text()&apos; data=u&apos;Quotes to Scrape&apos;&gt;]&gt;&gt;&gt; response.xpath(&quot;//title/text()&quot;).extract_first()u&apos;Quotes to Scrape&apos; XPath表达式非常强大，并且它是Scrapy Selectors的基础。起始CSS选择器在底层也是被转换为XPath去执行的。 Selectors 举例 抽取title和author HTML原文 123456789101112131415&lt;div class=\"quote\"&gt; &lt;span class=\"text\"&gt;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&lt;/span&gt; &lt;span&gt; by &lt;small class=\"author\"&gt;Albert Einstein&lt;/small&gt; &lt;a href=\"/author/Albert-Einstein\"&gt;(about)&lt;/a&gt; &lt;/span&gt; &lt;div class=\"tags\"&gt; Tags: &lt;a class=\"tag\" href=\"/tag/change/page/1/\"&gt;change&lt;/a&gt; &lt;a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\"&gt;deep-thoughts&lt;/a&gt; &lt;a class=\"tag\" href=\"/tag/thinking/page/1/\"&gt;thinking&lt;/a&gt; &lt;a class=\"tag\" href=\"/tag/world/page/1/\"&gt;world&lt;/a&gt; &lt;/div&gt;&lt;/div&gt; 进入抽取shellscrapy shell &#39;http://quotes.toscrape.com&#39; 获取quote quote = response.css(&#39;div.quote&#39;)[0] 抽取title title = quote.css(&#39;span.text::text&#39;).extract_first() 抽取author author = quote.css(&#39;small.author::text&#39;).extract_first() 抽取tags tags = quote.css(&#39;div.tags a.tag::text&#39;).extract() 我们学会了如何抽取指定信息后，将抽取结果存储到python的字段中 12345678&gt;&gt;&gt; for quote in response.css(\"div.quote\"):... text = quote.css(\"span.text::text\").extract_first()... author = quote.css(\"small.author::text\").extract_first()... tags = quote.css(\"div.tags a.tag::text\").extract()... print(dict(text=text, author=author, tags=tags))&#123;'tags': ['change', 'deep-thoughts', 'thinking', 'world'], 'author': 'Albert Einstein', 'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'&#125;&#123;'tags': ['abilities', 'choices'], 'author': 'J.K. Rowling', 'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”'&#125; ... a few more of these, omitted for brevity 2.3.4 存储数据scrapy crawl quotes -o quotes.json注意 重复执行爬虫 输出的文件不会覆盖 而是追加，从而导致了错误的json格式scrapy crawl quotes -o quotes.jl 2.3.5 获取新的连接HTML 12345&lt;ul class=&quot;pager&quot;&gt; &lt;li class=&quot;next&quot;&gt; &lt;a href=&quot;/page/2/&quot;&gt;Next &lt;span aria-hidden=&quot;true&quot;&gt;&amp;rarr;&lt;/span&gt;&lt;/a&gt; &lt;/li&gt;&lt;/ul&gt; 我们虽然可以通过 response.css(&#39;li.next a&#39;).extract_first() 拿到1&lt;a href=\"/page/2/\"&gt;Next &lt;span aria-hidden=\"true\"&gt;→&lt;/span&gt;&lt;/a&gt; 但是我们需要的仅仅是href，可是这样操作 response.css(&#39;li.next a::attr(href)&#39;).extract_first() 1&apos;/page/2/&apos; 现在我们的爬虫是这样的 123456789101112131415161718192021import scrapyclass QuotesSpider(scrapy.Spider): name = \"quotes\" start_urls = [ 'http://quotes.toscrape.com/page/1/', ] def parse(self, response): for quote in response.css('div.quote'): yield &#123; 'text': quote.css('span.text::text').extract_first(), 'author': quote.css('small.author::text').extract_first(), 'tags': quote.css('div.tags a.tag::text').extract(), &#125; next_page = response.css('li.next a::attr(href)').extract_first() if next_page is not None: next_page = response.urljoin(next_page) yield scrapy.Request(next_page, callback=self.parse) 加入一些简化操作 follow支持相对路径123next_page = response.css('li.next a::attr(href)').extract_first()if next_page is not None: yield response.follow(next_page, callback=self.parse) follow可以自动识别href 12for a in response.css('li.next a'): yield response.follow(a, callback=self.parse) 3 Demo帮同学爬一个健康网站的数据，网站也没有反爬手段，结构又很简单，所以现学现卖用scrapy（代码写的比较随意，仅供参考），其实向这种简单的爬虫，直接用requests更省事儿。project最终结构123456789101112131415└── boohee ├── boohee │ ├── __init__.py │ ├── items.py │ ├── middlewares.py │ ├── pipelines.py │ ├── settings.py │ └── spiders │ ├── __init__.py │ ├── food_detail.py │ └── food_name_spider.py │ ├── food_detail.json ├── food_name.json └── scrapy.cfg 核心代码 food_name.py 123456789101112131415161718192021222324import scrapyimport timefrom flask import jsonfile_res = json.loads(open('/Users/lixiwei-mac/Documents/IdeaProjects/rhinotech_spider/boohee_spider/boohee/food_name.json').read())cookie = 'gwdang_brwext_share=0; from_device=default; gwdang_brwext_more_force=0; history=1479334-223%2C2365144-3%2C1462046326-3%2C12596075384-3%2C2365148-3%2C2365158-3%2C4830462-3%2C3498623-3%2C2504829-3%2C11905178-3; gwdang_permanent_id=9b26f59a3e854641cafe23c267ea728b; gwdang_brwext_is_open=0; gwdang_brwext_first=1; gwdang_brwext_position=0; gwdang_brwext_close_update=0; gwdang_brwext_close_update_hour=0; gwdang_brwext_close_install=0; gwdang_brwext_style=top; gwdang_brwext_notice=0; gwdang_brwext_fold=0; gwdang_brwext_show_tip=1; gwdang_brwext_imageAd=1; gwdang_brwext_show_popup=1; gwdang_brwext_show_ljfqrcode=1; gwdang_brwext_hide_shoptip=0; gwdang_brwext_apptg_close=0; gwdang_brwext_show_lowpri=1; gwdang_brwext_show_guessfavor=1; gwdang_brwext_show_lowpri_right=1; gwdang_brwext_show_guessfavor_right=1; gwdang_brwext_show_vips=1; gwdang_brwext_show_wishlist=1; gwdang_brwext_show_guess=1; gwdang_brwext_show_promo=1; gwdang_search_way=0'headers = &#123;'Cookie':cookie&#125;class FoodNameSpider(scrapy.Spider): name = 'food_name' base_url = 'http://www.boohee.com/food/group/%s' start_urls = [base_url % x for x in range(41)] * 10 def parse(self, response): for food_ref in response.css('ul.food-list li h4 a::attr(href)').extract(): group_id = response.url.split('/')[5].split(\"?\")[0] food_name = response.xpath('//a[@href=\"%s\"]/@title' % food_ref).extract_first() res = &#123;'group_id': group_id, 'href': food_ref, 'food_name': food_name&#125; if res not in file_res: yield res for href in response.css('a.next_page'): time.sleep(0.1) yield response.follow(href, self.parse, headers=headers) food_detail.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import scrapyimport timefrom flask import jsonfood_names = json.loads(open('food_name.json').read())class FoodDetailSpider(scrapy.Spider): name = 'food_detail' start_urls = ['http://www.boohee.com' + food_name_obj['href'] for food_name_obj in food_names] def parse(self, response): calories_value = response.css('ul.basic-infor').xpath('//li/span/span/text()').extract_first() food_name = response.css('h2.crumb::text').extract()[-1].split(\"/\")[-1].strip() evaluation = ''.join(response.css('div.content p::text').extract()).strip() food_group_name = response.xpath('//h2/a/text()').extract()[1] # nutrition infomation nutrition_info = [] for nutr_dl in response.css('div.nutr-tag dl')[1:]: for nutr_dd in nutr_dl.css('dd'): nutr_key = nutr_dd.css('span.dt::text').extract_first() if nutr_dd.css('span.stress'): nutr_value = nutr_dd.css('span.stress::text').extract_first() else: nutr_value = nutr_dd.css('span.dd::text').extract_first() # print nutr_key, nutr_value nutrition_info.append(&#123;nutr_key: nutr_value&#125;) # widget-unit widget_unit_info = [] for w_u_tr in response.css('div.widget-unit tbody tr'): if w_u_tr.css('td a'): w_u_name = w_u_tr.css('td a::text').extract()[0] w_u_value = w_u_tr.css('td a::text').extract()[1] elif w_u_tr.css('td span'): w_u_name = w_u_tr.css('td span::text').extract()[0] w_u_value = w_u_tr.css('td span::text').extract()[1] else: w_u_name = w_u_tr.css('td::text').extract()[0] w_u_value = w_u_tr.css('td::text').extract()[1] # print w_u_name, w_u_value widget_unit_info.append(&#123;w_u_name: w_u_value&#125;) time.sleep(0.5) yield &#123;'food_group_name': food_group_name, 'food_name': food_name,'nutrition_info': nutrition_info, 'widget_unit_info': widget_unit_info, 'evaluation': evaluation&#125; 必须掌握 调试解析html的方法scrapy shell &#39; scrapy shell &#39;http://www.boohee.com/food/group/1&#39;&#39; 必须掌握导出结果的方法输出到控制台 scrapy crawl author输出到文件 scrapy crawl quotes -o quotes.json","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://seawaylee.github.io/tags/爬虫/"},{"name":"Python","slug":"Python","permalink":"https://seawaylee.github.io/tags/Python/"}]},{"title":"Docker学习笔记（一）- 快速入门","date":"2017-06-21T17:54:57.000Z","path":"2017/06/22/Linux/Docker学习笔记（一）- 快速入门/","text":"[TOC] 本文是我通过Docker官方文档学习时做的一些笔记。能够快速理清Docker中的各种概念并进行实践。 1 安装、运行1.1 下载https://download.docker.com/mac/stable/Docker.dmg下载完成后安装运行 1.2 测试是否启动成功docker [?] version info images ps [-a] run hello-world run -it ubuntu bash run -d -p 80:80 –name webserver nginx stop webserver rm -f webserver pull [image_name] start [image_name] 2 Get-Started2.1 概览2.1.1 什么是imageimage是一个轻量的、独立的、可执行的包，其中包含了运行某一软件所需要的全部资源（代码、运行时环境、库、环境变量、配置文件） 2.1.2 什么是containercontainer是image的一个运行实例–image在内存中被执行后的结果。container的运行完全与宿主隔离（默认），仅会根据配置去访问宿主机的文件、端口。containers在宿主机的内核中运行各种app。与虚拟机相比的优点是：这些containers比虚拟机更有特质。每一个container都运行于不同的进程中，不占用用其他程序的内存。 2.1.3 VM与Container的对比VM Container 2.1.4 Docker的层级 Stack: 一组相互关联的服务，共享依赖关系，并且可以协调一致。 Services: 对containers的管理与配置，构建出一个服务 Container: app真实运行的环境， 2.2 Containers将应用的依赖、运行时环境打包并运行在一个image中。使用Dockerfile定义一个container。 2.2.1 创建DockerfileDockerfile是container的配置文件mkdir -p docker_envs/firstapp &amp;&amp; cd docker_envs/firstapp &amp;&amp; vim DockerfileDockerfile 1234567891011121314151617181920# Use an official Python runtime as a base imageFROM python:2.7-slim# Set the working directory to /appWORKDIR /app# Copy the current directory contents into the container at /appADD . /app# Install any needed packages specified in requirements.txtRUN pip install -r requirements.txt# Make port 80 available to the world outside this containerEXPOSE 80# Define environment variableENV NAME World# Run app.py when the container launchesCMD [&quot;python&quot;, &quot;app.py&quot;] 2.2.2 创建其他配置文件从Dockerfile的配置中可以看到，我们还需要 app.py requirements.txt 两个文件 requirements.txt 12FlaskRedis app.py 123456789101112131415161718192021222324from flask import Flaskfrom redis import Redis, RedisErrorimport osimport socket# Connect to Redisredis = Redis(host=\"redis\", db=0, socket_connect_timeout=2, socket_timeout=2)app = Flask(__name__)@app.route(\"/\")def hello(): try: visits = redis.incr(\"counter\") except RedisError: visits = \"&lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;\" html = \"&lt;h3&gt;Hello &#123;name&#125;!&lt;/h3&gt;\" \\ \"&lt;b&gt;Hostname:&lt;/b&gt; &#123;hostname&#125;&lt;br/&gt;\" \\ \"&lt;b&gt;Visits:&lt;/b&gt; &#123;visits&#125;\" return html.format(name=os.getenv(\"NAME\", \"world\"), hostname=socket.gethostname(), visits=visits)if __name__ == \"__main__\": app.run(host='0.0.0.0', port=80) 当前 firstapp文件夹下有以下三个文件Dockerfile app.py requirements.txt 2.2.3 构建APP 构建app docker build friendlyhello 查看镜像 docker images 运行app docker run -p 4000:80 friendlyhello 访问app localhost:4000 即可访问container中80端口启动的服务 后台运行app docker run -d -p 4000:80 friendlyhello 查看container进程 docker ps 关闭container docker stop [CONTAINER ID] 2.2.4 共享image2.2.4.1 注册一个docker账号https://cloud.docker.com/ 2.2.4.2 登陆docker login输入用户名密码，若登陆成功则显示Login Succeeded 2.2.4.3 将本地image注册到docker云打标签的格式 docker tag image username/repository:tag举例 docker tag friendlyhello NikoBelic/get-started:part1上传 docker push NikoBelic/get-started:part1到docker云上查看 远程获取并运行 `docker run -p 4000:80 username/repository:tag 2.2.5 最常用命令12345678910111213141516docker build -t friendlyname . # Create image using this directory&apos;s Dockerfiledocker run -p 4000:80 friendlyname # Run &quot;friendlyname&quot; mapping port 4000 to 80docker run -d -p 4000:80 friendlyname # Same thing, but in detached modedocker ps # See a list of all running containersdocker stop &lt;hash&gt; # Gracefully stop the specified containerdocker ps -a # See a list of all containers, even the ones not runningdocker kill &lt;hash&gt; # Force shutdown of the specified containerdocker rm &lt;hash&gt; # Remove the specified container from this machinedocker rm $(docker ps -a -q) # Remove all containers from this machinedocker images -a # Show all images on this machinedocker rmi &lt;imagename&gt; # Remove the specified image from this machinedocker rmi $(docker images -q) # Remove all images from this machinedocker login # Log in this CLI session using your Docker credentialsdocker tag &lt;image&gt; username/repository:tag # Tag &lt;image&gt; for upload to registrydocker push username/repository:tag # Upload tagged image to registrydocker run username/repository:tag # Run image from a registry 2.3 Services一个分布式应用程序，其不同的部分被称为 服务。一般情况下服务的扩展非常困难，需要修改程序的实例数量、分配更多的计算资源等等。而在docker平台上，使用docker-compose.yml文件即可完成。 2.3.1 第一个 docker-compose.yml文件 创建文件 docker-compose.yml 于第二2节中相同的文件夹下 12345678910111213141516171819version: \"3\"services: web: # replace username/repo:tag with your name and image details image: username/repository:tag deploy: replicas: 5 resources: limits: cpus: \"0.1\" memory: 50M restart_policy: condition: on-failure ports: - \"80:80\" networks: - webnetnetworks: webnet: 初始化 docker集群 docker swarm init 部署 docker stack deploy -c docker-compose.yml getstartedlab 查看实例 docker stack ps getstartedlab 123456ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSz7tku2lor2y9 getstartedlab_web.1 lixiwei/get-started:part1 moby Running Running 14 seconds agowpc8euz8h3lt getstartedlab_web.2 lixiwei/get-started:part1 moby Running Running 14 seconds agovda14cmy3yxh getstartedlab_web.3 lixiwei/get-started:part1 moby Running Running 15 seconds ago0pcmypr87bba getstartedlab_web.4 lixiwei/get-started:part1 moby Running Running 14 seconds agol9adv4uufeer getstartedlab_web.5 lixiwei/get-started:part1 moby Running Running 15 seconds ago 多次访问 localhost:80 发现 CONTAINER ID 不相同，原因是load-balance采用了轮询机制 修改replicas后再次部署 docker stack deploy -c docker-compose.yml getstartedlab 关闭应用 docker stack rm getstartedlab 关闭应用后仍然会有一个单节点集群在运行(docker node ls 查看) 使用 docker swarm leave --foce 关闭 本小节使用命令 12345docker stack ls # List all running applications on this Docker hostdocker stack deploy -c &lt;composefile&gt; &lt;appname&gt; # Run the specified Compose filedocker stack services &lt;appname&gt; # List the services associated with an appdocker stack ps &lt;appname&gt; # List the running containers associated with an appdocker stack rm &lt;appname&gt; # Tear down an application 2.4 Swarms2.4.1 概念Swarm 是一个运行着docker的集群可以向SwarmManager发送docker指令。 SwarmManager 在其环境上运行commands，像单机模式一样。 Workers 提供程序运行空间，但不能给其他worker发送指令。 2.4.2 配置SwarmSwarm由多个节点组成，节点可以是真实物理机也可以使虚拟机。在SwarmManager使用 docker swarm init 来激活swarm模式。在Worker使用 docker swarm join 来将机器加入到集群中。 2.4.3 创建集群 首先需要安装virturalbox虚拟机软件 brew cask install virtualbox 创建虚拟机 docker-machine create --driver virtualbox myvm1 123456789101112131415161718192021222324Running pre-create checks...(myvm1) No default Boot2Docker ISO found locally, downloading the latest release...(myvm1) Latest release for github.com/boot2docker/boot2docker is v17.05.0-ce(myvm1) Downloading /Users/lixiwei-mac/.docker/machine/cache/boot2docker.iso from https://github.com/boot2docker/boot2docker/releases/download/v17.05.0-ce/boot2docker.iso...(myvm1) 0%....10%....20%....30%....40%....50%....60%....70%....80%....90%....100%Creating machine...(myvm1) Copying /Users/lixiwei-mac/.docker/machine/cache/boot2docker.iso to /Users/lixiwei-mac/.docker/machine/machines/myvm1/boot2docker.iso...(myvm1) Creating VirtualBox VM...(myvm1) Creating SSH key...(myvm1) Starting the VM...(myvm1) Check network to re-create if needed...(myvm1) Found a new host-only adapter: &quot;vboxnet0&quot;(myvm1) Waiting for an IP...Waiting for machine to be running, this may take a few minutes...Detecting operating system of created instance...Waiting for SSH to be available...Detecting the provisioner...Provisioning with boot2docker...Copying certs to the local machine directory...Copying certs to the remote machine...Setting Docker configuration on the remote daemon...Checking connection to Docker...Docker is up and running!To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env myvm1 docker-machine create --driver virtualbox myvm2 查看虚拟机 docker-machine ls 123NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORSmyvm1 - virtualbox Running tcp://192.168.99.100:2376 v17.05.0-cemyvm2 - virtualbox Running tcp://192.168.99.101:2376 v17.05.0-ce 向虚拟机发送命令 docker-machine ssh myvm1 &quot;docker swarm init&quot;(指定myvm1为manager) 若出现错误类似 –advertise-addr 12Error response from daemon: could not choose an IP address to advertise since this system has multiple addresses on different interfaces (10.0.2.15 on eth0 and 192.168.99.100 on eth1) - specify one with --advertise-addrexit status 1 则使用 docker-machine ssh myvm1 &quot;docker swarm init --advertise-addr 192.168.99.100:2377&quot; 设置成功 123456789Swarm initialized: current node (k650z1bprqz6qkvqhgpzvyk4k) is now a manager.To add a worker to this swarm, run the following command: docker swarm join \\ --token SWMTKN-1-5s83l6iknt9impn1pwdkog4oak3jktyjpy6de384juxsbcms1e-0hknwqjug1slnw1ehe7kgkzjc \\ 192.168.99.100:2377To add a manager to this swarm, run &apos;docker swarm join-token manager&apos; and follow the instructions. 指定myvm2加入swarm docker-machine ssh myvm2 &quot;docker swarm join --token SWMTKN-1-5s83l6iknt9impn1pwdkog4oak3jktyjpy6de384juxsbcms1e-0hknwqjug1slnw1ehe7kgkzjc 192.168.99.100:2377&quot; 若成功，则显示 This node joined a swarm as a worker. 进入myvm2的终端 docker-machine ssh myvm2 12345678910111213141516 ## . ## ## ## == ## ## ## ## ## === /&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\\___/ === ~~~ &#123;~~ ~~~~ ~~~ ~~~~ ~~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\_______/ _ _ ____ _ _| |__ ___ ___ | |_|___ \\ __| | ___ ___| | _____ _ __| &apos;_ \\ / _ \\ / _ \\| __| __) / _` |/ _ \\ / __| |/ / _ \\ &apos;__|| |_) | (_) | (_) | |_ / __/ (_| | (_) | (__| &lt; __/ ||_.__/ \\___/ \\___/ \\__|_____\\__,_|\\___/ \\___|_|\\_\\___|_|Boot2Docker version 17.05.0-ce, build HEAD : 5ed2840 - Fri May 5 21:04:09 UTC 2017Docker version 17.05.0-ce, build 89658bedocker@myvm2:~$ 查看集群节点 docker-machine ssh myvm1 docker node ls 123ID HOSTNAME STATUS AVAILABILITY MANAGER STATUSk650z1bprqz6qkvqhgpzvyk4k * myvm1 Ready Active Leaderpono9aqai63bx5iekdhy9x8if myvm2 Ready Active 2.4.4 在集群中部署app最困难的步骤已经完成了，现在只需要将上一节中的程序部署到新的swarm中即可。一定要记住，只有向myvm1这样的manager才能执行docker命令行，workers只用来工作。 复制docker-compose.yml到myvm1的home目录下docker-machine scp docker-compose.yml myvm1:~ 使用myvm1部署app docker-machine ssh myvm1 &quot;docker stack deploy -c docker-compose.yml getstartedlab&quot; 在上一节中的所有命令都可以在此节点中使用，例如 docker-machine ssh myvm1 &quot;docker stack ps getstartedlab&quot; 1234567891011ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSallz0w2fabv7 getstartedlab_web.1 lixiwei/get-started:part1 myvm1 Running Preparing 37 seconds agol7w76v6ldure getstartedlab_web.2 lixiwei/get-started:part1 myvm1 Running Preparing 37 seconds agojei5oc8wuvqr getstartedlab_web.3 lixiwei/get-started:part1 myvm1 Running Preparing 37 seconds agoxt6vr4dud1le getstartedlab_web.4 lixiwei/get-started:part1 myvm1 Running Preparing 37 seconds agoj95zpui1u926 getstartedlab_web.5 lixiwei/get-started:part1 myvm2 Running Preparing 37 seconds agov0lqwn40q6ui getstartedlab_web.6 lixiwei/get-started:part1 myvm2 Running Preparing 37 seconds agoz5nkfsixu6uc getstartedlab_web.7 lixiwei/get-started:part1 myvm2 Running Preparing 37 seconds agogsn6xv4jenoo getstartedlab_web.8 lixiwei/get-started:part1 myvm1 Running Preparing 37 seconds agoottug1z7sy17 getstartedlab_web.9 lixiwei/get-started:part1 myvm2 Running Preparing 37 seconds agox0zeq340z8i2 getstartedlab_web.10 lixiwei/get-started:part1 myvm2 Running Preparing 37 seconds ago 2.4.5 访问集群 查看所有虚拟机 docker-machine ls 123NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORSmyvm1 - virtualbox Running tcp://192.168.99.100:2376 v17.05.0-cemyvm2 - virtualbox Running tcp://192.168.99.101:2376 v17.05.0-ce 可以通过myvm1或者myvm2来访问app。刚才创建的app在它们之间被共享并且负载均衡。两个IP都能正常访问的原因是，swarm集群中的节点参与了入口的网络路由。 下图描述了swarm集群中节点的路由方式 2.4.6 配置APP 现在你可以使用2.3节中的方式，修改docker-compose.yml文件来扩展你的app。 修改代码以改变app的的功能。 使用 docker stack deploy 来重新部署改变的内容 向swarm集群中添加新的机器后，需要执行 docker stack deploy来让app在新的节点中运行 2.4.7 关闭appdocker-machine ssh myvm1 &quot;docker stack rm getstartedlab&quot; 2.4.8 移除swarm节点 移除worker docker-machine ssh myvm2 &quot;docker swarm leave&quot; 移除manager docker-machine ssh myvm1 &quot;docker swarm leave --force&quot; 2.4.9 小节总结本节中，我们学习了如何指定swarm集群中的节点作为manager或者worker，并在集群中部署应用。我们发现本节所使用的核心命令几乎与2.3节完全一致，只是命令作用于集群之上了。我们也看到了docker网络系统的强大，对containers的负载均衡策略，即使container位于不同的机器也可以被负载均衡所调用。 以下是本节所用命令总结 1234567891011121314docker-machine create --driver virtualbox myvm1 # Create a VM (Mac, Win7, Linux)docker-machine create -d hyperv --hyperv-virtual-switch &quot;myswitch&quot; myvm1 # Win10docker-machine env myvm1 # View basic information about your nodedocker-machine ssh myvm1 &quot;docker node ls&quot; # List the nodes in your swarmdocker-machine ssh myvm1 &quot;docker node inspect &lt;node ID&gt;&quot; # Inspect a nodedocker-machine ssh myvm1 &quot;docker swarm join-token -q worker&quot; # View join tokendocker-machine ssh myvm1 # Open an SSH session with the VM; type &quot;exit&quot; to enddocker-machine ssh myvm2 &quot;docker swarm leave&quot; # Make the worker leave the swarmdocker-machine ssh myvm1 &quot;docker swarm leave -f&quot; # Make master leave, kill swarmdocker-machine start myvm1 # Start a VM that is currently not runningdocker-machine stop $(docker-machine ls -q) # Stop all running VMsdocker-machine rm $(docker-machine ls -q) # Delete all VMs and their disk imagesdocker-machine scp docker-compose.yml myvm1:~ # Copy file to node&apos;s home dirdocker-machine ssh myvm1 &quot;docker stack deploy -c &lt;file&gt; &lt;app&gt;&quot; # Deploy an app 2.5 Stack在2.4节中，我们学会了如何建立一个swarm集群，并在集群中的多个机器上部署我们的app。 本节将学习Stack。 Stack是一组相互关联的服务，共享依赖关系，并且可以协调一致。 单个Stack能够定义和协调整个应用程序的功能（尽管非常复杂的应用程序可能希望使用多个Stack）。 我们在2.3节中其实已经使用过Stack，例如创建compose_file之后，使用docker stack deploy。但那仅是单个Stack运行于host上，在生产环境中这并不是常用的方式。 2.5.1 创建新的Service并部署在docker-compose.yml添加一个新的Service很容易。 编辑 docker-compose.yml 123456789101112131415161718192021222324252627282930version: &quot;3&quot;services: web: # replace username/repo:tag with your name and image details image: username/repo:tag deploy: replicas: 5 restart_policy: condition: on-failure resources: limits: cpus: &quot;0.1&quot; memory: 50M ports: - &quot;80:80&quot; networks: - webnet visualizer: image: dockersamples/visualizer:stable ports: - &quot;8080:8080&quot; volumes: - &quot;/var/run/docker.sock:/var/run/docker.sock&quot; deploy: placement: constraints: [node.role == manager] networks: - webnetnetworks: webnet: 与之前相比唯一新出现的就是一个对等的web服务–visualizer（可视化）。volumes: 允许visualizer访问宿主机的socket文件placement: 确保这个服务只在swarm的manager节点上运行 复制service配置文件到managerdocker-machine scp docket-compose.yml myvm1:~ 重新部署docker-machine ssh myvm1 &quot;docker stack deploy -c docker-compose.yml getstartedlab&quot; 更新services成功 12Updating service getstartedlab_web (id: 43lo6a1be3tr03bxkekt32kw5)Creating service getstartedlab_visualizer 查看可视化 正如我们之前配置的，visualizer只有一个实例，而web则在swarm集群中有很多实例。我们可以使用 docket-machine ssh myvm1 &quot;docker stack ps getstartedlab&quot;来证实其可视化结果 1234567ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTSfm0v360ndv24 getstartedlab_visualizer.1 dockersamples/visualizer:stable myvm1 Running Running 6 minutes agol7w76v6ldure getstartedlab_web.2 lixiwei/get-started:part1 myvm1 Running Running 6 hours agojei5oc8wuvqr getstartedlab_web.3 lixiwei/get-started:part1 myvm1 Running Running 6 hours agoz5nkfsixu6uc getstartedlab_web.7 lixiwei/get-started:part1 myvm2 Running Running 6 hours agogsn6xv4jenoo getstartedlab_web.8 lixiwei/get-started:part1 myvm1 Running Running 6 hours agox0zeq340z8i2 getstartedlab_web.10 lixiwei/get-started:part1 myvm2 Running Running 6 hours ago 下面我们创建一个需要有依赖的service：redis service 用来提供访问计数 2.5.2 持久化数据 继续修改 docker-compose.yml 添加redis服务 1234567891011121314151617181920212223242526272829303132333435363738394041version: &quot;3&quot;services: web: # replace username/repo:tag with your name and image details image: username/repo:tag deploy: replicas: 5 restart_policy: condition: on-failure resources: limits: cpus: &quot;0.1&quot; memory: 50M ports: - &quot;80:80&quot; networks: - webnet visualizer: image: dockersamples/visualizer:stable ports: - &quot;8080:8080&quot; volumes: - &quot;/var/run/docker.sock:/var/run/docker.sock&quot; deploy: placement: constraints: [node.role == manager] networks: - webnet redis: image: redis ports: - &quot;6379:6379&quot; volumes: - ./data:/data deploy: placement: constraints: [node.role == manager] networks: - webnetnetworks: webnet: Redis在docker中有官方的image，所以不需要提供username/repo。redis的端口6379已经在container中被预设，并且映射到了host（此时host不是我们所用的主机，而是myvm1这台虚拟机）的6379端口。更重要的是，在stack中部署和使用redis进行持久化数据时 redis只能在manager中运行，所以它永远使用同一个文件系统 redis在宿主机内任意的文件夹中所访问的文件，都将存储在/data 综上，redis将创建 “source of truth” 在你的host文件系统中用来存储data。若没有这个东西，redis将会把数据存储在container所在文件系统的/data中，也就是说如果container被重新部署了，那么redis存储的data将会被擦除。 “source of truth”由两部分构成： 确保Redis服务永远使用同一个host 创建的容器将 ./data(在host上) 作为 /data(redis container内部)访问。当containers创建或销毁，存储在主机上 ./data上的文件将被持久化，从而实现连续性。 下面准备部署使用Redis的Stack 在manager中创建 ./data目录docker-machine ssh myvm1 &quot;mkdir ./data&quot; 上传新的docker-compose.yml文件docker-machine scp docker-compose.yml myvm1:~ 再次部署服务docker-machine ssh myvm1 &quot;docker stack deploy -c docker-compose.yml getstartedlab&quot; 查看结果 2.6 部署APP","tags":[{"name":"Linux","slug":"Linux","permalink":"https://seawaylee.github.io/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://seawaylee.github.io/tags/Docker/"}]},{"title":"Hadoop学习笔记（六）- Hadoop的HA原理与配置","date":"2017-06-13T12:03:06.000Z","path":"2017/06/13/大数据/Hadoop/Hadoop学习笔记（六）- Hadoop的HA原理与配置/","text":"1 Hadoop的HA机制正式引入HA机制是从hadoop2.0开始，之前的版本中没有HA机制。 Hadoo的HA机制可以分成各个组件的HA机制 HDFS的HA YARN的HA 1.1 HDFS的HA 通过双NameNode消除单点故障 双NameNode工作要点如下： A. 元数据管理方式需要改变 双NameNode各自在内存中保存一份元数据(fsimage+edits合成结果) edits日志只能有一份，只有Active状态的NameNode节点可以对其进行写操作 双NameNode都可以读取edits 共享的edits仿造一个共享的存储介质中管理（qjournal和NFS两个主流实现） B. 需要一个状态管理功能模块 实现了一个ZKFC（ZooKeeperFailContrl），常驻在每一个NameNode所在的节点 每一个ZKFC负责监控自己所在的NameNode节点，利用ZK进行状态标识 当需要进行状态切换时，由ZKFC来负责切换（RPC调用NameNode中的方法） 切换时需要放置BrainSplit（多个NameNode都是Active状态）现象的发生（新Active节点主动发送杀死被取代Active状态的节点的命令，防止假死） 1.2 Yarn的HA 双ResourceManager消除单点故障 双RM工作要点如下： A. 直接由ZooKeeper检测双节点状态。 B. 假设Application在请求资源的过程中，当前Active状态的RM发生故障，则直接重新发送请求，并不产生严重影响。 2 HA环境搭建2.1 集群节点规划2.1.1 10节点规划 节点名称 节点服务 备注 Server01 NameNode、ZKFC start-dfs.sh Server02 NameNode、ZKFC Server03 ResourceManager Server04 ResourceManager Server05 DataNode、NodeManager start-yarn.sh Server06 DataNode、NodeManager Server07 DataNode、NodeManager Server08 JournalNode、ZooKeeper Server09 JournalNode、ZooKeeper Server10 JournalNode、ZooKeeper 2.1.2 3节点规划 节点名称 节点服务 备注 Server01 NameNode、ZKFC、ResourceManager、NodeManager、DataNode、ZooKeeper、JournalNode Server02 NameNode、ZKFC、ResourceManager、NodeManager、DataNode、ZooKeeper、JournalNode Server03 DataNode、NodeManager、ZooKeeper、JournalNode 2.2 配置文件core-site.xml 123456789101112131415161718&lt;configuration&gt; &lt;!-- 指定hdfs的nameservice为ns1 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ns1/&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop临时目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.4.1/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;weekend05:2181,weekend06:2181,weekend07:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970configuration&gt;&lt;!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 --&gt;&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ns1&lt;/value&gt;&lt;/property&gt;&lt;!-- ns1下面有两个NameNode，分别是nn1，nn2 --&gt;&lt;property&gt; &lt;name&gt;dfs.ha.namenodes.ns1&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;&lt;!-- nn1的RPC通信地址 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1.nn1&lt;/name&gt; &lt;value&gt;weekend01:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- nn1的http通信地址 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns1.nn1&lt;/name&gt; &lt;value&gt;weekend01:50070&lt;/value&gt;&lt;/property&gt;&lt;!-- nn2的RPC通信地址 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1.nn2&lt;/name&gt; &lt;value&gt;weekend02:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- nn2的http通信地址 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns1.nn2&lt;/name&gt; &lt;value&gt;weekend02:50070&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定NameNode的edits元数据在JournalNode上的存放位置 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://weekend05:8485;weekend06:8485;weekend07:8485/ns1&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;&lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.4.1/journaldata&lt;/value&gt;&lt;/property&gt;&lt;!-- 开启NameNode失败自动切换 --&gt;&lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置失败自动切换实现方式 --&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ns1&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt;&lt;/property&gt;&lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置sshfence隔离机制超时时间 --&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt;&lt;/property&gt;/configuration&gt; 3 集群运维测试3.1 DataNode动态上下线Datanode动态上下线很简单，步骤如下： 准备一台服务器，设置好环境 部署hadoop的安装包，并同步集群配置 联网上线，新datanode会自动加入集群 如果是一次增加大批datanode，还应该做集群负载重均衡 3.2 NameNode状态切换管理使用的命令上hdfs haadmin可用 hdfs haadmin –help查看所有帮助信息 可以看到，状态操作的命令示例： 查看namenode工作状态 hdfs haadmin -getServiceState nn1 将standby状态NameNode切换到active hdfs haadmin –transitionToActive nn1 将active状态NameNode切换到standby hdfs haadmin –transitionToStandby nn2 3.3 数据块的Balance 启动balancer的命令： start-balancer.sh -threshold 8 运行之后，会有Balancer进程出现。 上述命令设置了Threshold为8%，那么执行balancer命令的时候，首先统计所有DataNode的磁盘利用率的均值，然后判断如果某一个DataNode的磁盘利用率超过这个均值Threshold，那么将会把这个DataNode的block转移到磁盘利用率低的DataNode，这对于新节点的加入来说十分有用。Threshold的值为1到100之间，不显示的进行参数设置的话，默认是10。 3.4 HA下HDFS-API的变化客户端需要nameservice的配置信息，其他不变 12345678910111213141516171819202122232425/** * 如果访问的是一个ha机制的集群 * 则一定要把core-site.xml和hdfs-site.xml配置文件放在客户端程序的classpath下 * 以让客户端能够理解hdfs://ns1/中 “ns1”是一个ha机制中的namenode对——nameservice * 以及知道ns1下具体的namenode通信地址 * @author * */public class UploadFile &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); conf.set(\"fs.defaultFS\", \"hdfs://ns1/\"); FileSystem fs = FileSystem.get(new URI(\"hdfs://ns1/\"),conf,\"hadoop\"); fs.copyFromLocalFile(new Path(\"g:/eclipse-jee-luna-SR1-linux-gtk.tar.gz\"), new Path(\"hdfs://ns1/\")); fs.close(); &#125; &#125;","tags":[{"name":"大数据","slug":"大数据","permalink":"https://seawaylee.github.io/tags/大数据/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://seawaylee.github.io/tags/Hadoop/"}]},{"title":"Hadoop学习笔记（五）- MapReduce实践","date":"2017-06-12T14:52:45.000Z","path":"2017/06/12/大数据/Hadoop/Hadoop学习笔记（五）- MapReduce实践/","text":"在生产环境中，其实很少自己去写MR程序，一般都是直接在Hive上写SQL完成业务逻辑，但动手写MR程序有助于我们理解MR原理，而不是一个只会写SQL的所谓的“数据分析师”。: ) 1 流量分析排序需求： 对日志数据中的上下行流量信息汇总，并输出按照总流量倒序排序的结果。 123456789101112131415161718192021221363157985066 13726230503 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157995052 13826544101 5C-0E-8B-C7-F1-E0:CMCC 120.197.40.4 4 0 264 0 2001363157991076 13926435656 20-10-7A-28-CC-0A:CMCC 120.196.100.99 2 4 132 1512 2001363154400022 13926251106 5C-0E-8B-8B-B1-50:CMCC 120.197.40.4 4 0 240 0 2001363157993044 18211575961 94-71-AC-CD-E6-18:CMCC-EASY 120.196.100.99 iface.qiyi.com 视频网站 15 12 1527 2106 2001363157995074 84138413 5C-0E-8B-8C-E8-20:7DaysInn 120.197.40.4 122.72.52.12 20 16 4116 1432 2001363157993055 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 2001363157995033 15920133257 5C-0E-8B-C7-BA-20:CMCC 120.197.40.4 sug.so.360.cn 信息安全 20 20 3156 2936 2001363157983019 13719199419 68-A1-B7-03-07-B1:CMCC-EASY 120.196.100.82 4 0 240 0 2001363157984041 13660577991 5C-0E-8B-92-5C-20:CMCC-EASY 120.197.40.4 s19.cnzz.com 站点统计 24 9 6960 690 2001363157973098 15013685858 5C-0E-8B-C7-F7-90:CMCC 120.197.40.4 rank.ie.sogou.com 搜索引擎 28 27 3659 3538 2001363157986029 15989002119 E8-99-C4-4E-93-E0:CMCC-EASY 120.196.100.99 www.umeng.com 站点统计 3 3 1938 180 2001363157992093 13560439658 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 15 9 918 4938 2001363157986041 13480253104 5C-0E-8B-C7-FC-80:CMCC-EASY 120.197.40.4 3 3 180 180 2001363157984040 13602846565 5C-0E-8B-8B-B6-00:CMCC 120.197.40.4 2052.flash2-http.qq.com 综合门户 15 12 1938 2910 2001363157995093 13922314466 00-FD-07-A2-EC-BA:CMCC 120.196.100.82 img.qfc.cn 12 12 3008 3720 2001363157982040 13502468823 5C-0A-5B-6A-0B-D4:CMCC-EASY 120.196.100.99 y0.ifengimg.com 综合门户 57 102 7335 110349 2001363157986072 18320173382 84-25-DB-4F-10-1A:CMCC-EASY 120.196.100.99 input.shouji.sogou.com 搜索引擎 21 18 9531 2412 2001363157990043 13925057413 00-1F-64-E1-E6-9A:CMCC 120.196.100.55 t3.baidu.com 搜索引擎 69 63 11058 48243 2001363157988072 13760778710 00-FD-07-A4-7B-08:CMCC 120.196.100.82 2 2 120 120 2001363157985066 13726238888 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481 24681 2001363157993055 13560436666 C4-17-FE-BA-DE-D9:CMCC 120.196.100.99 18 15 1116 954 200 分析：基本思路：Job1：Map读取文件后输出 Reduce计算flowSum 输出到文件Job2：Map输出,Map-&gt;Reduce之间的shuffle会帮助我们对flowbean进行排序，Reduce不用作任何操作。 实现自定义的bean来封装流量信息，并将bean作为map输出的key来传输。 MR程序在处理数据的过程中会对数据排序(map输出的kv对传输到reduce之前，会排序)，排序的依据是map输出的key。 所以，我们如果要实现自己需要的排序规则，则可以考虑将排序因素放到key中，让key实现接口：WritableComparable，然后重写key的compareTo方法 代码实现 实现序列化的DO对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package hadoop.mr.flowcount;import org.apache.hadoop.io.WritableComparable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;/** * @author NikoBelic * @create 2017/5/7 22:50 */public class FlowBean implements WritableComparable&lt;FlowBean&gt;&#123; private Long upFlow; private Long downFlow; private Long sumFlow; public FlowBean() &#123; &#125; public FlowBean(Long upFlow, Long downFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = this.upFlow + this.downFlow; &#125; public Long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(Long upFlow) &#123; this.upFlow = upFlow; &#125; public Long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(Long downFlow) &#123; this.downFlow = downFlow; &#125; public Long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(Long sumFlow) &#123; this.sumFlow = sumFlow; &#125; public void set(Long upFlow, Long downFlow,Long sumFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = sumFlow; &#125; @Override public String toString() &#123; return upFlow + \" \" + downFlow + \" \" + sumFlow; &#125; @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeLong(upFlow); dataOutput.writeLong(downFlow); dataOutput.writeLong(sumFlow); &#125; @Override public void readFields(DataInput dataInput) throws IOException &#123; this.upFlow = dataInput.readLong(); this.downFlow = dataInput.readLong(); this.sumFlow = dataInput.readLong(); &#125; @Override public int compareTo(FlowBean o) &#123; return this.sumFlow &gt; o.getSumFlow() ? -1 : 1; &#125;&#125; 计算FlowSumMapReduce 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106package hadoop.mr.flowcount;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Partitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.net.URI;import java.net.URISyntaxException;import java.util.Iterator;/** * @author NikoBelic * @create 2017/5/7 22:54 */public class FlowCount&#123; public static void main(String[] args) throws IOException, URISyntaxException, InterruptedException, ClassNotFoundException &#123; String inputPath = \"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/main/java/hadoop/data/flow.txt\"; //String inputPath = \"hdfs://hadoop1:9000/flowbean/flow.txt\"; String outputPath = \"hdfs://localhost:9000/flowbean/result\"; Configuration conf = new Configuration(); //conf.set(\"mapreduce.framework.name\", \"yarn\"); //conf.set(\"yarn.resourcemanager.hostname\", \"hadoop1\"); Job job = Job.getInstance(conf ); job.setUser(\"root\"); job.setJarByClass(FlowCount.class); job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); //job.setPartitionerClass(FlowCountPartitioner.class); //job.setNumReduceTasks(4); FileInputFormat.setInputPaths(job, new Path(inputPath)); FileSystem fs = FileSystem.get(new URI(outputPath), conf, \"NikoBelic\"); if (fs.exists(new Path(outputPath))) &#123; fs.delete(new Path(outputPath)); &#125; FileOutputFormat.setOutputPath(job, new Path(outputPath)); job.waitForCompletion(true); &#125;&#125;class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] values = value.toString().split(\"\\\\t\"); String phoneNumber = values[1]; Long upFlow = Long.parseLong(values[values.length - 3]); Long downFlow = Long.parseLong(values[values.length - 2]); FlowBean flowBean = new FlowBean(upFlow, downFlow); context.write(new Text(phoneNumber), flowBean); &#125;&#125;class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException &#123; Iterator&lt;FlowBean&gt; iterator = values.iterator(); FlowBean currBean; Long totalUp = 0L; Long totalDown = 0L; while (iterator.hasNext()) &#123; currBean = iterator.next(); totalUp += currBean.getUpFlow(); totalDown += currBean.getDownFlow(); &#125; context.write(key, new FlowBean(totalUp, totalDown)); &#125;&#125;class FlowCountPartitioner extends Partitioner&lt;Text, FlowBean&gt;&#123; @Override public int getPartition(Text key, FlowBean value, int i) &#123; return key.charAt(3) % 4; &#125;&#125; 输出Sum计算结果 12345678910111213141516171819202113480253104 180 180 36013502468823 7335 110349 11768413560436666 1116 954 207013560439658 2034 5892 792613602846565 1938 2910 484813660577991 6960 690 765013719199419 240 0 24013726230503 2481 24681 2716213726238888 2481 24681 2716213760778710 120 120 24013826544101 264 0 26413922314466 3008 3720 672813925057413 11058 48243 5930113926251106 240 0 24013926435656 132 1512 164415013685858 3659 3538 719715920133257 3156 2936 609215989002119 1938 180 211818211575961 1527 2106 363318320173382 9531 2412 1194384138413 4116 1432 5548 排序MapReduce 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package hadoop.mr.flowcount;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.net.URI;import java.net.URISyntaxException;/** * @author NikoBelic * @create 2017/5/15 21:37 */public class FlowCountSort&#123; public static void main(String[] args) throws IOException, URISyntaxException, InterruptedException, ClassNotFoundException &#123; String inputPath = \"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/main/java/hadoop/data/flowsum.txt\"; String outputPath = \"hdfs://localhost:9000/flowsortbean/result\"; Configuration conf = new Configuration(); //conf.set(\"mapreduce.framework.name\", \"yarn\"); //conf.set(\"yarn.resourcemanager.hostname\", \"localhost\"); Job job = Job.getInstance(conf); job.setJarByClass(FlowCountSort.class); job.setMapperClass(FlowCountSortMapper.class); job.setReducerClass(FlowCountSortReducer.class); job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); //job.setPartitionerClass(FlowCountPartitioner.class); //job.setNumReduceTasks(4); FileInputFormat.setInputPaths(job, new Path(inputPath)); FileSystem fs = FileSystem.get(new URI(\"hdfs://localhost:9000\"), conf, \"NikoBelic\"); if (fs.exists(new Path(outputPath))) &#123; fs.delete(new Path(outputPath)); &#125; FileOutputFormat.setOutputPath(job, new Path(outputPath)); job.waitForCompletion(true); &#125;&#125;class FlowCountSortMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt;&#123; private FlowBean resultBean = new FlowBean(); private Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String values[] = value.toString().split(\" \"); Long upFlow = Long.valueOf(values[1]); Long downFlow = Long.valueOf(values[2]); Long sumFlow = Long.valueOf(values[values.length - 1]); String phoneNumber = values[0]; v.set(phoneNumber); resultBean.set(upFlow, downFlow, sumFlow); context.write(resultBean, v); &#125;&#125;class FlowCountSortReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt;&#123; @Override protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(values.iterator().next(), key); &#125;&#125; 输出排序结果 12345678910111213141516171819202113502468823 7335 110349 117684 11768413925057413 11058 48243 59301 5930113726238888 2481 24681 27162 2716213726230503 2481 24681 27162 2716218320173382 9531 2412 11943 1194313560439658 2034 5892 7926 792613660577991 6960 690 7650 765015013685858 3659 3538 7197 719713922314466 3008 3720 6728 672815920133257 3156 2936 6092 609284138413 4116 1432 5548 554813602846565 1938 2910 4848 484818211575961 1527 2106 3633 363315989002119 1938 180 2118 211813560436666 1116 954 2070 207013926435656 132 1512 1644 164413480253104 180 180 360 36013826544101 264 0 264 26413926251106 240 0 240 24013760778710 120 120 240 24013719199419 240 0 240 240 2 Join算法2.1 Reduce端的Join算法需求： 订单数据表t_order：1001,20150710,P0001,21002,20150710,P0001,31002,20150710,P0002,31003,20150710,P0003,3 商品信息表t_productP0001,小米5,1001,2P0002,锤子T1,1000,3P0003,锤子,1002,4 假如数据量巨大，两表的数据是以文件的形式存储在HDFS中，需要用mapreduce程序来实现一下SQL查询运算： select a.id,a.date,b.name,b.category_id,b.price from t_order a join t_product b on a.pid = b.id 思路 通过将关联的条件作为map输出的key，将两表满足join条件的数据并携带数据所来源的文件信息，发往同一个reduce task，在reduce中进行数据的串联。即 map=&gt;(productId,orderDO/productDO),reduce=&gt;orderDO + productDO 代码实现 实现序列化接口的DO 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140package hadoop.mr.join;import org.apache.hadoop.io.Writable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;/** * @author NikoBelic * @create 2017/5/22 21:22 */public class InfoBean implements Writable&#123; String orderId; String timestamp; String amount; String productId; String productName; String categoryId; String price; public InfoBean() &#123; &#125; public void setAll(String orderId, String timestamp, String productId, String amount, String productName, String categoryId, String price) &#123; this.orderId = orderId; this.timestamp = timestamp; this.productId = productId; this.amount = amount; this.productName = productName; this.categoryId = categoryId; this.price = price; &#125; @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeUTF(orderId); dataOutput.writeUTF(timestamp); dataOutput.writeUTF(productId); dataOutput.writeUTF(amount); dataOutput.writeUTF(productName); dataOutput.writeUTF(categoryId); dataOutput.writeUTF(price); &#125; @Override public void readFields(DataInput dataInput) throws IOException &#123; this.orderId = dataInput.readUTF(); this.timestamp = dataInput.readUTF(); this.productId = dataInput.readUTF(); this.amount = dataInput.readUTF(); this.productName = dataInput.readUTF(); this.categoryId = dataInput.readUTF(); this.price = dataInput.readUTF(); &#125; public String getOrderId() &#123; return orderId; &#125; public void setOrderId(String orderId) &#123; this.orderId = orderId; &#125; public String getTimestamp() &#123; return timestamp; &#125; public void setTimestamp(String timestamp) &#123; this.timestamp = timestamp; &#125; public String getAmount() &#123; return amount; &#125; public void setAmount(String amount) &#123; this.amount = amount; &#125; public String getProductId() &#123; return productId; &#125; public void setProductId(String productId) &#123; this.productId = productId; &#125; public String getProductName() &#123; return productName; &#125; public void setProductName(String productName) &#123; this.productName = productName; &#125; public String getCategoryId() &#123; return categoryId; &#125; public void setCategoryId(String categoryId) &#123; this.categoryId = categoryId; &#125; public String getPrice() &#123; return price; &#125; public void setPrice(String price) &#123; this.price = price; &#125; @Override public String toString() &#123; return \"InfoBean&#123;\" + \"orderId='\" + orderId + '\\'' + \", timestamp='\" + timestamp + '\\'' + \", amount='\" + amount + '\\'' + \", productId='\" + productId + '\\'' + \", productName='\" + productName + '\\'' + \", categoryId='\" + categoryId + '\\'' + \", price='\" + price + '\\'' + '&#125;'; &#125;&#125; MR 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package hadoop.mr.join;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.net.URI;import java.util.HashMap;import java.util.Map;/** * @author NikoBelic * @create 2017/5/24 09:55 */public class MapJoin&#123; public static void main(String[] args) throws Exception &#123; String inputPath = \"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/main/java/hadoop/data/join/order.txt\"; String outputPath = \"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/main/java/hadoop/data/join/result\"; Configuration conf = new Configuration(); //conf.set(\"mapreduce.framework.name\", \"yarn\"); //conf.set(\"yarn.resourcemanager.hostname\", \"hadoop1\"); Job job = Job.getInstance(conf); //job.setUser(\"NikoBelic\"); //job.setJarByClass(FlowCount.class); job.addCacheFile(new URI(\"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/main/java/hadoop/data/join/product.txt\")); job.setUser(\"NikoBelicll\"); job.setMapperClass(MapperJoin.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.setInputPaths(job, new Path(inputPath)); FileSystem fs = FileSystem.get(new URI(outputPath), conf, \"NikoBelic\"); if (fs.exists(new Path(outputPath))) &#123; fs.delete(new Path(outputPath)); &#125; FileOutputFormat.setOutputPath(job, new Path(outputPath)); job.waitForCompletion(true); &#125;&#125;class MapperJoin extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Map&lt;String, String&gt; productMap = new HashMap&lt;&gt;(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; URI[] cacheFiles = context.getCacheFiles(); for (URI cacheFile : cacheFiles) &#123; System.out.println(cacheFile.getPath()); BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(cacheFile.getPath()))); String line = null; while ((line = br.readLine()) != null) &#123; String[] fields = line.split(\",\"); productMap.put(fields[0], fields[1]); &#125; &#125; &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] fields = value.toString().split(\",\"); context.write(new Text(value.toString() + \",\" + productMap.get(fields[2])), NullWritable.get()); &#125;&#125; 输出 12341001,20150710,P0001,2,小米51002,20150710,P0001,3,小米51002,20150710,P0002,3,锤子T11003,20150710,P0003,3,锤子 3 Map端Join算法在上面的算法中，join的操作是在reduce阶段完成，reduce端的处理压力太大，map节点的运算负载则很低，资源利用率不高，且在reduce阶段极易产生数据倾斜。 解决方案：在Map端实现Map算法 原理阐述适用于关联表中有小表的情形；可以将小表分发到所有的map节点，这样，map节点就可以在本地对自己所读到的大表数据进行join并输出最终结果，可以大大提高join操作的并发度，加快处理速度 实现示例–先在mapper类中预先定义好小表，进行join–引入实际场景中的解决方案：一次加载数据库或者用distributedcache 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package hadoop.mr.join;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.net.URI;import java.util.HashMap;import java.util.Map;/** * @author NikoBelic * @create 2017/5/24 09:55 */public class MapJoin&#123; public static void main(String[] args) throws Exception &#123; String inputPath = \"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/main/java/hadoop/data/join/order.txt\"; String outputPath = \"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/main/java/hadoop/data/join/result\"; Configuration conf = new Configuration(); //conf.set(\"mapreduce.framework.name\", \"yarn\"); //conf.set(\"yarn.resourcemanager.hostname\", \"hadoop1\"); Job job = Job.getInstance(conf); //job.setUser(\"NikoBelic\"); //job.setJarByClass(FlowCount.class); job.addCacheFile(new URI(\"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/main/java/hadoop/data/join/product.txt\")); job.setUser(\"NikoBelicll\"); job.setMapperClass(MapperJoin.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.setInputPaths(job, new Path(inputPath)); FileSystem fs = FileSystem.get(new URI(outputPath), conf, \"NikoBelic\"); if (fs.exists(new Path(outputPath))) &#123; fs.delete(new Path(outputPath)); &#125; FileOutputFormat.setOutputPath(job, new Path(outputPath)); job.waitForCompletion(true); &#125;&#125;class MapperJoin extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Map&lt;String, String&gt; productMap = new HashMap&lt;&gt;(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; URI[] cacheFiles = context.getCacheFiles(); for (URI cacheFile : cacheFiles) &#123; System.out.println(cacheFile.getPath()); BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(cacheFile.getPath()))); String line = null; while ((line = br.readLine()) != null) &#123; String[] fields = line.split(\",\"); productMap.put(fields[0], fields[1]); &#125; &#125; &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] fields = value.toString().split(\",\"); context.write(new Text(value.toString() + \",\" + productMap.get(fields[2])), NullWritable.get()); &#125;&#125; 输出 12341001,20150710,P0001,2,小米51002,20150710,P0001,3,小米51002,20150710,P0002,3,锤子T11003,20150710,P0003,3,锤子 4 社交粉丝数据分析需求 以下是qq的好友列表数据，冒号前是一个用，冒号后是该用户的所有好友（数据中的好友关系是单向的）A:B,C,D,F,E,OB:A,C,E,KC:F,A,D,ID:A,E,F,LE:B,C,D,M,LF:A,B,C,D,E,O,MG:A,C,D,E,FH:A,C,D,E,OI:A,OJ:B,OK:A,C,DL:D,E,FM:E,F,GO:A,H,I,J 求出哪些人两两之间有共同好友，及他俩的共同好友都有谁？ 思路 Job1：例：数据源 A:B,C,D,F,E,O Map读取数据，输出,,….. Reduce合并数据得到 说明 ACD的共同好友都有B Job2:数据源 Map读取数据并输出 ,, 两两输出共同好友 注意排序、去重 Reduce合并结果即可 输出 …. 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128package hadoop.mr.friends;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.net.URI;import java.net.URISyntaxException;import java.util.Arrays;/** * @author NikoBelic * @create 2017/5/23 12:58 */public class CommonFriends&#123; public static void main(String[] args) throws IOException, URISyntaxException, InterruptedException, ClassNotFoundException &#123; String inputPath = \"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/main/java/hadoop/data/friends/result\"; String outputPath = \"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/main/java/hadoop/data/friends/result2\"; Configuration conf = new Configuration(); //conf.set(\"mapreduce.framework.name\", \"yarn\"); //conf.set(\"yarn.resourcemanager.hostname\", \"hadoop1\"); Job job = Job.getInstance(conf); //job.setUser(\"NikoBelic\"); //job.setJarByClass(FlowCount.class); job.setMapperClass(FriendsMapperStepTwo.class); job.setReducerClass(FriendsReducerStepOne.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.setInputPaths(job, new Path(inputPath)); FileSystem fs = FileSystem.get(new URI(outputPath), conf, \"NikoBelic\"); if (fs.exists(new Path(outputPath))) &#123; fs.delete(new Path(outputPath)); &#125; FileOutputFormat.setOutputPath(job, new Path(outputPath)); job.waitForCompletion(true); &#125;&#125;class FriendsMapperStepOne extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String host = value.toString().split(\":\")[0]; String[] friends = value.toString().split(\":\")[1].split(\",\"); for (String friend : friends) &#123; context.write(new Text(friend), new Text(host)); &#125; &#125;&#125;class FriendsReducerStepOne extends Reducer&lt;Text, Text, Text, Text&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder sb = new StringBuilder(); for (Text value : values) &#123; sb.append(value).append(\",\"); &#125; context.write(key, new Text(sb.deleteCharAt(sb.length() - 1).toString())); &#125;&#125;class FriendsMapperStepTwo extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String valueStr = value.toString(); String commonFriend = valueStr.split(\"\\t\")[0]; String[] hosts = valueStr.split(\"\\t\")[1].split(\",\"); for (int i = 0; i &lt; hosts.length - 1; i++) &#123; for (int j = i + 1; j &lt; hosts.length; j++) &#123; context.write(new Text(sortRelation(hosts[i] + \",\" + hosts[j])),new Text(commonFriend)); &#125; &#125; &#125; private String sortRelation(String relation) &#123; String[] friends = relation.split(\",\"); Arrays.sort(friends); return (friends[0] + \",\" + friends[1]).intern(); &#125;&#125;////class FriendsReducerStepTwo extends Reducer&lt;Text, Text, Text, Text&gt;//&#123;// @Override// protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException// &#123;// StringBuilder sb = new StringBuilder();// for (Text value : values)// &#123;// sb.append(value).append(\",\");// &#125;// context.write(key, new Text(sb.deleteCharAt(sb.length() - 1).toString()));// &#125;//&#125;// Job1输出结果 1234567891011121314A I,K,C,B,G,F,H,O,DB A,F,J,EC A,E,B,H,F,G,KD G,C,K,A,L,F,E,HE G,M,L,H,A,F,B,DF L,M,D,C,G,AG MH OI O,CJ OK BL D,EM E,FO A,H,I,J,F Job2输出结果 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374A,B C,EA,C D,FA,D F,EA,E B,D,CA,F C,B,O,D,EA,G F,D,E,CA,H D,O,E,CA,I OA,J O,BA,K D,CA,L E,D,FA,M F,EB,C AB,D E,AB,E CB,F E,A,CB,G C,A,EB,H E,C,AB,I AB,K C,AB,L EB,M EB,O AC,D F,AC,E DC,F D,AC,G D,F,AC,H A,DC,I AC,K D,AC,L D,FC,M FC,O I,AD,E LD,F A,ED,G E,A,FD,H E,AD,I AD,K AD,L E,FD,M E,FD,O AE,F B,M,D,CE,G D,CE,H D,CE,J BE,K D,CE,L DF,G C,D,A,EF,H A,E,O,C,DF,I O,AF,J O,BF,K A,C,DF,L E,DF,M EF,O AG,H A,D,E,CG,I AG,K C,A,DG,L F,D,EG,M F,EG,O AH,I A,OH,J OH,K A,D,CH,L D,EH,M EH,O AI,J OI,K AI,O AK,L DK,O AL,M F,E 5 倒排索引建立需求：有大量的文本（文档、网页），需要建立搜索索引 假设有三个文件a.txt 123hello tomhello jerryhello tom b.txt 123hello jerryhello jerrytom jerry c.txt 12hello jerryhello tom 要求得到key与文件名之间的索引，并按词频倒排序 思路 Job1： Map读取文件，输出&lt;关键词:文件名&gt; =&gt; ,,… Reduce合并文件 输出,,&gt;…. Job2: Map读取文件，切割、合并文件字符串 输出 Reduce合并关键词并按词频倒排序 输出 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148package hadoop.mr.reverseindex;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;import org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.net.URI;/** * @author NikoBelic * @create 2017/5/24 15:07 */public class ReverseIndex&#123; public static void main(String[] args) throws Exception &#123; String inputPath1 = \"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/main/java/hadoop/data/revers_index\"; String outputPath1 = \"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/main/java/hadoop/data/revers_index/result1\"; String inputPath2 = \"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/main/java/hadoop/data/revers_index/result1/part-r-00000\"; String outputPath2 = \"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/main/java/hadoop/data/revers_index/result2\"; org.apache.hadoop.conf.Configuration conf = new org.apache.hadoop.conf.Configuration(); //conf.set(\"mapreduce.framework.name\", \"yarn\"); //conf.set(\"yarn.resourcemanager.hostname\", \"hadoop1\"); Job job1 = Job.getInstance(conf,\"Job1\"); job1.setMapperClass(ReverseIndexMapperStepOne.class); job1.setReducerClass(ReverseIndexReducerStepOne.class); job1.setMapOutputKeyClass(Text.class); job1.setMapOutputValueClass(LongWritable.class); job1.setOutputKeyClass(Text.class); job1.setOutputValueClass(LongWritable.class); FileInputFormat.setInputPaths(job1, new Path(inputPath1)); FileSystem fs = FileSystem.get(new URI(outputPath1), conf, \"NikoBelic\"); if (fs.exists(new Path(outputPath1))) &#123; fs.delete(new Path(outputPath1)); &#125; FileOutputFormat.setOutputPath(job1, new Path(outputPath1)); //job1.waitForCompletion(true); Job job2 = Job.getInstance(conf,\"Job2\"); job2.setMapperClass(ReverseIndexMapperStepTwo.class); job2.setReducerClass(ReverseIndexReducerStepTwo.class); job2.setMapOutputKeyClass(Text.class); job2.setMapOutputValueClass(Text.class); job2.setOutputKeyClass(Text.class); job2.setOutputValueClass(Text.class); FileInputFormat.setInputPaths(job2, new Path(inputPath2)); if (fs.exists(new Path(outputPath2))) &#123; fs.delete(new Path(outputPath2)); &#125; FileOutputFormat.setOutputPath(job2, new Path(outputPath2)); job1.waitForCompletion(true); ControlledJob ctrlJob1 = new ControlledJob(conf); ctrlJob1.setJob(job1); ControlledJob ctrlJob2 = new ControlledJob(conf); ctrlJob2.setJob(job2); JobControl jobControl = new JobControl(\"MyControl\"); jobControl.addJob(ctrlJob1); jobControl.addJob(ctrlJob2); jobControl.run(); System.out.println(jobControl.getFailedJobList()); jobControl.stop(); &#125; static class ReverseIndexMapperStepOne extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; FileSplit fileSplit = (FileSplit) context.getInputSplit(); String[] fields = value.toString().split(\" \"); String fileName = fileSplit.getPath().getName(); for (String field : fields) &#123; context.write(new Text(field + \"--\" + fileName), new LongWritable(1L)); &#125; &#125; &#125; static class ReverseIndexReducerStepOne extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123; Long totalCount = 0L; for (LongWritable value : values) &#123; totalCount += value.get(); &#125; context.write(key, new LongWritable(totalCount)); &#125; &#125; static class ReverseIndexMapperStepTwo extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] fields = value.toString().split(\"--\"); String word = fields[0]; String freq = fields[1]; context.write(new Text(word),new Text(freq)); &#125; &#125; static class ReverseIndexReducerStepTwo extends Reducer&lt;Text, Text, Text, Text&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuilder sb = new StringBuilder(); for (Text value : values) &#123; sb.append(value).append(\" \"); &#125; context.write(key, new Text(sb.toString())); &#125; &#125;&#125; Job1输出 123456789hello--a.txt 3hello--b.txt 2hello--c.txt 2jerry--a.txt 1jerry--b.txt 3jerry--c.txt 1tom--a.txt 2tom--b.txt 1tom--c.txt 1 Job2输出 123hello c.txt 2 b.txt 2 a.txt 3 jerry c.txt 1 b.txt 3 a.txt 1 tom c.txt 1 b.txt 1 a.txt 2 6 最大订单金额有如下订单数据(订单号、商品id、订单金额)1234567Order_0000001,Pdt_01,222.8Order_0000001,Pdt_05,25.8Order_0000002,Pdt_05,325.8Order_0000002,Pdt_03,522.8Order_0000002,Pdt_04,122.4Order_0000003,Pdt_01,222.8Order_0000003,Pdt_01,322.8 现在需要求出每一个订单中成交金额最大的一笔交易 传统方法:Map输出 Reduce只输出最大金额的订单信息 新方法:自定义GroupingComparator 1、利用“订单id和成交金额”作为key，可以将map阶段读取到的所有订单数据按照id分区，按照金额排序，发送到reduce2、在reduce端利用groupingcomparator将订单id相同的kv聚合成组，然后取第一个即是最大值 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189package hadoop.mr.orders;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.*;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Partitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import java.net.URI;/** * @author NikoBelic * @create 2017/6/4 22:18 */public class Orders&#123; public static void main(String[] args) throws Exception &#123; String inputPath = \"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/main/java/hadoop/data/orders\"; String outputPath = \"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/main/java/hadoop/data/orders/result\"; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); //job.setUser(\"NikoBelic\"); //job.setJarByClass(FlowCount.class); job.setMapperClass(OrdersMapper.class); job.setReducerClass(OrdersReducer.class); job.setPartitionerClass(OrdersPartitioner.class); job.setNumReduceTasks(3); job.setMapOutputKeyClass(OrderDO.class); job.setMapOutputValueClass(NullWritable.class); job.setOutputKeyClass(OrderDO.class); job.setOutputValueClass(NullWritable.class); job.setGroupingComparatorClass(OrderIdGroupingComparator.class); FileInputFormat.setInputPaths(job, new Path(inputPath)); FileSystem fs = FileSystem.get(new URI(outputPath), conf, \"NikoBelic\"); if (fs.exists(new Path(outputPath))) &#123; fs.delete(new Path(outputPath)); &#125; FileOutputFormat.setOutputPath(job, new Path(outputPath)); job.waitForCompletion(true); &#125;&#125;class OrdersMapper extends Mapper&lt;LongWritable, Text, OrderDO, NullWritable&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] fields = value.toString().split(\",\"); context.write(new OrderDO(fields[0], fields[1], Double.valueOf(fields[2])), NullWritable.get()); &#125;&#125;class OrdersPartitioner extends Partitioner&lt;OrderDO, NullWritable&gt;&#123; @Override public int getPartition(OrderDO orderDO, NullWritable nullWritable, int i) &#123; System.out.println((orderDO.getOrderNo().hashCode() &amp; Integer.MAX_VALUE) % i); return (orderDO.getOrderNo().hashCode() &amp; Integer.MAX_VALUE) % i; &#125;&#125;class OrdersReducer extends Reducer&lt;OrderDO, NullWritable, OrderDO, NullWritable&gt;&#123; @Override protected void reduce(OrderDO key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, NullWritable.get()); &#125;&#125;class OrderIdGroupingComparator extends WritableComparator&#123; public OrderIdGroupingComparator() &#123; super(OrderDO.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; OrderDO o1 = (OrderDO) a; OrderDO o2 = (OrderDO) b; return o1.getOrderNo().compareTo(o2.getOrderNo()); &#125;&#125;class OrderDO implements WritableComparable&lt;OrderDO&gt;&#123; private String orderNo; private String productName; private Double price; public OrderDO() &#123; &#125; public OrderDO(String orderNo, String productName, Double price) &#123; this.orderNo = orderNo; this.productName = productName; this.price = price; &#125; public String getOrderNo() &#123; return orderNo; &#125; public void setOrderNo(String orderNo) &#123; this.orderNo = orderNo; &#125; public String getProductName() &#123; return productName; &#125; public void setProductName(String productName) &#123; this.productName = productName; &#125; public Double getPrice() &#123; return price; &#125; public void setPrice(Double price) &#123; this.price = price; &#125; @Override public int compareTo(OrderDO o) &#123; if (this.orderNo.compareTo(o.getOrderNo()) == 0) &#123; return -this.price.compareTo(o.getPrice()); &#125; else &#123; return this.orderNo.compareTo(o.getOrderNo()); &#125; &#125; @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeUTF(this.orderNo); dataOutput.writeUTF(this.productName); dataOutput.writeDouble(this.price); &#125; @Override public void readFields(DataInput dataInput) throws IOException &#123; this.orderNo = dataInput.readUTF(); this.productName = dataInput.readUTF(); this.price = dataInput.readDouble(); &#125; @Override public String toString() &#123; return \"OrderDO&#123;\" + \"orderNo='\" + orderNo + '\\'' + \", productName='\" + productName + '\\'' + \", price=\" + price + '&#125;'; &#125;&#125; 输出 三个文件 1OrderDO&#123;orderNo=&apos;Order_0000001&apos;, productName=&apos;Pdt_01&apos;, price=222.8&#125; 1OrderDO&#123;orderNo=&apos;Order_0000002&apos;, productName=&apos;Pdt_03&apos;, price=522.8&#125; 1OrderDO&#123;orderNo=&apos;Order_0000003&apos;, productName=&apos;Pdt_01&apos;, price=322.8&#125;","tags":[{"name":"大数据","slug":"大数据","permalink":"https://seawaylee.github.io/tags/大数据/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://seawaylee.github.io/tags/Hadoop/"}]},{"title":"Hadoop学习笔记（四）- MapReduce运行剖析","date":"2017-06-12T12:00:26.000Z","path":"2017/06/12/大数据/Hadoop/Hadoop学习笔记（四）- MapReduce运行剖析/","text":"1 MapReduce中的Shuffle机制1.1 Shuffle概述MR中，Map阶段处理的数据如何传递给Reduce阶段是MR框架中最关键的一个流程–Shuffle。Shuffle的核心机制：数据分区、排序、缓存具体来说就是将MapTask输出的处理结果分发给ReduceTask，并在分发的过程中对数据按key进行了分区和排序。 Shuffle缓存流程 1.2 Shuffle过程Shuffle是MR处理流程中的一个过程，它的每一个处理步骤是分散在各个MapTask和ReduceTask节点上完成的，整体来看，分为3个操作： 分区partition Sort根据key排序 Combiner进行局部value的合并 详细流程 MapTask收集map方法输出的kv对，放到内存缓冲区中 从内从缓冲区中不断溢出到本地磁盘文件，可能会溢出多个文件 多个溢出文件会被合并成大的溢出文件 在溢出及合并的过程中，都要调用partitioner进行分组和针对key进行排序（快速排序） ReduceTask根据自己的分区号，去各个Maptask机器上取响应的结果分区数据 ReduceTask会取到同一个分区的、来自不同MapTask的结果文件，并将这些文件再进行合并（归并排序） 合并成大文件后，shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中逐一取出K-VGroup，调用用户自定义的reduce方法） Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘的IO次数越少，执行速度就越快。缓冲区的大小可以通过参数io.sort.mb调整，默认为100M。 2 MapReduce与Yarn2.1 Yarn概述Yarn是一个资源调度平台，负责为运算程序提供服务器的运算资源，相当于一个分布式的OS平台，而MapReduce等运算程序则相当于运行于OS之上的应用程序。 yarn并不清楚用户提交程序的运行机制 yarn只提供运算资源的调度（用户向yarn申请资源，yarn就负责分配资源） yarn中的主管角色叫ResourceManager yarn中具体提供运算资源的角色叫NodeManager yarn与运行的用户程序完全解耦，这就意味着yarn上可以运行各种类型的分布式运算程序，例如MR、Storm、Spark、Tez…. 所以Spark、Storm等运算框架都可以整合在yarn上运行，只要他们各自的框架中有符合yarn规范的资源请求机制即可。 yarn就成为一个通用的资源调度平台，从此，企业中以前存在在的各种运算集群都可以整合在一个物理集群上，提高资源利用率，方便数据共享。 3 MR运算全流程图","tags":[{"name":"大数据","slug":"大数据","permalink":"https://seawaylee.github.io/tags/大数据/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://seawaylee.github.io/tags/Hadoop/"}]},{"title":"InfluxDB学习笔记（一）- InfluxDB简介","date":"2017-06-12T04:36:28.000Z","path":"2017/06/12/Nosql/InfluxDB简介/","text":"1 简介InfluxDB 是一个开源分布式时序、事件和指标数据库。使用 Go 语言编写，无需外部依赖。其设计目标是实现分布式和水平伸缩扩展。它有三大特性： Time Series （时间序列）：你可以使用与时间有关的相关函数（如最大，最小，求和等） Metrics（度量）：你可以实时对大量数据进行计算 Eevents（事件）：它支持任意的事件数据 特点 schemaless(无结构)，可以是任意数量的列 Scalable min, max, sum, count, mean, median 一系列函数，方便统计 Native HTTP API, 内置http支持，使用http读写 Powerful Query Language 类似sql Built-in Explorer 自带管理工具 2 CLI(Command Line Interface)操作2.1 安装 启动Install: brew install influxdbStart: influxd runConnect: influx 2.2 CLI 帮助信息 123456789101112131415161718192021Usage: connect &lt;host:port&gt; connects to another node specified by host:port auth prompts for username and password pretty toggles pretty print for the json format use &lt;db_name&gt; sets current database format &lt;format&gt; specifies the format of the server responses: json, csv, or column precision &lt;format&gt; specifies the format of the timestamp: rfc3339, h, m, s, ms, u or ns consistency &lt;level&gt; sets write consistency level: any, one, quorum, or all history displays command history settings outputs the current settings for the shell clear clears settings such as database or retention policy. run &apos;clear&apos; for help exit/quit/ctrl+d quits the influx shell show databases show database names show series show series information show measurements show measurement information show tag keys show tag key information show field keys show field key information A full list of influxql commands can be found at: https://docs.influxdata.com/influxdb/latest/query_language/spec/ 创建数据库 create database rhinotech 显示所有DB show databases 123456&gt; show databasesname: databasesname----_internalrhinotech 使用数据库 use rhinotech 写数据 insert students,name=NikoBelic,age=18,region=china value=888,shit=999 Now that we have a database, InfluxDB is ready to accept queries and writes. First, a short primer on the datastore. Data in InfluxDB is organized by “time series”, which contain a measured value, like “cpu_load” or “temperature”. Time series have zero to many points, one for each discrete sample of the metric. Points consist of time (a timestamp), a measurement (“cpu_load”, for example), at least one key-value field (the measured value itself, e.g. “value=0.64”, or “temperature=21.2”), and zero to many key-value tags containing any metadata about the value (e.g. “host=server01”, “region=EMEA”, “dc=Frankfurt”). Conceptually you can think of a measurement as an SQL table, where the primary index is always time. tags and fields are effectively columns in the table. tags are indexed, and fields are not. The difference is that, with InfluxDB, you can have millions of measurements, you don’t have to define schemas up-front, and null values aren’t stored. 时序库与关系库相关名词对应： 时序库 关系库 备注 time id 主键 measurement table 表 tags column + index 带索引的列 fields column 普通列 tags和fields差不多，tags自带索引，因此查询时候更快。 查询数 select * from students 123456&gt; select * from studentsname: studentstime age name region shit value---- --- ---- ------ ---- -----1494490902239708732 18 NikoBelic china 8881494492099547889163 18 NikoBelic china 999 888 3 HTTP 读写数据3.1 Writing Data With Http API 创建数据库 curl -i -XPOST http://localhost:8086/query --data-urlencode &quot;q=CREATE DATABASE mydb&quot; 写数据库 curl -i -XPOST &#39;http://localhost:8086/write?db=mydb&#39; --data-binary &#39;cpu_load_info,host=server1,region=A3-1-1 load=0.64&#39; 从文件导入数据 文件格式 123cpu_load_short, host=server02 value=0.67cpu_load_short, host=server02, region=us-west value=0.55 1422568543702900257cpu_load_short, direction=in,host=server01,region=us-west value=2.0 1422568543702900257 导入 curl -i -XPOST &#39;http://localhost:8086/write?db=mydb&#39; --data-binary @cpu_data.txt 注意 如果文件中的数据数量超过5000个，建议切分成多个文件批量导入，因为http请求的默认超时时间为5s，虽然请求超时后influxdb仍然会继续导入数据，但是客户端将无法收到导入成功的提示信息。 3.2 Querying Data With Http API 查询 curl -G &#39;http://localhost:8086/query?pretty=true&#39; --data-urlencode &quot;db=mydb&quot; --data-urlencode &quot;q=SELECT * FROM \\&quot;cpu_load_info\\&quot;&quot; 结果 1234567891011121314151617181920212223242526272829303132333435&#123; \"results\": [ &#123; \"statement_id\": 0, \"series\": [ &#123; \"name\": \"cpu_load_info\", \"columns\": [ \"time\", \"host\", \"load\", \"region\", \"size\" ], \"values\": [ [ \"2017-05-13T05:28:52.323145124Z\", \"server1\", 0.64, \"A3-1-1\", null ], [ \"2017-05-13T05:55:47.007680205Z\", null, 0.64, \"A3-1-1\", \"120\" ] ] &#125; ] &#125; ]&#125; 使用 --data-urlencode &quot;epoch=h&quot; 可以控制显示的时间戳格式[h,m,s,ms,u,ns]","tags":[{"name":"Nosql","slug":"Nosql","permalink":"https://seawaylee.github.io/tags/Nosql/"}]},{"title":"Hadoop学习笔记（三）- MapReduce详解","date":"2017-05-12T05:55:17.000Z","path":"2017/05/12/大数据/Hadoop/Hadoop学习笔记（三）- MapReduce详解/","text":"1 MapReduce背景及原理1.1 背景Mapreduce是一个分布式运算程序的编程框架，是用户开发“基于hadoop的数据分析应用”的核心框架；Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上； Why MapReduce? （1）海量数据在单机上处理因为硬件资源限制，无法胜任 （2）而一旦将单机版程序扩展到集群来分布式运行，将极大增加程序的复杂度和开发难度 （3）引入mapreduce框架后，开发人员可以将绝大部分工作集中在业务逻辑的开发上，而将分布式计算中的复杂性交由框架来处理 1.2 MR框架结构核心及运行机制1.2.1 结构一个完整的mapreduce程序在分布式运行时有三类实例进程： 1、MRAppMaster：负责整个程序的过程调度及状态协调 2、mapTask：负责map阶段的整个数据处理流程 3、ReduceTask：负责reduce阶段的整个数据处理流程 流程解析 一个mr程序启动的时候，最先启动的是MRAppMaster，MRAppMaster启动后根据本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相应数量的maptask进程 maptask进程启动之后，根据给定的数据切片范围进行数据处理，主体流程为： 利用客户指定的inputformat来获取RecordReader读取数据，形成输入KV对 将输入KV对传递给客户定义的map()方法，做逻辑运算，并将map()方法输出的KV对收集到缓存 将缓存中的KV对按照K分区排序后不断溢写到磁盘文件 MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动相应数量的reducetask进程，并告知reducetask进程要处理的数据范围（数据分区） Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若干台maptask运行所在机器上获取到若干个maptask输出结果文件，并在本地进行重新归并排序，然后按照相同key的KV为一个组，调用客户定义的reduce()方法进行逻辑运算，并收集运算输出的结果KV，然后调用客户指定的outputformat将结果数据输出到外部存储 1.2.2 MapTask并行度的决定机制一个job的map阶段并行度由客户端在提交job时决定，而客户端对map阶段并行度的规划的基本逻辑为： 将待处理数据执行逻辑切片（即按照一个特定切片大小，将待处理数据划分成逻辑上的多个split），然后每一个split分配一个mapTask并行实例处理这段逻辑及形成的切片规划描述文件，由FileInputFormat实现类的getSplits()方法完成，其过程如下图： 1.2.3 FileInputFormat切片机制切片定义在InputFormat类中的getSplit()方法 FileInputFormat中默认的切片机制 简单地按照文件的内容长度进行切片 切片大小，默认等于block大小 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片 比如待处理数据有两个文件：file1.txt 320Mfile2.txt 10M 经过FileInputFormat的切片机制运算后，形成的切片信息如下：file1.txt.split1– 0~128file1.txt.split2– 128~256file1.txt.split3– 256~320file2.txt.split1– 0~10M FileIputFormat中切片大小的参数配置 通过分析源码，在FileInputFormat中，计算切片大小的逻辑：Math.max(minSize, Math.min(maxSize, blockSize)); 切片主要由这几个值来运算决定 1234minsize：默认值：1 配置参数： mapreduce.input.fileinputformat.split.minsize maxsize：默认值：Long.MAXValue 配置参数：mapreduce.input.fileinputformat.split.maxsize 因此，默认情况下，切片大小=blocksize maxsize（切片最大值）：参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值minsize （切片最小值）：参数调的比blockSize大，则可以让切片变得比blocksize还大 选择并发数的影响因素： 运算节点的硬件配置 运算任务的类型：CPU密集型还是IO密集型 运算任务的数据量 1.2.4 ReduceTask并行度的决定reducetask的并行度同样影响整个job的执行并发度和执行效率，但与maptask的并发数由切片数决定不同，Reducetask数量的决定是可以直接手动设置： //默认值是1，手动设置为4job.setNumReduceTasks(4); 如果数据分布不均匀，就有可能在reduce阶段产生数据倾斜注意： reducetask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个reducetask 尽量不要运行太多的reduce task。对大多数job来说，最好reduce的个数最多和集群中的reduce持平，或者比集群的 reduce slots小。这个对于小集群而言，尤其重要。 1.3 MR程序运行模式1.3.1 本地运行模式 mapreduce程序是被提交给LocalJobRunner在本地以单进程的形式运行 而处理的数据及输出结果可以在本地文件系统，也可以在hdfs上 怎样实现本地运行？写一个程序，不要带集群的配置文件（本质是你的mr程序的conf中是否有mapreduce.framework.name=local以及yarn.resourcemanager.hostname参数） 本地模式非常便于进行业务逻辑的debug，只要在eclipse中打断点即可 1.3.2 集群运行模式 将mapreduce程序提交给yarn集群resourcemanager，分发到很多的节点上并发执行 处理的数据和输出结果应该位于hdfs文件系统 提交集群的实现步骤： A、将程序打成JAR包，然后在集群的任意一个节点上用hadoop命令启动$ hadoop jar wordcount.jar cn.itcast.bigdata.mrsimple.WordCountDriver inputpath outputpath B、直接在linux的eclipse中运行main方法（项目中要带参数：mapreduce.framework.name=yarn以及yarn的两个基本配置） C、如果要在windows的eclipse中提交job给集群，则要修改YarnRunner类 1.4 Combiner combiner是MR程序中Mapper和Reducer之外的一种组件 combiner组件的父类就是Reducer combiner和reducer的区别在于运行的位置： Combiner是在每一个maptask所在的节点运行 Reducer是接收全局所有Mapper的输出结果； combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量具体实现步骤： 自定义一个combiner继承Reducer，重写reduce方法 在job中设置： job.setCombinerClass(CustomCombiner.class) combiner能够应用的前提是不能影响最终的业务逻辑。而且，combiner的输出kv应该跟reducer的输入kv类型要对应起来 1.5 PartitionerMapreduce中会将mapTask输出的kv对，按照相同key分组，然后分发给不同的reduceTask默认的分发规则为：根据key的hashcode%reducetask数来分发组件Partitioner能够改写数据分发（分组） 1.6 GroupingComparator自定义GroupingComparator可以将Map输出的K-V按照自己的需求进行聚合 2 MR 编程实践2.1 编程规范 用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端) Mapper的输入数据是KV对的形式（KV的类型可自定义） Mapper的输出数据是KV对的形式（KV的类型可自定义） Mapper中的业务逻辑写在map()方法中 map()方法（maptask进程）对每一个调用一次 Reducer的输入数据类型对应Mapper的输出数据类型，也是KV Reducer的业务逻辑写在reduce()方法中 ReduceTask进程对每一组相同k的组调用一次reduce()方法 用户自定义的Mapper和Reducer都要继承各自的父类 整个程序需要一个Drvier来进行提交，提交的是一个描述了各种必要信息的job对象 2.2 WordCount示例代码Mapper 1234567891011121314151617181920212223242526272829303132333435/** * KEYIN: 默认情况下，是MR框架锁读到的一行文本的起始偏移量，Long * VALUEIN: 默认情况下，是MR框架所读到一行文本的内容，String * KEYOUT: 是用户自定义逻辑处理完成后输出数据中的Key，在此处是单词，String * VALUEOUT: 是用户自定义逻辑处理完成后输出数据中的Value，在此处是单词次数，Integer * * 在Hadoop中有更精简的序列化接口（纯粹的数据），所以不直接用Java中的基本数据类型对象（虽然支持序列化，但是有很多冗余信息例如继承结构），而用LongWritable * @author NikoBelic * @create 2017/5/3 09:16 */public class WordCountMapper extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt;&#123; /** * Map阶段的业务逻辑 * MapTask会对每一行输入数据调用一次此方法 * @Author SeawayLee * @Date 2017/05/03 09:29 */ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 将文本内容转换成字符串 String line = value.toString(); // 对一行文本切分成单词数组 String[] words = line.split(\" \"); // 将单词输出为&lt;word,1&gt; for (String word : words) &#123; // 将单词作为key，将次数1作为value，以便于后续的数据分发 // 可以根据单词分发，以便于相同的单词会到相同的ReduceTask中 context.write(new Text(word), new IntWritable(1)); &#125; System.out.println(\"MapTask正在处理第 \" + key.toString() + \" 行文本,ObjHashCode:\" + this.hashCode() + \",IP:\" + Inet4Address.getLocalHost().getHostAddress()); &#125;&#125; Reducer 1234567891011121314151617181920212223242526272829303132/** * KEYIN,VALUEIN 对应 Mapper输出的KEYOUT,VALUEOUT * KEYOUT,VALUEOUT 是自动以Reduce逻辑处理结果的输出数据类型 * KEYOUT是单词，VALUEOUT是总次数 * @author NikoBelic * @create 2017/5/3 09:49 */public class WordCountReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt;&#123; /** * ReduceTask处理逻辑 * &lt;Niko,1&gt; &lt;Niko,1&gt; &lt;Niko,1&gt; &lt;Niko,1&gt; * &lt;Belic,1&gt; &lt;Belic,1&gt; &lt;Belic,1&gt; &lt;Belic,1&gt; * @param key 一组相同单词kv的key * @param values * * @Author SeawayLee * @Date 2017/05/03 09:52 */ @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0; for (IntWritable value : values) &#123; count += value.get(); &#125; context.write(key, new IntWritable(count)); System.out.println(\"ReduceTask正在处理第 \" + key.toString() + \" 行文本,ObjHashCode:\" + this.hashCode() + \",IP:\" + Inet4Address.getLocalHost().getHostAddress()); &#125;&#125; Driver 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * Driver相当于Yarn集群的客户端 * 需要在此封装MR程序的相关运行参数，指定jar包 * 最后提交给Yarn * * @author NikoBelic * @create 2017/5/3 * 10:00 */public class WordCountDriver&#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException, URISyntaxException &#123; Configuration conf = new Configuration(); conf.set(\"mapreduce.framework.name\", \"yarn\"); conf.set(\"yarn.resourcemanager.hostname\", \"10.5.151.241\"); //conf.set(\"mapred.jar\", \"hadoop.jar\"); Job job = Job.getInstance(conf); //job.setJar(\"/home/hadoop/wc.jar\"); // 指定本程序的jar包所在的本地路径，需要提交给Yarn // setJar方法传入jar包绝对路径，必须将jar部署到指定位置，很不灵活 // 而使用setJarByClass，可以从JVM中找到该类，比较常用 job.setJarByClass(WordCountDriver.class); job.setJar(\"hadoop.jar\"); // 指定本业务job要使用的Map和Reduce业务类 job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); // 指定Mapper输数据的KV类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 指定最终输出数据的KV类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 指定job的输入源 FileInputFormat.setInputPaths(job, new Path(args[0])); // 指定job的输出结构存储位置 FileSystem fs = FileSystem.get(new URI(\"hdfs://hadoop1:9000\"), conf, \"root\"); if (fs.exists(new Path(args[1]))) &#123; fs.delete(new Path(args[1]), true); &#125; FileOutputFormat.setOutputPath(job, new Path(args[1])); // 将job中配置的相关参数，以及job所用的java类所在的jar包提交给yarn去运行 //job.submit(); // 以阻塞的方式提交job，便于查看job处理进度 boolean res = job.waitForCompletion(true); System.exit(res ? 0 : 1); &#125;&#125; 3 MapReduce参数优化3.1 资源相关参数以下参数是在用户自己的mr应用程序中配置就可以生效 (1) mapreduce.map.memory.mb: 一个Map Task可使用的资源上限（单位:MB），默认为1024。如果Map Task实际使用的资源量超过该值，则会被强制杀死。 (2) mapreduce.reduce.memory.mb: 一个Reduce Task可使用的资源上限（单位:MB），默认为1024。如果Reduce Task实际使用的资源量超过该值，则会被强制杀死。 (3) mapreduce.map.java.opts: Map Task的JVM参数，你可以在此配置默认的java heap size等参数, e.g. “-Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc” （@taskid@会被Hadoop框架自动换为相应的taskid）, 默认值: “” (4) mapreduce.reduce.java.opts: Reduce Task的JVM参数，你可以在此配置默认的java heap size等参数, e.g. “-Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc”, 默认值: “” (5) mapreduce.map.cpu.vcores: 每个Map task可使用的最多cpu core数目, 默认值: 1 (6) mapreduce.reduce.cpu.vcores: 每个Reduce task可使用的最多cpu core数目, 默认值: 1 以下在yarn启动之前就配置在服务器的配置文件中才能生效 (7) yarn.scheduler.minimum-allocation-mb 1024 给应用程序container分配的最小内存 (8) yarn.scheduler.maximum-allocation-mb 8192 给应用程序container分配的最大内存 (9) yarn.scheduler.minimum-allocation-vcores 1 (10)yarn.scheduler.maximum-allocation-vcores 32 (11)yarn.nodemanager.resource.memory-mb 8192 shuffle性能优化的关键参数，应在yarn启动之前就配置好 mapreduce.task.io.sort.mb 100 //shuffle的环形缓冲区大小，默认100mmapreduce.map.sort.spill.percent 0.8 //环形缓冲区溢出的阈值，默认80% 3.2 容错相关参数 (1) mapreduce.map.maxattempts: 每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。 (2) mapreduce.reduce.maxattempts: 每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。 (3) mapreduce.map.failures.maxpercent: 当失败的Map Task失败比例超过该值为，整个作业则失败，默认值为0. 如果你的应用程序允许丢弃部分输入数据，则该该值设为一个大于0的值，比如5，表示如果有低于5%的Map Task失败（如果一个Map Task重试次数超过mapreduce.map.maxattempts，则认为这个Map Task失败，其对应的输入数据将不会产生任何结果），整个作业扔认为成功。 (4) mapreduce.reduce.failures.maxpercent: 当失败的Reduce Task失败比例超过该值为，整个作业则失败，默认值为0. (5) mapreduce.task.timeout: Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该task处于block状态，可能是卡住了，也许永远会卡主，为了防止因为用户程序永远block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是300000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。 3.3 本地运行mapreduce 作业设置以下几个参数:mapreduce.framework.name=localmapreduce.jobtracker.address=localfs.defaultFS=local 3.4 效率和稳定性相关参数 (1) mapreduce.map.speculative: 是否为Map Task打开推测执行机制，默认为false (2) mapreduce.reduce.speculative: 是否为Reduce Task打开推测执行机制，默认为false (3) mapreduce.job.user.classpath.first &amp; mapreduce.task.classpath.user.precedence：当同一个class同时出现在用户jar包和hadoop jar中时，优先使用哪个jar包中的class，默认为false，表示优先使用hadoop jar中的class。 (4) mapreduce.input.fileinputformat.split.minsize: FileInputFormat做切片时的最小切片大小，(5)mapreduce.input.fileinputformat.split.maxsize: FileInputFormat做切片时的最大切片大小(切片的默认大小就等于blocksize，即 134217728) 4 MR编程小结MR在编程的时候，基本上一个固化的模式，没有太多可灵活改变的地方，除了以下几处： 1、输入数据接口：InputFormat —&gt; FileInputFormat(文件类型数据读取的通用抽象类) DBInputFormat （数据库数据读取的通用抽象类） 默认使用的实现类是： TextInputFormat job.setInputFormatClass(TextInputFormat.class) TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回 2、逻辑处理接口： Mapper 完全需要用户自己去实现其中 map() setup() clean() 3、Map输出的结果在shuffle阶段会被partition以及sort，此处有两个接口可自定义： Partitioner 有默认实现 HashPartitioner，逻辑是 根据key和numReduces来返回一个分区号； key.hashCode()&amp;Integer.MAXVALUE % numReduces通常情况下，用默认的这个HashPartitioner就可以，如果业务上有特别的需求，可以自定义 Comparable 当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，override其中的compareTo()方法 4、Reduce端的数据分组比较接口 ： Groupingcomparator reduceTask拿到输入数据（一个partition的所有数据）后，首先需要对数据进行分组，其分组的默认原则是key相同，然后对每一组kv数据调用一次reduce()方法，并且将这一组kv中的第一个kv的key作为参数传给reduce的key，将这一组数据的value的迭代器传给reduce()的values参数 利用上述这个机制，我们可以实现一个高效的分组取最大值的逻辑： 自定义一个bean对象用来封装我们的数据，然后改写其compareTo方法产生倒序排序的效果 然后自定义一个Groupingcomparator，将bean对象的分组逻辑改成按照我们的业务分组id来分组（比如订单号）这样，我们要取的最大值就是reduce()方法中传进来key 5、逻辑处理接口：Reducer 完全需要用户自己去实现其中 reduce() setup() clean() 6、输出数据接口： OutputFormat —&gt; 有一系列子类 FileOutputformat DBoutputFormat ….. 默认实现类是TextOutputFormat，功能逻辑是： 将每一个KV对向目标文本文件中输出为一行","tags":[{"name":"大数据","slug":"大数据","permalink":"https://seawaylee.github.io/tags/大数据/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://seawaylee.github.io/tags/Hadoop/"}]},{"title":"Hadoop学习笔记（二） - HDFS详解","date":"2017-05-02T11:14:03.000Z","path":"2017/05/02/大数据/Hadoop/Hadoop学习笔记（二）- HDFS详解/","text":"HDFS详解1 HDFS 基本概念1.1 前言 设计思想 分而治之：将大文件、大批量文件，分布式存放在大量服务器上，以便于采取分而治之的方式对海量数据进行运算分析； 在大数据系统中作用 为各类分布式运算框架（如：mapreduce，spark，tez，……）提供数据存储服务 重点概念 文件切块，副本存放，元数据 1.2 HDFS概念和特性首先，它是一个文件系统，用于存储文件，通过统一的命名空间——目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色； 重要特性如下： HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M HDFS文件系统会给客户端提供一个统一的抽象目录树，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data 目录结构及文件分块信息(元数据)的管理由namenode节点承担。namenode是HDFS集群主节点，负责维护整个hdfs文件系统的目录树，以及每一个路径（文件）所对应的block块信息（block的id，及所在的datanode服务器） 文件的各个block的存储管理由datanode节点承担。datanode是HDFS集群从节点，每一个block都可以在多个datanode上存储多个副本（副本数量也可以通过参数设置dfs.replication） HDFS是设计成适应一次写入，多次读出的场景，且不支持文件的修改(注：适合用来做数据分析，并不适合用来做网盘应用，因为，不便修改，延迟大，网络开销大，成本太高) 1.3 基本操作 命令 功能 示例 -help 输出命令参数手册 -ls 显示目录信息 hadoop fs -ls / -mkdir 创建目录 hadoop fs -mkdir -p /aaa/bb/cc/dd -moveFromLocal 从本地剪切到hdfs hadoop fs -moveFromLocal /home/hadoop/1.txt /aaa/bb/cc/dd -moveToLocal 从hdfs剪切到本地 hadoop fs -moveToLocal /aaa/bb/cc/dd /home/hadoop/1.txt -appendToFile 追加一个文件到已经存在的文件末尾 hadoop fs -appendToFile ./hello.txt /hello.txt -cat 查看文件内容 hadoop fs -cat /hello.txt -tail 显示一个文件的末尾 hadoop fs -tail /access_log.1 -text 以字符形式打印一个文件的内容 hadoop fs -text /access_log.1 -chgrp/-chmod/-chown 修改文件权限 hadoop fs -chmod 755 /hello.txt -copyFromLocal 从本地文件系统中拷贝文件到hdfs路径去 -copyToLocal 从hdfs拷贝到本地 -cp 从hdfs的一个路径拷贝到hdfs另一个路径 -mv 在hdfs中移动文件 -get 等同于copyToLocal -put 等同于copyFromLocal -getmerge 合并下载多个文件 -rm 删除文件或文件夹 hadoop fs -rm -r /aaa/bbb -rmdir 删除空目录 hadoop fs -rmdir /aaa/bbb/ccc -df 统计问价那系统的可用空间信息 hadoop fs -df -h / -du 统计文件夹的大小信息 hadoop fs -du -s -h /aaa/* -count 统计一个指定目录下的文件节点数量 hadoop fs -count /aaa/ -setrep 设置hdfs中文件的副本数量 hadoop fs -setrep 3 /aaa/jdk.tar 2 HDFS 原理2.1 概述 HDFS集群分为两大角色：NameNode、DataNode NameNode负责管理整个文件系统的元数据 DataNode 负责管理用户的文件数据块 文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台datanode上 每一个文件块可以有多个副本，并存放在不同的datanode上 Datanode会定期向Namenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量 HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行 2.2 HDFS 读写数据详细步骤2.2.1 写数据 根namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在 namenode返回是否可以上传 client请求第一个 block该传输到哪些datanode服务器上 namenode返回3个datanode服务器ABC client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将真个pipeline建立完成，逐级返回客户端 client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答 当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。 2.2.2 读数据 2.3 NameNode工作机制NAMENODE职责：1、负责客户端请求的响应2、元数据的管理（查询，修改） namenode对数据的管理采用了三种存储形式：1、内存元数据(NameSystem)2、磁盘元数据镜像文件3、数据操作日志文件（可通过日志运算出元数据） 2.3.1 元数据的存储机制A、内存中有一份完整的元数据(内存meta data)B、磁盘有一个“准完整”的元数据镜像（fsimage）文件(在namenode的工作目录中)C、用于衔接内存metadata和持久化元数据镜像fsimage之间的操作日志（edits文件）注：当客户端对hdfs中的文件进行新增或者修改操作，操作记录首先被记入edits日志文件中，当客户端操作成功后，相应的元数据会更新到内存meta.data中 2.3.2 元数据的checkpoint每隔一段时间，会由secondaryNamenode 将 namenode 上积累的所有edits和一个最新的fsimage下载到本地，并架子啊到内存进行merge（这个过程称为checkpoint） checkpoint的详细过程 checkpoint操作的出发条件配置参数 chekcpoint的附带作用 namenode和secondary namenode的工作目录存储结构完全相同，所以，当namenode故障退出需要重新恢复时，可以从secondary namenode的工作目录中将fsimage拷贝到namenode的工作目录，以恢复namenode的元数据 2.3.3 元数据目录说明1、在第一次部署好Hadoop集群的时候，我们需要在NameNode（NN）节点上格式化磁盘：$HADOOP_HOME/bin/hdfs namenode -format 2、格式化完成之后，将会在$dfs.namenode.name.dir/current目录下如下的文件结构 123456current/|-- VERSION|-- edits_*|-- fsimage_0000000000008547077|-- fsimage_0000000000008547077.md5`-- seen_txid 3、其中的dfs.name.dir是在hdfs-site.xml文件中配置的，默认值如下： 1234567891011&lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;file://$&#123;hadoop.tmp.dir&#125;/dfs/name&lt;/value&gt;&lt;/property&gt;&lt;!--hadoop.tmp.dir是在core-site.xml中配置的，默认值如下--&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/tmp/hadoop-$&#123;user.name&#125;&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt;&lt;/property&gt; 4、fs.namenode.name.dir属性可以配置多个目录，如/data1/dfs/name,/data2/dfs/name,/data3/dfs/name,….。各个目录存储的文件结构和内容都完全一样，相当于备份，这样做的好处是当其中一个目录损坏了，也不会影响到Hadoop的元数据，特别是当其中一个目录是NFS（网络文件系统Network File System，NFS）之上，即使你这台机器损坏了，元数据也得到保存。 5、下面对$dfs.namenode.name.dir/current/目录下的文件进行解释。 VERSION文件是Java属性文件，内容大致如下： 123456namespaceID=934548976clusterID=CID-cdff7d73-93cd-4783-9399-0a22e6dce196cTime=0storageType=NAME_NODEblockpoolID=BP-893790215-192.168.24.72-1383809616115layoutVersion=-47 1、namespaceID是文件系统的唯一标识符，在文件系统首次格式化之后生成的 2、storageType说明这个文件存储的是什么进程的数据结构信息（如果是DataNode，storageType=DATA_NODE） 3、cTime表示NameNode存储时间的创建时间，由于我的NameNode没有更新过，所以这里的记录值为0，以后对NameNode升级之后，cTime将会记录更新时间戳 4、layoutVersion表示HDFS永久性数据结构的版本信息， 只要数据结构变更，版本号也要递减，此时的HDFS也需要升级，否则磁盘仍旧是使用旧版本的数据结构，这会导致新版本的NameNode无法使用 5、clusterID是系统生成或手动指定的集群ID，在-clusterid选项中可以使用它；如下说明 a、使用如下命令格式化一个Namenode：$HADOOP_HOME/bin/hdfs namenode -format [-clusterId ]选择一个唯一的cluster_id，并且这个cluster_id不能与环境中其他集群有冲突。如果没有提供cluster_id，则会自动生成一个唯一的ClusterID。 b、使用如下命令格式化其他Namenode：$HADOOP_HOME/bin/hdfs namenode -format -clusterId c、升级集群至最新版本。在升级过程中需要提供一个ClusterID，例如：$HADOOP_PREFIX_HOME/bin/hdfs start namenode –config $HADOOP_CONF_DIR -upgrade -clusterId 如果没有提供ClusterID，则会自动生成一个ClusterID。 6、blockpoolID：是针对每一个Namespace所对应的blockpool的ID，上面的这个BP-893790215-192.168.24.72-1383809616115就是在我的ns1的namespace下的存储块池的ID，这个ID包括了其对应的NameNode节点的ip地址。 seen_txid $dfs.namenode.name.dir/current/seentxid非常重要，是存放transactionId的文件，format之后是0，它代表的是namenode里面的edits*文件的尾数，namenode重启的时候，会按照seen_txid的数字，循序从头跑edits_0000001~到seen_txid的数字。所以当你的hdfs发生异常重启的时候，一定要比对seen_txid内的数字是不是你edits最后的尾数，不然会发生建置namenode时metaData的资料有缺少，导致误删Datanode上多余Block的资讯。文件中记录的是edits滚动的序号，每次重启namenode时，namenode就知道要将哪些edits进行加载edits current目录 $dfs.namenode.name.dir/current目录下在format的同时也会生成fsimage和edits文件，及其对应的md5校验文件。 2.4 NameNode的SafeModeNameNode在刚启动时，内存中只有文件名、文件块的BlockId、文件的副本量，但不知道Block所在的DataNode。NameNode需要等待所有的DataNode向他汇报自身持有的块信息，NameNode才能在元数据中补全文件块信息中的位置信息。只有当NameNode找到99.8%(默认)的块位置信息时，才会退出安全模式，正常对外提供服务。 3 HDFS API3.1 普通方式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159package hadoop.hdfs;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.*;import org.junit.Before;import org.junit.Test;import java.io.IOException;import java.net.URI;import java.net.URISyntaxException;import java.util.Iterator;import java.util.Map;/** * HDFS 基本API使用 * @author NikoBelic * @create 2017/4/25 15:11 */public class HDFSTest&#123; FileSystem fs = null; Configuration conf = null; @Before public void init() throws IOException, URISyntaxException, InterruptedException &#123; /* 客户端去操作HDFS时，是有一个用户身份的 默认情况下，HDFS客户端API会从JVM中获取一个参数来作为自己的用户身份，HADOOP_USER_NAME 也可以在构建客户端FS对象时指定身份 */ conf = new Configuration(); // 拿到一个文件系统操作的客户端实例对象 fs = FileSystem.get(new URI(\"hdfs://10.5.151.241:9000\"),conf,\"root\"); &#125; /** * 配置文件参数 * @Author SeawayLee * @Date 2017/05/02 12:22 */ @Test public void testConfig() &#123; Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = conf.iterator(); while (iterator.hasNext()) &#123; Map.Entry&lt;String, String&gt; entry = iterator.next(); System.out.println(entry.getKey() + \":\" + entry.getValue()); &#125; &#125; /** * 上传文件 * @Author SeawayLee * @Date 2017/05/02 12:23 */ @Test public void testUpload() throws IOException, InterruptedException &#123; fs.copyFromLocalFile(new Path(\"/Users/lixiwei-mac/百度云同步盘/MAC云存储/电子书/机器学习/机器学习-Mitchell-中文-清晰版.pdf\"),new Path(\"/机器学习-Mitchell-中文-清晰版.pdf\")); fs.close(); &#125; /** * 下载文件 * @Author SeawayLee * @Date 2017/05/02 12:23 */ @Test public void testDownload() throws Exception &#123; fs.copyToLocalFile(new Path(\"/paper.txt\"),new Path(\"/Users/lixiwei-mac/Documents/IdeaProjects/bigdatalearning/src/data\")); fs.close(); &#125; /** * 创建文件夹 * @Author SeawayLee * @Date 2017/05/02 14:46 */ @Test public void testMkdir() throws IOException &#123; boolean mkdirs = fs.mkdirs(new Path(\"/tesMkdir/aaa/bbb\")); System.out.println(mkdirs); &#125; /** * 删除文件或文件夹 * @Author SeawayLee * @Date 2017/05/02 14:46 */ @Test public void testDelete() throws IOException &#123; boolean delete = fs.delete(new Path(\"/tesMkdir\"), true); System.out.println(delete); &#125; /** * 查看文件信息 迭代方式 * @Author SeawayLee * @Date 2017/05/02 14:46 */ @Test public void testLs() throws IOException &#123; RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(\"/\"), true); while (listFiles.hasNext()) &#123; LocatedFileStatus file = listFiles.next(); System.out.println(\"BlockSize:\" + file.getBlockSize() / 1024.0 / 1024.0 + \"MB\"); System.out.println(\"Owner:\" + file.getOwner()); System.out.println(\"Replication:\" + file.getReplication()); System.out.println(\"Permission:\" + file.getPermission()); System.out.println(\"Name:\" + file.getPath().getName()); BlockLocation[] blockLocations = file.getBlockLocations(); System.out.println(\"\"); for (BlockLocation blockLocation : blockLocations) &#123; System.out.println(\"Block-Name:\"); for (String s : blockLocation.getNames()) &#123; System.out.print(s + \" \"); &#125; System.out.println(\"\"); System.out.println(\"Block-Offset:\" + blockLocation.getOffset()); System.out.println(\"Block-Length:\" + blockLocation.getLength()); System.out.println(\"Block-Hosts:\"); for (String host : blockLocation.getHosts()) &#123; System.out.print(host + \" \"); &#125; System.out.println(\"\"); &#125; System.out.println(\"==========================\"); &#125; &#125; /** * 查看文件信息 数组方式 * @Author SeawayLee * @Date 2017/05/02 14:46 */ @Test public void testLs2() throws IOException &#123; FileStatus[] fileStatuses = fs.listStatus(new Path(\"/\")); for (FileStatus fileStatus : fileStatuses) &#123; System.out.println(fileStatus.getPath().getName()); &#125; &#125;&#125; 3.2 流方式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package hadoop.hdfs;import org.apache.commons.io.IOUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.junit.Before;import org.junit.Test;import sun.nio.ch.IOUtil;import java.io.*;import java.net.URI;import java.net.URISyntaxException;/** * 流方式操作HDFS * @author NikoBelic * @create 2017/5/2 13:31 */public class HdfsStreamAccess&#123; FileSystem fs = null; Configuration conf = null; @Before public void init() throws IOException, URISyntaxException, InterruptedException &#123; conf = new Configuration(); fs = FileSystem.get(new URI(\"hdfs://10.5.151.241:9000\"), conf, \"root\"); &#125; /** * 通过流的方式上传文件 * @Author SeawayLee * @Date 2017/05/02 13:41 */ @Test public void testUpload() throws IOException &#123; FSDataOutputStream outputStream = fs.create(new Path(\"/linux系统安装过程.avi\")); FileInputStream inputStream = new FileInputStream(\"/Users/lixiwei-mac/linux系统安装过程.avi\"); IOUtils.copy(inputStream, outputStream); &#125; /** * 通过流的方式下载文件 * @Author SeawayLee * @Date 2017/05/02 13:55 */ @Test public void testDownload() throws IOException &#123; FSDataInputStream inputStream = fs.open(new Path(\"/TreeFor.png\")); FileOutputStream outputStream = new FileOutputStream(\"/Users/lixiwei-mac/Downloads/fuck.txt\"); IOUtils.copy(inputStream, outputStream); &#125; /** * 通过流的方式指读取文件大小(MapReduce从HDFS读文件进行切片时肯定会用到) * @Author SeawayLee * @Date 2017/05/02 14:18 */ @Test public void testRandomAccess() throws IOException &#123; FSDataInputStream inputStream = fs.open(new Path(\"/TreeFor.png\")); inputStream.seek(50); InputStreamReader isr = new InputStreamReader(inputStream); BufferedReader bf = new BufferedReader(isr); char[] buffer = new char[1024 * 20]; int len = 0; while ((len = bf.read(buffer) )!= -1) &#123; System.out.print(String.valueOf(buffer,0,len)); &#125; &#125; /** * 流的方式读取文件 * @Author SeawayLee * @Date 2017/05/02 14:21 */ @Test public void testCat() throws IOException &#123; FSDataInputStream inputStream = fs.open(new Path(\"/TreeFor.png\")); IOUtils.copy(inputStream, System.out); &#125;&#125; 4 Hadoop中的RPC框架Hadoop中各个节点的远程调用非常频繁，他自己封装了一套RPC框架，我们也可以直接拿来用，只需要导入Hadoop的common包即可，与Hadoop集群启动与否毫无关系。 假设我们需要远程调用一个登陆服务，使用Hadoop的RPC框架很容易就可以实现。 服务接口 123456public interface LoginServiceInterface&#123; public static final long versionID = 1L; public String login(String username, String passowrd);&#125; 服务实现 12345678910public class LoginServiceImpl implements LoginServiceInterface&#123; @Override public String login(String username, String passowrd) &#123; System.out.println(username + \", 你好啊!\"); return username + \" Successfully login....\"; &#125;&#125; 服务启动 12345678910111213public class PublishServer&#123; public static void main(String[] args) throws IOException &#123; RPC.Builder builder = new RPC.Builder(new Configuration()); builder.setBindAddress(\"localhost\") .setPort(8888) .setProtocol(LoginServiceInterface.class) .setInstance(new LoginServiceImpl()); RPC.Server loginServer = builder.build(); loginServer.start(); &#125;&#125; 客户端远程调用 12345678910public class LoginAction&#123; public static void main(String[] args) throws IOException &#123; LoginServiceInterface loginService = RPC.getProxy(LoginServiceInterface.class, 2L, new InetSocketAddress(\"localhost\", 8888), new Configuration()); String res = loginService.login(\"NikoBelic\", \"asdasd\"); System.out.println(res); &#125;&#125; 客户端输出 NikoBelic, 你好啊!","tags":[{"name":"大数据","slug":"大数据","permalink":"https://seawaylee.github.io/tags/大数据/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://seawaylee.github.io/tags/Hadoop/"}]},{"title":"Hadoop学习笔记（一）- Hadoop快速入门","date":"2017-04-24T07:08:31.000Z","path":"2017/04/24/大数据/Hadoop/Hadoop学习笔记（一）- Hadoop快速入门/","text":"1 Hadoop生态圈简介 重点组件： HDFS：分布式文件系统 MAPREDUCE：分布式运算程序开发框架 HIVE：基于大数据技术（文件系统+运算框架）的SQL数据仓库工具 HBASE：基于HADOOP的分布式海量数据库 ZOOKEEPER：分布式协调服务基础组件 Mahout：基于mapreduce/spark/flink等分布式运算框架的机器学习算法库 Oozie：工作流调度框架 Sqoop：数据导入导出工具 Flume：日志数据采集框架 2 搭建Hadoop集群2.1 下载、解压、配置环境变量1wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz 配置免密登陆 2.2 Hadoop配置hadoop-env.sh 1export JAVA_HOME=/home/hadoop/apps/jdk1.7.0_51 core-site.xml 123456789&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/app/data/hadoop_data&lt;/value&gt;&lt;/property&gt; hdfs-site.xml 1234&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt; mapred-site.xml 12345&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; yarn-site.xml 123456789&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; slaves 123hadoop1hadoop2hadoop3 3 Hadoop Shell 命令3.1 集群相关 start-dfs.sh 启动hdfs start-yarn.sh 启动yarn start-all.sh 直接启动hdfs + yarn stop-all.sh 停止 hdfs + yarn hadoop namenode -format 3.2 HDFS 命令 功能 示例 备注 -help 输出命令参数手册 -ls 显示目录信息 hadoop fs -ls / -mkdir 创建目录 hadoop fs -mkdir -p /aaa/bb/cc/dd -moveFromLocal 从本地剪切到hdfs hadoop fs -moveFromLocal /home/hadoop/1.txt /aaa/bb/cc/dd -moveToLocal 从hdfs剪切到本地 hadoop fs -moveToLocal /aaa/bb/cc/dd /home/hadoop/1.txt -appendToFile 追加一个文件到已经存在的文件末尾 hadoop fs -appendToFile ./hello.txt /hello.txt -cat 查看文件内容 hadoop fs -cat /hello.txt -tail 显示一个文件的末尾 hadoop fs -tail /access_log.1 -text 以字符形式打印一个文件的内容 hadoop fs -text /access_log.1 -chgrp/-chmod/-chown 修改文件权限 hadoop fs -chmod 755 /hello.txt -copyFromLocal 从本地文件系统中拷贝文件到hdfs路径去 -copyToLocal 从hdfs拷贝到本地 -cp 从hdfs的一个路径拷贝到hdfs另一个路径 -mv 在hdfs中移动文件 -get 等同于copyToLocal -put 等同于copyFromLocal -getmerge 合并下载多个文件 -rm 删除文件或文件夹 hadoop fs -rm -r /aaa/bbb -rmdir 删除空目录 hadoop fs -rmdir /aaa/bbb/ccc -df 统计问价那系统的可用空间信息 hadoop fs -df -h / -du 统计文件夹的大小信息 hadoop fs -du -s -h /aaa/* -count 统计一个指定目录下的文件节点数量 hadoop fs -count /aaa/ -setrep 设置hdfs中文件的副本数量 hadoop fs -setrep 3 /aaa/jdk.tar 4 使用Hadoop集群4.1 查看集群状态hdfs dfsadmin -report 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364Configured Capacity: 64389906432 (59.97 GB)Present Capacity: 38095904768 (35.48 GB)DFS Remaining: 38095818752 (35.48 GB)DFS Used: 86016 (84 KB)DFS Used%: 0.00%Under replicated blocks: 0Blocks with corrupt replicas: 0Missing blocks: 0-------------------------------------------------Live datanodes (3):Name: 10.5.151.242:50010 (hadoop2)Hostname: localhostDecommission Status : NormalConfigured Capacity: 21463302144 (19.99 GB)DFS Used: 28672 (28 KB)Non DFS Used: 8654106624 (8.06 GB)DFS Remaining: 12809166848 (11.93 GB)DFS Used%: 0.00%DFS Remaining%: 59.68%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Tue Apr 25 19:30:28 CST 2017Name: 10.5.151.241:50010 (hadoop1)Hostname: localhostDecommission Status : NormalConfigured Capacity: 21463302144 (19.99 GB)DFS Used: 28672 (28 KB)Non DFS Used: 9653018624 (8.99 GB)DFS Remaining: 11810254848 (11.00 GB)DFS Used%: 0.00%DFS Remaining%: 55.03%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Tue Apr 25 19:30:28 CST 2017Name: 10.5.151.243:50010 (hadoop3)Hostname: localhostDecommission Status : NormalConfigured Capacity: 21463302144 (19.99 GB)DFS Used: 28672 (28 KB)Non DFS Used: 7986876416 (7.44 GB)DFS Remaining: 13476397056 (12.55 GB)DFS Used%: 0.00%DFS Remaining%: 62.79%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Tue Apr 25 19:30:27 CST 2017 或者打开 http://hadoop1:50070查看 4.2 上传、下载文件HDFS默认文件大小块为128MB，若文件大于128MB，HDFS将会对文件进行切割后存储。HDFS对文件执行的切割非常简单，我们可以将下载后的多个切块使用cat block1 &gt;&gt; tmp.txt ,cat block2 &gt;&gt; tmp.txt 命令将完整的文件拼接出来。 HDFS文件的增删改查可以使用Hadoop的Shell命令进行操作。 4.3 测试MapReduce 上传文本文件到hdfs /paper.txt cd /opt/app/hadoop-2.6.4/share/hadoop/mapreduce hadoop jar hadoop-mapreduce-examples-2.6.4.jar wordcount /paper.txt /wordcount/output 到 http://hadoop1:8088 查看job执行流程 到HDFS查看统计结果 hadoop fs -cat /wordcount/output/part-r-00000 5 问题解决记录5.1 CentOS7 64位系统问题启动报错 1WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 解决 vim /etc/profile 12345678JAVA_HOME=/usr/java/jdk1.7.0_79CLASSPATH=.:$JAVA_HOME/lib.tools.jarZOOKEEPER_HOME=/opt/app/zookeeper-3.4.5HADOOP_HOME=/opt/app/hadoop-2.6.4PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$ZOOKEEPER_HOME/bin:$JAVA_HOME/bin:$PATHexport HADOOP_COMMON_LIB_NATIVE_DIR=&quot;$HADOOP_HOME/lib/native/&quot;export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib/&quot;export HADOOP_HOME ZOOKEEPER_HOME JAVA_HOME CLASSPATH PATH 5.2 格式化DFS报错 1UnknownHostException - Formatting HDFS on Mac OSX Mavericks 解决 vim /etc/host添加主机名 10-5-151-241 1127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 10-5-151-241 5.3 启动DataNode失败报错 1org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60 解决 根据日志中的路径，cd /home/hadoop/tmp/dfs 能看到 data和name两个文件夹， 将name/current下的VERSION中的clusterID复制到data/current下的VERSION中，覆盖掉原来的clusterID 让两个保持一致 然后重启，启动后执行jps，查看进程 或 直接删除data文件夹","tags":[{"name":"大数据","slug":"大数据","permalink":"https://seawaylee.github.io/tags/大数据/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://seawaylee.github.io/tags/Hadoop/"}]},{"title":"JVM学习笔记（一）- 基础知识","date":"2017-04-19T12:39:28.000Z","path":"2017/04/19/Java基础/JVM/JVM学习笔记（基础知识）/","text":"1 Java内存模型1.1 内存模型图解Java虚拟机在执行Java程序的过程中，会把它所管理的内存划分为若干个不同的数据区。这些区域有各自的用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，有的区域则依赖用户线程的启动和结束而建立和销毁，我们可以将这些区域统称为Java运行时数据区域。 如下图是一个内存模型的关系图（详情见图：内存划分.png）： 如上图所示，Java虚拟机运行时数据区域被分为五个区域：堆(Heap)、栈(Stack)、本地方法栈(Native Stack)、方法区(Method Area)、程序计数器(Program Count Register)。 1.2 堆（Heap）对于大多数应用来说，Java Heap是Java虚拟机管理的内存的最大一块，这块区域随着虚拟机的启动而创建。在实际的运用中，我们创建的对象和数组就是存放在堆里面。如果你听说线程安全的问题，就会很明确的知道Java Heap是一块共享的区域，操作共享区域的成员就有了锁和同步。 与Java Heap相关的还有Java的垃圾回收机制（GC）,Java Heap是垃圾回收器管理的主要区域。程序猿所熟悉的新生代、老生代、永久代的概念就是在堆里面，现在大多数的GC基本都采用了分代收集算法。如果再细致一点，Java Heap还有Eden空间，From Survivor空间,To Survivor空间等。 Java Heap可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可。 1.3 栈（Stack）相对于Java Heap来讲，Java Stack是线程私有的，她的生命周期与线程相同。Java Stack描述的是Java方法执行时的内存模型，每个方法执行时都会创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。从下图从可以看到，每个线程在执行一个方法时，都意味着有一个栈帧在当前线程对应的栈帧中入栈和出栈。 图中可以看到每一个栈帧中都有局部变量表。局部变量表存放了编译期间的各种基本数据类型，对象引用等信息。 1.4 本地方法栈（Native Stack）本地方法栈（Native Stack）与Java虚拟机站（Java Stack）所发挥的作用非常相似，他们之间的区别在于虚拟机栈为虚拟机栈执行java方法（也就是字节码）服务，而本地方法栈则为使用到Native方法服务。 1.5 方法区（Method Area）方法区（Method Area）与堆（Java Heap）一样，是各个线程共享的内存区域，它用于存储虚拟机加载的类信息，常量，静态变量，即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是她却有一个别名叫做非堆（Non-Heap）。分析下Java虚拟机规范，之所以把方法区描述为堆的一个逻辑部分，应该觉得她们都是存储数据的角度出发的。一个存储对象数据（堆），一个存储静态信息(方法区)。 在上文中，我们看到堆中有新生代、老生代、永久代的描述。为什么我们将新生代、老生代、永久代三个概念一起说，那是因为HotSpot虚拟机的设计团队选择把GC分代收集扩展至方法区，或者说使用永久代来实现方法区而已。这样HotSpot的垃圾收集器就能想管理Java堆一样管理这部分内存。简单点说就是HotSpot虚拟机中内存模型的分代，其中新生代和老生代在堆中，永久代使用方法区实现。根据官方发布的路线图信息，现在也有放弃永久代并逐步采用Native Memory来实现方法区的规划，在JDK1.7的HotSpot中，已经把原本放在永久代的字符串常量池移出。 1.6 小结 线程私有的数据区域有： Java虚拟机栈（Java Stack） 本地方法栈（Native Stack） 线程共有的数据区域有： 堆（Java Heap） 方法区 2 GC算法2.1 标记-清楚算法（Mark-Sweep）1、标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象2、在标记完成后统一回收所有被标记的对象 缺点： 一个是效率问题，标记和清除两个过程的效率都不高； 另一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 2.2 复制算法（Copying）1、将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。2、当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 优点： 这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等 复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为了原来的一半，未免太高了一点。缺点： 复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低 2.3 标记整理算法（Mark-Compact）1、标记2、让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存 2.4 分代收集算法（Generational Collection）1、根据对象存活周期的不同将内存划分为几块。2、一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。3、在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。4、老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记—清理”或者“标记—整理”算法来进行回收。 3 垃圾收集器3.1 Serial收集器1、是一个单线程的收集器，“Stop The World”2、对于运行在Client模式下的虚拟机来说是一个很好的选择3、简单而高效 3.2 Serial Old收集器1、Serial收集器的老年代版本，它同样是一个单线程收集器，使用“标记-整理”算法。2、主要意义也是在于给Client模式下的虚拟机使用。3、如果在Server模式下，那么它主要还有两大用途： 一种用途是在JDK 1.5以及之前的版本中与Parallel Scavenge收集器搭配使用[1]， 另一种用途就是作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure时使用。 3.3 ParNew收集器1、Serial收集器的多线程版本2、单CPU不如Serial3、Server模式下新生代首选,目前只有它能与CMS收集器配合工作4、使用-XX：+UseConcMarkSweepGC选项后的默认新生代收集器，也可以使用-XX：+UseParNewGC选项来强制指定它。5、-XX：ParallelGCThreads：限制垃圾收集的线程数。 3.4 Parallel Scavenge收集器1、吞吐量优先”收集器2、新生代收集器，复制算法，并行的多线程收集器3、目标是达到一个可控制的吞吐量（Throughput）。4、吞吐量=运行用户代码时间/（运行用户代码时间+垃圾收集时间），虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。5、两个参数用于精确控制吞吐量: -XX：MaxGCPauseMillis是控制最大垃圾收集停顿时间 -XX：GCTimeRatio直接设置吞吐量大小 -XX：+UseAdaptiveSizePolicy:动态设置新生代大小、Eden与Survivor区的比例、晋升老年代对象年龄 6、并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。7、并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行的，可能会交替执行），用户程序在继续运行，而垃圾收集程序运行于另一个CPU上。 3.5 Parallel Old收集器1、Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理”算法。2、在注重吞吐量以及CPU资源敏感的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器。 3.6 CMS收集器1、以获取最短回收停顿时间为目标的收集器。2、非常符合互联网站或者B/S系统的服务端上，重视服务的响应速度，希望系统停顿时间最短的应用3、基于“标记—清除”算法实现的4、CMS收集器的内存回收过程是与用户线程一起并发执行的5、它的运作过程分为4个步骤，包括： 初始标记，“Stop The World”，只是标记一下GC Roots能直接关联到的对象，速度很快 并发标记，并发标记阶段就是进行GC RootsTracing的过程 重新标记，Stop The World”，是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，但远比并发标记的时间短并发清除（CMS concurrent sweep） 6、优点：并发收集、低停顿7、缺点： 对CPU资源非常敏感。 无法处理浮动垃圾，可能出现“Concurrent Mode Failure”失败而导致另一次Full GC的产生。 一款基于“标记—清除”算法实现的收集器 3.7 G1（Garbage-First）收集器1、当今收集器技术发展的最前沿成果之一2、G1是一款面向服务端应用的垃圾收集器。3、优点： 并行与并发：充分利用多CPU、多核环境下的硬件优势 分代收集：不需要其他收集器配合就能独立管理整个GC堆 空间整合：“标记—整理”算法实现的收集器，局部上基于“复制”算法不会产生内存空间碎片 可预测的停顿：能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒 4、G1收集器的运作大致可划分为以下几个步骤： 初始标记：标记一下GC Roots能直接关联到的对象，需要停顿线程，但耗时很短 并发标记：是从GC Root开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行 最终标记：修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录 筛选回收：对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划 3.8 垃圾收集器参数总结收集器设置： -XX:+UseSerialGC:年轻串行（Serial），老年串行（Serial Old） -XX:+UseParNewGC:年轻并行（ParNew），老年串行（Serial Old） -XX:+UseConcMarkSweepGC:年轻并行（ParNew），老年串行（CMS），备份（Serial Old） -XX:+UseParallelGC:年轻并行吞吐（Parallel Scavenge），老年串行（Serial Old） -XX:+UseParalledlOldGC:年轻并行吞吐（Parallel Scavenge），老年并行吞（Parallel Old） 收集器参数： -XX:ParallelGCThreads=n:设置并行收集器收集时使用的CPU数。并行收集线程数。 -XX:MaxGCPauseMillis=n:设置并行收集最大暂停时间 -XX:GCTimeRatio=n:设置垃圾回收时间占程序运行时间的百分比。公式为1/(1+n) -XX:+CMSIncrementalMode:设置为增量模式。适用于单CPU情况。 -XX:ParallelGCThreads=n:设置并发收集器年轻代收集方式为并行收集时，使用的CPU数。并行收集线程数。 4 JVM参数列表 java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:NewRatio=4 -XX:SurvivorRatio=4 -XX:MaxPermSize=16m -XX:MaxTenuringThreshold=0 -Xmx3550m：最大堆内存为3550M。 -Xms3550m：初始堆内存为3550m。 此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。 -Xmn2g：设置年轻代大小为2G。 整个堆大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。 -Xss128k：设置每个线程的堆栈大小。 JDK5.0以后每个线程堆栈大小为1M，在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在 3000~5000左右。 -XX:NewRatio=4:设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）。 设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5 -XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的大小比值。 设置为4，则两个Survivor区与一个Eden区的比值为2:4，一个Survivor区占整个年轻代的1/6 -XX:MaxPermSize=16m:设置持久代大小为16m。 -XX:MaxTenuringThreshold=15：设置垃圾最大年龄。 如果设置为0的话，则年轻代对象不经过Survivor区，直 接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象 再年轻代的存活时间，增加在年轻代即被回收的概论。 收集器设置 -XX:+UseSerialGC:设置串行收集器 -XX:+UseParallelGC:设置并行收集器 -XX:+UseParalledlOldGC:设置并行年老代收集器 -XX:+UseConcMarkSweepGC:设置并发收集器 垃圾回收统计信息 -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:filename 并行收集器设置 -XX:ParallelGCThreads=n:设置并行收集器收集时使用的CPU数。并行收集线程数。 -XX:MaxGCPauseMillis=n:设置并行收集最大暂停时间 -XX:GCTimeRatio=n:设置垃圾回收时间占程序运行时间的百分比。公式为1/(1+n) 并发收集器设置 -XX:+CMSIncrementalMode:设置为增量模式。适用于单CPU情况。 -XX:ParallelGCThreads=n:设置并发收集器年轻代收集方式为并行收集时，使用的CPU数。并行收集线程数。 5 JVM监控工具5.1 jconsolejconsole是一种集成了上面所有命令功能的可视化工具，可以分析jvm的内存使用情况和线程等信息。 5.2 jvisualvm提供了和jconsole的功能类似，提供了一大堆的插件。插件中，Visual GC（可视化GC）还是比较好用的，可视化GC可以看到内存的具体使用情况。","tags":[{"name":"Java基础","slug":"Java基础","permalink":"https://seawaylee.github.io/tags/Java基础/"},{"name":"JVM","slug":"JVM","permalink":"https://seawaylee.github.io/tags/JVM/"}]},{"title":"Netty学习笔记（一）","date":"2017-04-18T06:23:41.000Z","path":"2017/04/18/大数据/大数据基础/Netty学习笔记（一）/","text":"1 Netty简介 Netty是基于Java NIO的网络应用框架. Netty是一个NIO client-server(客户端服务器)框架，使用Netty可以快速开发网络应用，例如服务器和客户端协议。Netty提供了一种新的方式来使开发网络应用程序，这种新的方式使得它很容易使用和有很强的扩展性。Netty的内部实现时很复杂的，但是Netty提供了简单易用的api从网络处理代码中解耦业务逻辑。Netty是完全基于NIO实现的，所以整个Netty都是异步的。 网络应用程序通常需要有较高的可扩展性，无论是Netty还是其他的基于Java NIO的框架，都会提供可扩展性的解决方案。Netty中一个关键组成部分是它的异步特性. 2 最简单的API用法2.1 引入jar包12345&lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.9.Final&lt;/version&gt;&lt;/dependency&gt; 2.2 代码实现Netty客户端 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * Netty客户端 * @author NikoBelic * @create 2017/4/18 13:59 */public class EchoClient&#123; private final String host; private final int port; public EchoClient(String host, int port) &#123; this.host = host; this.port = port; &#125; public void start() throws InterruptedException &#123; EventLoopGroup nioEventLoopGroup = null; try &#123; // 创建Bootstrap对象用来引导启动客户端 Bootstrap bootstrap = new Bootstrap(); // 创建EventLoopGroup对象并设置到Bootstrap中，EventLoopGroup可以理解为是一个线程池，这个线程池用来处理连接、接受数据、发送数据 nioEventLoopGroup = new NioEventLoopGroup(); // 创建InetSocketAddress并设置到Bootstrap中，InetSocketAddress是指定连接的服务器地址 bootstrap.group(nioEventLoopGroup).channel(NioSocketChannel.class).remoteAddress(new InetSocketAddress(host, port)).handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; // 添加一个ChannelInitializer&lt;SocketChannel&gt;，客户端成功连接服务器后会被执行 @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; socketChannel.pipeline().addLast(new EchoClientHandler()); &#125; &#125;); // 调用Bootstrap.connect()来连接服务器 ChannelFuture f = bootstrap.connect().sync(); // 最后关闭EventLoopGroup来释放资源 f.channel().closeFuture().sync(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; nioEventLoopGroup.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; new EchoClient(\"localhost\", 20000).start(); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * Netty客户端业务处理 * @author NikoBelic * @create 2017/4/18 14:13 */public class EchoClientHandler extends SimpleChannelInboundHandler&#123; /** * 客户端连接服务器后被调用 * * @Author SeawayLee * @Date 2017/04/18 14:15 */ @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println(\"客户端连接服务器，开始发送数据....\"); byte[] req = \"Query Time Order\".getBytes(); ByteBuf firstMessage = Unpooled.buffer(req.length); firstMessage.writeBytes(req); ctx.writeAndFlush(firstMessage); &#125; /** * 从服务端接收到数据后调用 * * @Author SeawayLee * @Date 2017/04/18 14:18 */ @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, Object o) throws Exception &#123; System.out.println(\"Client 读取 Server 数据...\"); // 服务端返回消息后 ByteBuf buf = (ByteBuf) o; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, \"UTF-8\"); System.out.println(\"服务端数据为：\" + body); &#125; /** * 发生异常时被调用 * * @Author SeawayLee * @Date 2017/04/18 14:18 */ @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println(\"客户端发生异常...\"); cause.printStackTrace(); ctx.close(); &#125;&#125; Netty服务端 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Netty服务端 * @author NikoBelic * @create 2017/4/18 13:45 */public class EchoServer&#123; private final int port; public EchoServer(int port) &#123; this.port = port; &#125; public void start() throws InterruptedException &#123; EventLoopGroup eventLoopGroup = null; try &#123; // 创建ServerBootstrap实例来引导绑定和启动服务器 ServerBootstrap serverBootstrap = new ServerBootstrap(); // 创建NioEventLoopGroup对象来处理时间，如接受新连接、接受数据、写数据等等 eventLoopGroup = new NioEventLoopGroup(); // 指定通道类型为NIOServerSocketChannel，设置InetSocketAddress让服务器鉴定某个端口以等待客户连接 serverBootstrap.group(eventLoopGroup).channel(NioServerSocketChannel.class).localAddress(\"localhost\", port).childHandler(new ChannelInitializer&lt;Channel&gt;() &#123; @Override protected void initChannel(Channel channel) throws Exception &#123; channel.pipeline().addLast(new EchoServerHandler()); &#125; &#125;); // 最后绑定服务器等待直到绑定完成，调用sync()方法会阻塞直到服务器完成绑定，然后服务器等待通道关闭，因为使用sync()，所以关闭操作也会被阻塞。 ChannelFuture channelFuture = serverBootstrap.bind().sync(); System.out.println(\"开始监听，端口为：\" + channelFuture.channel().localAddress()); channelFuture.channel().closeFuture().sync(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; eventLoopGroup.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; new EchoServer(20000).start(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738/** * Netty服务端业务处理 * @author NikoBelic * @create 2017/4/18 13:54 */public class EchoServerHandler extends ChannelInboundHandlerAdapter&#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println(\"Server 正在读取数据...\"); // 读取数据 ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, \"UTF-8\"); System.out.println(\"Server接收到客户端数据：\" + body); // 向客户端写数据 System.out.println(\"Server 向 Client发送数据\"); String currentTime = new Date(System.currentTimeMillis()).toString(); ByteBuf resp = Unpooled.copiedBuffer(currentTime.getBytes()); ctx.write(resp); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; System.out.println(\"Server 读取数据完毕\"); ctx.flush();// 刷新后才将数据发出到SocketChannel &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 启动服务端 启动客户端 服务端显示 12345开始监听，端口为：/127.0.0.1:20000Server 正在读取数据...Server接收到客户端数据：Query Time OrderServer 向 Client发送数据Server 读取数据完毕 客户端显示 123客户端连接服务器，开始发送数据....Client 读取 Server 数据...服务端数据为：Tue Apr 18 14:19:39 CST 2017 3 Netty中的Handler3.1 简介Handler在netty中，无疑占据着非常重要的地位。Handler与Servlet中的filter很像，通过Handler可以完成通讯报文的解码编码、拦截指定的报文、统一对日志错误进行处理、统一对请求进行计数、控制Handler执行与否。 Netty中的所有handler都实现自ChannelHandler接口。按照输入输出来分，分为ChannelInboundHandler、ChannelOutboundHandler两大类。ChannelInboundHandler对从客户端发往服务器的报文进行处理，一般用来执行解码、读取客户端数据、进行业务处理等；ChannelOutboundHandler对从服务器发往客户端的报文进行处理，一般用来进行编码、发送报文到客户端。 Netty中，可以注册多个handler。ChannelInboundHandler按照注册的先后顺序执行；ChannelOutboundHandler按照注册的先后顺序逆序执行，如下图所示，按照注册的先后顺序对Handler进行排序，request进入Netty后的执行顺序为： 示例代码 12345678910111213141516171819202122232425262728293031323334353637383940414243public class EchoServer &#123; private final int port; public EchoServer(int port) &#123; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup eventLoopGroup = null; try &#123; //server端引导类 ServerBootstrap serverBootstrap = new ServerBootstrap(); //连接池处理数据 eventLoopGroup = new NioEventLoopGroup(); serverBootstrap.group(eventLoopGroup) .channel(NioServerSocketChannel.class)//指定通道类型为NioServerSocketChannel，一种异步模式，OIO阻塞模式为OioServerSocketChannel .localAddress(\"localhost\",port)//设置InetSocketAddress让服务器监听某个端口已等待客户端连接。 .childHandler(new ChannelInitializer&lt;Channel&gt;() &#123;//设置childHandler执行所有的连接请求 @Override protected void initChannel(Channel ch) throws Exception &#123; // 注册两个InboundHandler，执行顺序为注册顺序，所以应该是InboundHandler1 InboundHandler2 // 注册两个OutboundHandler，执行顺序为注册顺序的逆序，所以应该是OutboundHandler2 OutboundHandler1 ch.pipeline().addLast(new EchoInHandler1()); ch.pipeline().addLast(new EchoInHandler2()); ch.pipeline().addLast(new EchoOutHandler1()); ch.pipeline().addLast(new EchoOutHandler2()); &#125; &#125;); // 最后绑定服务器等待直到绑定完成，调用sync()方法会阻塞直到服务器完成绑定,然后服务器等待通道关闭，因为使用sync()，所以关闭操作也会被阻塞。 ChannelFuture channelFuture = serverBootstrap.bind().sync(); System.out.println(\"开始监听，端口为：\" + channelFuture.channel().localAddress()); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; eventLoopGroup.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; new EchoServer(20000).start(); &#125;&#125; 1234567891011121314151617181920212223public class EchoInHandler1 extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println(\"in1\"); // 通知执行下一个InboundHandler ctx.fireChannelRead(msg); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.flush();//刷新后才将数据发出到SocketChannel &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132public class EchoInHandler2 extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println(\"in2\"); ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, \"UTF-8\"); System.out.println(\"接收客户端数据:\" + body); //向客户端写数据 System.out.println(\"server向client发送数据\"); String currentTime = new Date(System.currentTimeMillis()).toString(); ByteBuf resp = Unpooled.copiedBuffer(currentTime.getBytes()); ctx.write(resp); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.flush();//刷新后才将数据发出到SocketChannel &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 123456789101112public class EchoOutHandler2 extends ChannelOutboundHandlerAdapter &#123; @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception &#123; System.out.println(\"out2\"); // 执行下一个OutboundHandler /*System.out.println(\"at first..msg = \"+msg); msg = \"hi newed in out2\";*/ super.write(ctx, msg, promise); &#125;&#125; 12345678910111213public class EchoOutHandler1 extends ChannelOutboundHandlerAdapter &#123; @Override // 向client发送消息 public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception &#123; System.out.println(\"out1\"); /*System.out.println(msg);*/ String currentTime = new Date(System.currentTimeMillis()).toString(); ByteBuf resp = Unpooled.copiedBuffer(currentTime.getBytes()); ctx.write(resp); ctx.flush(); &#125;&#125; 3.2 小结在使用Handler的过程中，需要注意： 1、ChannelInboundHandler之间的传递，通过调用 ctx.fireChannelRead(msg) 实现；调用ctx.write(msg) 将传递到ChannelOutboundHandler。 2、ctx.write()方法执行后，需要调用flush()方法才能令它立即执行。 3、流水线pipeline中outhandler不能放在最后，否则不生效 4、Handler的消费处理放在最后一个处理。 4 Netty发送对象Netty中，通讯的双方建立连接后，会把数据按照ByteBuf的方式进行传输，例如http协议中，就是通过HttpRequestDecoder对ByteBuf数据流进行处理，转换成http的对象。基于这个思路，我自定义一种通讯协议：Server和客户端直接传输java对象。 实现的原理是通过Encoder把java对象转换成ByteBuf流进行传输，通过Decoder把ByteBuf转换成java对象进行处理，处理逻辑如下图所示： 客户端部分代码 12345678@Overrideprotected void initChannel(SocketChannel ch) throws Exception &#123; // 注册编码的handler ch.pipeline().addLast(new PersonEncoder()); //out //注册处理消息的handler ch.pipeline().addLast(new EchoClientHandler()); //in&#125; 12345678910111213141516171819202122232425262728293031323334public class EchoClientHandler extends SimpleChannelInboundHandler&lt;ByteBuf&gt; &#123; // 客户端连接服务器后被调用 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; Person person = new Person(); person.setName(\"angelababy\"); person.setSex(\"girl\"); person.setAge(18); ctx.write(person); ctx.flush(); &#125; // • 从服务器接收到数据后调用 @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception &#123; System.out.println(\"client 读取server数据..\"); // 服务端返回消息后 ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, \"UTF-8\"); System.out.println(\"服务端数据为 :\" + body); &#125; // • 发生异常时被调用 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println(\"client exceptionCaught..\"); // 释放资源 ctx.close(); &#125;&#125; 12345678910111213141516/** * 序列化 * 将object转换成Byte[] * @author wilson * */public class PersonEncoder extends MessageToByteEncoder&lt;Person&gt; &#123; @Override protected void encode(ChannelHandlerContext ctx, Person msg, ByteBuf out) throws Exception &#123; //工具类：将object转换为byte[] byte[] datas = ByteObjConverter.objectToByte(msg); out.writeBytes(datas); ctx.flush(); &#125;&#125; 服务端部分代码 1234567@Overrideprotected void initChannel(Channel ch) throws Exception &#123; //注册解码的handler ch.pipeline().addLast(new PersonDecoder()); //IN1 反序列化 //添加一个入站的handler到ChannelPipeline ch.pipeline().addLast(new EchoServerHandler()); //IN2&#125; 12345678910111213141516171819202122232425public class EchoServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; Person person = (Person) msg; System.out.println(person.getName()); System.out.println(person.getAge()); System.out.println(person.getSex()); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; System.out.println(\"server 读取数据完毕..\"); ctx.flush();//刷新后才将数据发出到SocketChannel &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 123456789101112131415161718/** * 反序列化 * 将Byte[]转换为Object * @author wilson * */public class PersonDecoder extends ByteToMessageDecoder &#123; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; //工具类：将ByteBuf转换为byte[] ByteBufToBytes read = new ByteBufToBytes(); byte[] bytes = read.read(in); //工具类：将byte[]转换为object Object obj = ByteObjConverter.byteToObject(bytes); out.add(obj); &#125; &#125;","tags":[{"name":"大数据基础","slug":"大数据基础","permalink":"https://seawaylee.github.io/tags/大数据基础/"}]},{"title":"SpringMVC - 拦截器与过滤器区别","date":"2017-04-11T09:55:02.000Z","path":"2017/04/11/JavaWeb/拦截器与过滤器的区别/","text":"1 过滤器和拦截器的区别： ①拦截器是基于Java的反射机制的，而过滤器是基于函数回调。 ②拦截器不依赖与servlet容器，过滤器依赖与servlet容器。 ③拦截器只能对action请求起作用，而过滤器则可以对几乎所有的请求起作用。 ④拦截器可以访问action上下文、值栈里的对象，而过滤器不能访问。 ⑤在action的生命周期中，拦截器可以多次被调用，而过滤器只能在容器初始化时被调用一次。 ⑥拦截器可以获取IOC容器中的各个bean，而过滤器就不行，这点很重要，在拦截器里注入一个service，可以调用业务逻辑。 写了点测试代码，顺便整理一下思路，搞清楚这几者之间的顺序： 1.过滤器是JavaEE标准，采用函数回调的方式进行。是在请求进入容器之后，还未进入Servlet之前进行预处理，并且在请求结束返回给前端这之间进行后期处理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class MyFilter implements Filter&#123; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; System.out.println(\"过滤器 - init \"); &#125; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; System.out.println(\"过滤器 - doFilter \"); filterChain.doFilter(servletRequest,servletResponse); &#125; @Override public void destroy() &#123; System.out.println(\"过滤器 - destroy \"); &#125;&#125;``` chain.doFilter(request, response);这个方法的调用作为分水岭。事实上调用Servlet的doService()方法是在chain.doFilter(request, response);这个方法中进行的。- 2.拦截器是被包裹在过滤器之中的。 - preHandle()这个方法是在过滤器的chain.doFilter(request, response)方法的前一步执行，也就是在 [System.out.println(\"before...\")][chain.doFilter(request, response)]之间执行。 - postHandle()方法之后，在return ModelAndView之前进行，可以操控Controller的ModelAndView内容。 - afterCompletion()方法是在过滤器返回给前端前一步执行，也就是在[chain.doFilter(request, response)][System.out.println(\"after...\")]之间执行。 ```java/** * Spring拦截器 * * @author NikoBelic * @create 2017/4/11 18:01 */@Componentpublic class MyInterceptor implements HandlerInterceptor&#123; @Override public boolean preHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o) throws Exception &#123; System.out.println(\"拦截器 - PreHandle - \" + httpServletRequest.getRemoteAddr()); return true; &#125; @Override public void postHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, ModelAndView modelAndView) throws Exception &#123; System.out.println(\"拦截器 - postHandle - \" + httpServletRequest.getRemoteAddr()); &#125; @Override public void afterCompletion(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) throws Exception &#123; System.out.println(\"拦截器 - afterCompletion - \" + httpServletRequest.getRemoteAddr()); &#125;&#125; 配置过滤器1234567891011public class SpitterWebInitializer extends AbstractAnnotationConfigDispatcherServletInitializer&#123; @Override protected Filter[] getServletFilters() &#123; CharacterEncodingFilter characterEncodingFilter = new CharacterEncodingFilter(); characterEncodingFilter.setEncoding(\"UTF-8\"); characterEncodingFilter.setForceEncoding(true); DelegatingFilterProxy testFilterChain = new DelegatingFilterProxy(new MyFilter()); return new Filter[]&#123;characterEncodingFilter, testFilterChain&#125;; &#125;&#125; 配置拦截器12345678910@Configuration@EnableWebMvc@ComponentScan(\"mvc.controller\")public class WebConfig extends WebMvcConfigurerAdapter&#123; @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(new MyInterceptor()).addPathPatterns(\"/interceptor/*\"); &#125;&#125; 测试访问 http://localhost:8080/SpringInAction/interceptor/dosomethingConsole输出12345过滤器 - doFilter 拦截器 - PreHandle - 0:0:0:0:0:0:0:1I am doing something...拦截器 - postHandle - 0:0:0:0:0:0:0:1拦截器 - afterCompletion - 0:0:0:0:0:0:0:1 3.SpringMVC的机制是由同一个Servlet来分发请求给不同的Controller，其实这一步是在Servlet的service()方法中执行的。所以过滤器、拦截器、service()方法，dispatc()方法的执行顺序应该是这样的，大致画了个图：其实非常好测试，自己写一个过滤器，一个拦截器，然后在这些方法中都加个断点，一路F8下去就得出了结论。 总结：拦截器功在对请求权限鉴定方面确实很有用处，在我所参与的这个项目之中，第三方的远程调用每个请求都需要参与鉴定，所以这样做非常方便，而且他是很独立的逻辑，这样做让业务逻辑代码很干净。和框架的其他功能一样，原理很简单，使用起来也很简单，大致看了下SpringMVC这一部分的源码，其实还是比较容易理解的。我们项目中仅仅用到了preHandle这个方法，而未用其他的，框架提供了一个已经实现了拦截器接口的适配器类HandlerInterceptorAdapter，继承这个类然后重写一下需要用到的方法就行了，可以少几行代码，这种方式Java中很多地方都有体现。大家还可以参考一下这个电子书的截图：","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"}]},{"title":"Java实现Socket通讯","date":"2017-04-10T13:46:55.000Z","path":"2017/04/10/Java基础/Java实现Socket通讯/","text":"Socket 服务端1234567891011121314151617181920212223242526/** * Socket服务端 * @author NikoBelic * @create 2017/4/10 20:51 */public class SocketServer&#123; public static void main(String[] args) throws IOException &#123; // 创建Socket服务端，绑定到本地8899端口 ServerSocket serverSocket = new ServerSocket(); serverSocket.bind(new InetSocketAddress(\"localhost\", 8899)); // 使用线程池异步处理业务逻辑（否则将不支持多客户端） ExecutorService threadPool = new ThreadPoolExecutor(0, 3, 60L, TimeUnit.SECONDS, new LinkedBlockingDeque&lt;&gt;(2)); Socket socket; while (true) &#123; // 接收客户端请求（阻塞方法） socket = serverSocket.accept(); // 创建线程 处理业务逻辑 threadPool.execute(new SocketTask(socket)); &#125; &#125;&#125; Socket 服务端业务逻辑处理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 业务逻辑处理线程 * @author NikoBelic * @create 2017/4/10 21:16 */public class SocketTask implements Runnable&#123; private Socket socket; public SocketTask(Socket socket) &#123; this.socket = socket; &#125; @Override public void run() &#123; InputStream in = null; OutputStream out = null; PrintWriter pw; BufferedReader reader; String clientStrs; try &#123; // 从套接字中获取输入流（客户端传输的数据流） in = socket.getInputStream(); reader = new BufferedReader(new InputStreamReader(in)); // 从套接字中获取输出流（回传给客户端的数据流） out = socket.getOutputStream(); pw = new PrintWriter(out); // 从输入流中读取数据（注意：readline是阻塞方法） while ((clientStrs = reader.readLine()) != null) &#123; System.out.println(\"Server端接收到:\" + clientStrs); // 给客户端回传的内容 pw.println(\"我收到了:\" + clientStrs); pw.flush(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; try &#123; in.close(); out.close(); socket.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; Socket 客户端1234567891011121314151617181920212223242526272829/** * Socket客户端 * @author NikoBelic * @create 2017/4/10 21:02 */public class SocketClient&#123; public static void main(String[] args) throws IOException &#123; // 创建套接字，与服务端进行通讯 Socket socket = new Socket(\"localhost\", 8899); // 从套接字中获得输出流（向服务端发送的数据流） OutputStream outputStream = socket.getOutputStream(); PrintWriter pw = new PrintWriter(outputStream); // 从套接字中获得输入流（服务端回传给客户端的数据流） InputStream inputStream = socket.getInputStream(); Scanner scanner = new Scanner(System.in); String input; while ((input = scanner.nextLine()) != \"end\") &#123; // 向服务端发送数据流 pw.println(input); pw.flush(); // 读取端返回的处理结果 System.out.println(new BufferedReader(new InputStreamReader(inputStream)).readLine()); &#125; &#125;&#125; 测试客户端Console 123456你好啊我收到了:你好啊Java网络编程我收到了:Java网络编程不错哦！！！嘿嘿嘿！我收到了:不错哦！！！嘿嘿嘿！ 服务端Console123Server端接收到:你好啊Server端接收到:Java网络编程Server端接收到:不错哦！！！嘿嘿嘿！","tags":[{"name":"Java基础","slug":"Java基础","permalink":"https://seawaylee.github.io/tags/Java基础/"}]},{"title":"动态代理和反射","date":"2017-04-10T06:12:23.000Z","path":"2017/04/10/Java基础/动态代理和反射/","text":"1 反射 通过反射的方式可以获取class对象中的属性、方法、构造函数等 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/** * Java反射测试 * @author NikoBelic * @create 2017/4/10 14:19 */public class ReflectionTest&lt;T&gt;&#123; /** * 通过class类路径创建对象 * * @param className 类的相对路径 * @param params 构造器的参数 * @param _private 是否使用私有构造函数 * @Author SeawayLee * @Date 2017/04/10 14:27 */ public T getInstance(String className, Map&lt;Object, Object&gt; params, Boolean _private) throws ClassNotFoundException, IllegalAccessException, InstantiationException, NoSuchMethodException, InvocationTargetException, NoSuchFieldException, IOException &#123; Class clazz = Class.forName(className); Class&lt;?&gt;[] paramTypes = new Class&lt;?&gt;[params != null ? params.size() : 0]; Object[] paramValues = new Object[params != null ? params.size() : 0]; Constructor constructor; T instance; if (params == null) &#123; instance = (T) clazz.newInstance(); Field name = clazz.getDeclaredField(\"name\"); Field age = clazz.getDeclaredField(\"age\"); name.setAccessible(true); age.setAccessible(true); name.set(instance, \"默认name\"); age.set(instance, 99); &#125; else &#123; int index = 0; for (Map.Entry entry : params.entrySet()) &#123; paramTypes[index] = (Class&lt;?&gt;) entry.getKey(); paramValues[index] = entry.getValue(); index++; &#125; constructor = !_private ? clazz.getConstructor(paramTypes) : clazz.getDeclaredConstructor(paramTypes); constructor.setAccessible(true); // 强制取消Java的权限检测 instance = (T) constructor.newInstance(paramValues); &#125; // 获取私有方法 Method method = clazz.getDeclaredMethod(\"talk\"); method.setAccessible(true); Object invokeResult = method.invoke(instance); System.out.println(\"私有函数反射结果输出:\" + invokeResult); // 获取类加载器 System.out.println(\"类加载器：\" + clazz.getClassLoader()); // 获取接口 System.out.println(\"接口：\"); for (Class c : clazz.getInterfaces()) System.out.println(c); // 获取父类 System.out.println(\"父类：\" + clazz.getGenericSuperclass()); // 获取静态资源文件 byte[] buff = new byte[1024]; clazz.getResourceAsStream(\"config.properties\").read(buff); System.out.println(\"配置文件:\" + new String(buff,\"UTF-8\")); return instance; &#125; public static void main(String[] args) throws ClassNotFoundException, InstantiationException, IllegalAccessException, NoSuchMethodException, InvocationTargetException, NoSuchFieldException, IOException &#123; ReflectionTest&lt;Person&gt; ref = new ReflectionTest(); // 构造参数 Map&lt;Object, Object&gt; params = new HashMap&lt;&gt;(); params.put(String.class, \"NikoBelic\"); //params.put(Integer.class, 18); //Person p = ref.getInstance(\"reflection.Person\", params, true); // 通过有参构造器创建实例 Person p = ref.getInstance(\"reflection.Person\", null, true); // 通过午餐构造器创建实例 System.out.println(p); &#125;&#125; 动态代理 假设在之前的代码调用阶段，我们用action调用service的方法实现业务即可。由于之前在service中实现的业务可能不能够满足当先客户的要求，需要我们重新修改service中的方法，但是service的方法不只在我们这个模块使用，在其他模块也在调用，其他模块调用的时候，现有的service方法已经能够满足业务需求，所以我们不能只为了我们的业务而修改service，导致其他模块授影响。 那怎么办呢？ 可以通过动态代理的方式，扩展我们的service中的方法实现，使得在原油的方法中增加更多的业务，而不是实际修改service中的方法，这种实现技术就叫做动态代理。动态代理：在不修改原业务的基础上，基于原业务方法，进行重新的扩展，实现新的业务。 例如下面的例子： 旧业务 买家调用action，购买衣服，衣服在数据库的标价为50元，购买流程就是简单的调用。 新业务 在原先的价格上可以使用优惠券，但是这个功能在以前没有实现过，我们通过代理类，代理了原先的接口方法，在这个方法的基础上，修改了返回值。 123456789101112131415161718192021222324252627282930/** * 动态代理测试 * @author NikoBelic * @create 2017/4/10 16:22 */public class ProductServiceProxy&#123; /** * 获取动态代理实例 * @Author SeawayLee * @Date 2017/04/10 17:06 */ public ProductService getProxy(Class clazz,Class[] interfaces,Integer discount) throws ClassNotFoundException &#123; Object proxy = Proxy.newProxyInstance(clazz.getClassLoader(), interfaces, (proxyObj, method, args) -&gt; &#123; Integer returnValue = (Integer) method.invoke(new ProductServiceImpl(), args); return returnValue - discount; &#125;); return (ProductService) proxy; &#125; public static void main(String[] args) throws ClassNotFoundException &#123; ProductServiceProxy proxy = new ProductServiceProxy(); // 获取代理类 ProductService productServiceProxy = proxy.getProxy(ProductServiceImpl.class,new Class[]&#123;ProductService.class&#125;,40); System.out.println(productServiceProxy.getPrice()); &#125;&#125;","tags":[{"name":"Java基础","slug":"Java基础","permalink":"https://seawaylee.github.io/tags/Java基础/"}]},{"title":"JMS消息队列","date":"2017-04-10T05:01:12.000Z","path":"2017/04/10/大数据/大数据基础/JMS/","text":"1 JMS1.1 什么是JMS JMS即Java消息服务（Java Message Service）应用程序接口是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。Java消息服务是一个与具体平台无关的API，绝大多数MOM提供商都对JMS提供支持。 1.2 JMS体系结构JMS由以下元素组成。 JMS提供者provider：连接面向消息中间件的，JMS接口的一个实现。提供者可以是Java平台的JMS实现，也可以是非Java平台的面向消息中间件的适配器。 JMS客户：生产或消费基于消息的Java的应用程序或对象。 JMS生产者：创建并发送消息的JMS客户。 JMS消费者：接收消息的JMS客户。 JMS消息：包括可以在JMS客户之间传递的数据的对象 JMS队列：一个容纳那些被发送的等待阅读的消息的区域。与队列名字所暗示的意思不同，消息的接受顺序并不一定要与消息的发送顺序相同。一旦一个消息被阅读，该消息将被从队列中移走。 JMS主题：一种支持发送消息给多个订阅者的机制。 1.3 ActiveMQ安装 下载、解压缩ActiveMQ 查看配置文件 activemq.xml 启动 activemq.sh start 登陆http://localhost:8161/admin 查看可视化界面 2 JMS应用程序结构支持两种模型2.1 点对点或队列模型在点对点或队列模型下，一个生产者向一个特定的队列发布消息，一个消费者从该队列中读取消息。这里，生产者知道消费者的队列，并直接将消息发送到消费者的队列。 这种模式被概括为： 只有一个消费者将获得消息 生产者不需要在接收者消费该消息期间处于运行状态，接收者也同样不需要在消息发送时处于运行状态。 每一个成功处理的消息都由接收者签收 2.2 发布者/订阅者模型发布者/订阅者模型支持向一个特定的消息主题发布消息。0或多个订阅者可能对接收来自特定消息主题的消息感兴趣。在这种模型下，发布者和订阅者彼此不知道对方。这种模式好比是匿名公告板。 这种模式被概括为： 多个消费者可以获得消息 在发布者和订阅者之间存在时间依赖性。发布者需要建立一个订阅（subscription），以便客户能够订阅。订阅者必须保持持续的活动状态以接收消息，除非订阅者建立了持久的订阅。在那种情况下，在订阅者未连接时发布的消息将在订阅者重新连接时重新发布。 3 Java测试代码Spring整合JMS","tags":[{"name":"高并发","slug":"高并发","permalink":"https://seawaylee.github.io/tags/高并发/"},{"name":"大数据基础","slug":"大数据基础","permalink":"https://seawaylee.github.io/tags/大数据基础/"}]},{"title":"Http请求中Content-Type讲解以及在Spring MVC中的应用","date":"2017-03-29T13:33:14.000Z","path":"2017/03/29/JavaWeb/Http中的Content-Type讲解/","text":"引言 在Http请求中，我们每天都在使用Content-type来指定不同格式的请求信息，但是却很少有人去全面了解content-type中允许的值有多少，这里将讲解Content-Type的可用值，以及在spring MVC中如何使用它们来映射请求信息。 1 Content-Type MediaType，即是Internet Media Type，互联网媒体类型；也叫做MIME类型，在Http协议消息头中，使用Content-Type来表示具体请求中的媒体类型信息。 1234类型格式：type/subtype(;parameter)? type 主类型，任意的字符串，如text，如果是*号代表所有； subtype 子类型，任意的字符串，如html，如果是*号代表所有； parameter 可选，一些参数，如Accept请求头的q参数， Content-Type的 charset参数。 例如： Content-Type: text/html;charset:utf-8; 常见的媒体格式类型如下： text/html ： HTML格式 text/plain ：纯文本格式 text/xml ： XML格式 image/gif ：gif图片格式 image/jpeg ：jpg图片格式 image/png：png图片格式 以application开头的媒体格式类型： application/xhtml+xml ：XHTML格式 application/xml ： XML数据格式 application/atom+xml ：Atom XML聚合格式 application/json ： JSON数据格式 application/pdf ：pdf格式 application/msword ： Word文档格式 application/octet-stream ： 二进制流数据（如常见的文件下载） application/x-www-form-urlencoded ： 中默认的encType，form表单数据被编码为key/value格式发送到服务器（表单默认的提交数据的格式） 另外一种常见的媒体格式是上传文件之时使用的： multipart/form-data ： 需要在表单中进行文件上传时，就需要使用该格式 以上就是我们在日常的开发中，经常会用到的若干content-type的内容格式。 2 Spring MVC中关于关于Content-Type类型信息的使用首先我们来看看RequestMapping中的Class定义： 123456789101112@Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Mappingpublic @interface RequestMapping &#123; String[] value() default &#123;&#125;; RequestMethod[] method() default &#123;&#125;; String[] params() default &#123;&#125;; String[] headers() default &#123;&#125;; String[] consumes() default &#123;&#125;; String[] produces() default &#123;&#125;;&#125; value: 指定请求的实际地址， 比如 /action/info之类。 method： 指定请求的method类型， GET、POST、PUT、DELETE等 consumes： 指定处理请求的提交内容类型（Content-Type），例如application/json, text/html; produces: 指定返回的内容类型，仅当request请求头中的(Accept)类型中包含该指定类型才返回 params： 指定request中必须包含某些参数值是，才让该方法处理 headers： 指定request中必须包含某些指定的header值，才能让该方法处理请求 其中，consumes， produces使用content-typ信息进行过滤信息；headers中可以使用content-type进行过滤和判断。 3 使用示例3.1 SpringMVC最基本配置文件Web应用配置123456789101112131415161718192021222324252627282930313233343536373839404142434445public class SpitterWebInitializer extends AbstractAnnotationConfigDispatcherServletInitializer&#123; /** * Spring上下文配置 * ContextLoaderListener * @Author NikoBelic * @Date 09/01/2017 20:40 */ @Override protected Class&lt;?&gt;[] getRootConfigClasses() &#123; return new Class&lt;?&gt;[]&#123;RootConfig.class&#125;; &#125; /** * SpringMVC上下文配置 * DisparcherServlet * @Author NikoBelic * @Date 09/01/2017 20:41 */ @Override protected Class&lt;?&gt;[] getServletConfigClasses() &#123; return new Class&lt;?&gt;[]&#123;WebConfig.class&#125;; &#125; /** * SpringMVC请求拦截,将DispatcherServlet映射到/ * @Author NikoBelic * @Date 09/01/2017 20:41 */ @Override protected String[] getServletMappings() &#123; return new String[]&#123;\"/\"&#125;; &#125; /** * 配置文件上传限制 * @Author NikoBelic * @Date 09/01/2017 20:40 */ @Override protected void customizeRegistration(ServletRegistration.Dynamic registration) &#123; registration.setMultipartConfig(new MultipartConfigElement(\"/Users/lixiwei-mac/Desktop/tmp\",20971520, 41943040, 0)); &#125;&#125; DispatcherServlet配置1234567@Configuration@EnableWebMvc@ComponentScan(\"mvc.controller\")public class WebConfig extends WebMvcConfigurerAdapter&#123;&#125; Listener配置 1234@Configurationpublic class RootConfig&#123;&#125; 3.2 测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132@Controller@RequestMapping(value = \"/mvc\")public class TestController&#123; /** * 测试启动 * * @Author SeawayLee * @Date 2017/03/29 22:08 */ @ResponseBody @RequestMapping(value = \"/\") public Object helloMVC(HttpServletRequest request, HttpServletResponse response) &#123; return \"Let's Go\"; &#125; /** * 测试Content-Type * * @Author SeawayLee * @Date 2017/03/29 22:24 */ @ResponseBody @RequestMapping(value = \"/testContentType\", consumes = \"text/html;charset=utf-8\", produces = \"application/json;charset=utf-8\", params = &#123;\"patientId\"&#125;, headers = &#123;\"ABC\"&#125;) public Object testContentType(HttpServletRequest request, HttpServletResponse response, String patientId) &#123; return patientId + \"\\n\" + request.getContentType(); &#125; /** * 这种参数接收方式，实质上是UrlParam * UrlParam一定会导致中文乱码，除非客户端手动将中文URLEncode * * @Author SeawayLee * @Date 2017/03/29 22:49 */ @ResponseBody @RequestMapping(value = \"/getJson1\", method = RequestMethod.POST, produces = \"application/json;charset=utf-8\") public Object getJson1(HttpServletRequest request, HttpServletResponse response, String patientId, String patientName) throws UnsupportedEncodingException &#123; System.out.println(patientName); return new Patient(patientId, patientName); &#125; /** * @ReuqestBody 指明被注解的参数是请求的body，可以是一个json结构，只要双方字符集统一，不会出现中文乱码 * @Author SeawayLee * @Date 2017/03/29 23:14 */ @ResponseBody @RequestMapping(value = \"/getJson2\", method = RequestMethod.POST, produces = \"application/json\") public Object getJson2(HttpServletRequest request, @RequestBody JSONObject patientJson) throws UnsupportedEncodingException &#123; System.out.println(patientJson); return patientJson; &#125; /** * 和上面一样 * @Author SeawayLee * @Date 2017/03/29 23:16 */ @ResponseBody @RequestMapping(value = \"/getJson3\", method = RequestMethod.POST, produces = \"application/json\") public Object getJson3(HttpServletRequest request, @RequestBody Map&lt;String, String&gt; params) throws UnsupportedEncodingException &#123; System.out.println(params); return params; &#125; /** * 如果请求参数中既有json，又有独立的参数，客户端需要将数据以表单的形式提交 * @Author SeawayLee * @Date 2017/03/29 23:36 */ @ResponseBody @RequestMapping(value = \"/getJson4\", method = RequestMethod.POST, produces = \"application/json;charset=utf-8\") public Object getJson4(HttpServletRequest request, @RequestParam(required = true) String patientJson,@RequestParam(required = false) String city) throws UnsupportedEncodingException &#123; System.out.println(patientJson); System.out.println(city); return patientJson; &#125; /** * 同上 * @Author SeawayLee * @Date 2017/03/29 23:40 */ @ResponseBody @RequestMapping(value = \"/getJson5\", method = RequestMethod.POST, produces = \"application/json;charset=utf-8\") public Object getJson5(HttpServletRequest request) throws UnsupportedEncodingException &#123; String patientJson = request.getParameter(\"patientJson\"); String city = request.getParameter(\"city\"); System.out.println(patientJson); System.out.println(city); return patientJson; &#125; class Patient implements Serializable &#123; private String patientId; private String patientName; public Patient(String patientId, String patientName) &#123; this.patientId = patientId; this.patientName = patientName; &#125; public String getPatientId() &#123; return patientId; &#125; public void setPatientId(String patientId) &#123; this.patientId = patientId; &#125; public String getPatientName() &#123; return patientName; &#125; public void setPatientName(String patientName) &#123; this.patientName = patientName; &#125; &#125;&#125; Content-Type测试结果 RequestMapping配置 错误代码 原因分析 consumes = “application/json;charset=utf-8” 415 Unsupported Media Type 客户端没有设置对应的媒体类型 params = {“patientId”} 400 Bad Request 客户端没有穿啊书必要的参数 headers = {“ABC”} 404 Not Found 客户端没有传输必要的headers","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"}]},{"title":"SecureCRT使用日常快捷键","date":"2017-03-28T10:37:34.000Z","path":"2017/03/28/工具/SecureCRT/","text":"文件传输 使用 rz 传输文件 使用 Cmd + Shift + P 传输文件(直接拖船或put) 命令 Ctrl + l 清屏 Ctrl + a 跳转到行首 Ctrl + e 跳转到行末 Cmd + Enter 全屏 Cmd + K 快速创建新Session Option + Cmd + m 切换到SessionManager Option + Cmd + g 切换到CommandWindow （可以给所有Session发送命令）","tags":[{"name":"工具","slug":"工具","permalink":"https://seawaylee.github.io/tags/工具/"}]},{"title":"ZooKeeper学习笔记","date":"2017-03-28T06:43:47.000Z","path":"2017/03/28/大数据/大数据基础/ZooKeeper/","text":"1 概念简介 Zookeeper是一个分布式协调服务；就是为用户的分布式应用程序提供协调服务 zookeeper是为别的分布式程序服务的 Zookeeper本身就是一个分布式程序（只要有半数以上节点存活，zk就能正常服务） Zookeeper所提供的服务涵盖：主从协调、服务器节点动态上下线、统一配置管理、分布式共享锁、统一名称服务…… 虽然说可以提供各种服务，但是zookeeper在底层其实只提供了两个功能： 管理(存储，读取)用户程序提交的数据； 并为用户程序提供数据节点监听服务； Zookeeper集群的角色： Leader 和 follower （Observer）只要集群中有半数以上节点存活，集群就能提供服务 2 安装、配置安装 wget –no-check-certificate https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/zookeeper-3.4.5.tar.gz tar -zxvf zookeeper-3.4.5.tar.gz 配置 修改ZooKeeper配置文件12cd zookeeper/confcp zoo_sample.cfg zoo.cfg 添加和修改以下内容 vim zoo.cfg 12345dataDir=/opt/data/zookeeperdataLogDir=/opt/data/zookeeper/logserver.1=hadoop1:2888:3888 (主机名, 心跳端口、数据端口)server.2=hadoop1:2888:3888server.3=hadoop1:2888:3888 创建数据存储文件夹 1234mkdir -m 755 /opt/data/zookeepermkdir -m 755 /opt/data/zookeeper/logcd /opt/data/zookeeperecho 1 &gt; myid # 其他两台机器设置为2，3 环境变量配置 vim /etc/profile12export ZOOKEEPER_HOME=/home/hadoop/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin 启动 分别启动三个节点zkServer.sh start 查看节点状态 zkServer.sh status 3 ZooKeeper结构和命令3.1 ZK特性 Zookeeper：一个leader，多个follower组成的集群 全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的 分布式读写，更新请求转发，由leader实施 更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行 数据更新原子性，一次数据更新要么成功，要么失败 实时性，在一定时间范围内，client能读到最新数据 3.2 ZK数据结构 层次化的目录结构，命名符合常规文件系统规范(见下图) 每个节点在zookeeper中叫做znode,并且其有一个唯一的路径标识 节点Znode可以包含数据和子节点（但是EPHEMERAL类型的节点不能有子节点，下一页详细讲解） 客户端应用可以在节点上设置监视器（后续详细讲解） 3.3 节点类型 Znode有两种类型： 短暂（ephemeral）（断开连接自己删除） 持久（persistent）（断开连接不删除） Znode有四种形式的目录节点（默认是persistent ） PERSISTENT PERSISTENT_SEQUENTIAL（持久序列/test0000000019 ） EPHEMERAL EPHEMERAL_SEQUENTIAL 创建znode时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护 在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序 3.4 ZK命令行操作 ls / 使用 ls 命令来查看当前 ZooKeeper 中所包含的内容 create /zk &quot;myData“ 创建一个新的 znode ，使用 create /zk myData 。这个命令创建了一个新的 znode 节点“ zk ”以及与它关联的字符串： get /zk 我们运行 get 命令来确认 znode 是否包含我们所创建的字符串 get /zk watch 监听这个节点的变化,当另外一个客户端改变/zk时,它会打出下面的12WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/zk set /zk &quot;zsl“ 通过 set 命令来对 zk 所关联的字符串进行设置： delete /zk 将刚才创建的 znode 删除： rmr /zk 删除节点 4 ZK-API 应用4.1 ZooKeeper API 描述 功能 描述 create 在本地目录树中创建一个节点 delete 删除一个节点 exists 测试本地是否存在目标节点 get/set data 从目标节点上读取 / 写数据 get/set ACL 获取 / 设置目标节点访问控制列表信息 get children 检索一个子节点上的列表 sync 等待要被传送的数据 4.2 Demo CRUD123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110/** * ZooKeeper Java客户端API * @author NikoBelic * @create 2017/3/29 10:53 */public class SimpleZkClient&#123; private static final String connString = \"hadoop1:2181,hadoop2:2181,hadoop3:2181\"; private static final int sessionTimeout = 2000; ZooKeeper zkClient = null; /** * 创建连接、监听回调函数 * @Author SeawayLee * @Date 2017/03/29 13:52 */ @Before public void init() throws IOException &#123; zkClient = new ZooKeeper(connString, sessionTimeout, (watchedEvent) -&gt; &#123; // 收到时间通知后的回调函数（事件处理逻辑） System.out.println(watchedEvent.getType() + \"===\" + watchedEvent.getPath()); try &#123; zkClient.getChildren(\"/\", true); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;); &#125; /** * 创建节点 * @Author SeawayLee * @Date 2017/03/29 13:52 */ @Test public void testCreate() throws KeeperException, InterruptedException &#123; // Params(创建节点的路径，节点数据，节点访问权限，节点类型) String nodeCreated = zkClient.create(\"/java_test2\", \"HelloWorld\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125; /**获取所有节点 * * @Author SeawayLee * @Date 2017/03/29 13:52 */ @Test public void getChildren() throws KeeperException, InterruptedException &#123; List&lt;String&gt; children = zkClient.getChildren(\"/\", true); for (String child : children) &#123; System.out.println(child); &#125; TimeUnit.SECONDS.sleep(Long.MAX_VALUE); &#125; /** * 判断节点是否存在 * @Author SeawayLee * @Date 2017/03/29 13:51 */ @Test public void testExist() throws KeeperException, InterruptedException &#123; Stat exists = zkClient.exists(\"/java_test2\", false); System.out.println(exists == null ? \"节点不存在\" : \"节点存在\"); &#125; /** * 获取节点 * @Author SeawayLee * @Date 2017/03/29 13:51 */ @Test public void getData() throws KeeperException, InterruptedException &#123; byte[] data = zkClient.getData(\"/java_test2\", false, null); System.out.println(new String(data)); &#125; /** * 删除节点 * @Author SeawayLee * @Date 2017/03/29 13:51 */ @Test public void deleteZnode() throws KeeperException, InterruptedException &#123; // 参数2：指定要删除的版本，-1表示删除所有版本 zkClient.delete(\"/java_test2\", -1); &#125; /** * 更新节点 * @Author SeawayLee * @Date 2017/03/29 13:51 */ @Test public void updateZnode() throws KeeperException, InterruptedException &#123; zkClient.setData(\"/test\", \"update test\".getBytes(), -1); System.out.println(new String(zkClient.getData(\"/test\", false, null))); &#125;&#125; 4.3 监听器的工作机制 监听器是一个接口，我们的代码中可以实现Wather这个接口，实现其中的process方法，方法中即我们自己的业务逻辑 监听器的注册是在获取数据的操作中实现： getData(path,watch?)监听的事件是：节点数据变化事件 getChildren(path,watch?)监听的事件是：节点下的子节点增减变化事件 5 实现分布式应用的（主节点HA）客户端动态更新主节点状态5.1 需求 某分布式系统中，主节点可以有多台，可以动态上下线 任意一台客户端都能实时感知到主节点服务器的上下线 5.2 代码实现服务端 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * 分布式应用服务端主节点注册 * * @author NikoBelic * @create 2017/3/30 17:21 */public class DistributedServer&#123; private static final String connStr = \"hadoop1:8571,hadoop2:8572,hadoop3:8573\"; private static final String parentNode = \"/servers\"; private ZooKeeper zkClient = null; private void connectToZk() throws IOException &#123; zkClient = new ZooKeeper(connStr, 5000, (watchedEvent) -&gt; &#123; try &#123; zkClient.getChildren(parentNode, false, null); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; private void registServer(String serverName) throws KeeperException, InterruptedException &#123; String created = zkClient.create(parentNode + \"/\" + serverName, serverName.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(serverName + \" is online..\" + created); &#125; private void handleBussiness(String serverName) throws InterruptedException &#123; System.out.println(serverName + \"start working...\"); TimeUnit.SECONDS.sleep(Integer.MAX_VALUE); &#125; public static void main(String[] args) throws IOException, KeeperException, InterruptedException &#123; // 获取zk连接 DistributedServer distributedServer = new DistributedServer(); String serverName = \"server03\"; distributedServer.connectToZk(); // 当一台服务器连接，则创建一个临时节点 distributedServer.registServer(serverName ); // 服务端开始处理业务 distributedServer.handleBussiness(serverName ); &#125;&#125; 客户端 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class DistributedClient&#123; private static final String connStr = \"hadoop1:8571,hadoop2:8572,hadoop3:8573\"; private static final String parentNode = \"/servers\"; private volatile List&lt;String&gt; servers; private ZooKeeper zkClient = null; private void connectToZk() throws IOException &#123; zkClient = new ZooKeeper(connStr, 5000, (watchedEvent) -&gt; &#123; try &#123; // 当接收到节点变化事件，重新获取服务器列表，并再次建立监听 getServerList(); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; /** * 获取服务器子节点信息，并对父节点进行监听 * * @Author SeawayLee * @Date 2017/03/30 20:09 */ private void getServerList() throws KeeperException, InterruptedException &#123; List&lt;String&gt; children = zkClient.getChildren(parentNode, true); if (servers == null) servers = new ArrayList&lt;&gt;(); servers.clear(); for (String child : children) &#123; byte[] data = zkClient.getData(parentNode + \"/\" + child, false, null); servers.add(new String(data)); &#125; // 打印服务器列表 System.out.println(servers.toString()); &#125; private void handleBussiness() throws InterruptedException &#123; System.out.println(\"Client start working...\"); TimeUnit.SECONDS.sleep(Integer.MAX_VALUE); &#125; public static void main(String[] args) throws IOException, KeeperException, InterruptedException &#123; DistributedClient distributedClient = new DistributedClient(); distributedClient.connectToZk(); distributedClient.getServerList(); distributedClient.handleBussiness(); &#125;&#125; 6 分布式锁的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118public class DistributeLock&#123; // 超时时间 private static final int SESSION_TIMEOUT = 5000; // zookeeper server列表 private String hosts = \"hadoop1:8571,hadoop2:8572,hadoop3:8573\"; private String groupNode = \"locks\"; private String subNode = \"sub\"; private ZooKeeper zk; // 当前client创建的子节点 private String thisPath; // 当前client等待的子节点 private String waitPath; private CountDownLatch latch = new CountDownLatch(1); /** * 连接zookeeper */ public void connectZookeeper() throws Exception &#123; zk = new ZooKeeper(hosts, SESSION_TIMEOUT, event -&gt; &#123; try &#123; // 连接建立时, 打开latch, 唤醒wait在该latch上的线程 if (event.getState() == Watcher.Event.KeeperState.SyncConnected) &#123; latch.countDown(); &#125; // 发生了waitPath的删除事件 if (event.getType() == Watcher.Event.EventType.NodeDeleted &amp;&amp; event.getPath().equals(waitPath)) &#123; doSomething(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;); // 等待连接建立 latch.await(); // 创建子节点 thisPath = zk.create(\"/\" + groupNode + \"/\" + subNode, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); // wait一小会, 让结果更清晰一些 Thread.sleep(10); // 注意, 没有必要监听\"/locks\"的子节点的变化情况 List&lt;String&gt; childrenNodes = zk.getChildren(\"/\" + groupNode, false); // 列表中只有一个子节点, 那肯定就是thisPath, 说明client获得锁 if (childrenNodes.size() == 1) &#123; doSomething(); &#125; else &#123; String thisNode = thisPath.substring((\"/\" + groupNode + \"/\").length()); // 排序 Collections.sort(childrenNodes); int index = childrenNodes.indexOf(thisNode); if (index == -1) &#123; // never happened &#125; else if (index == 0) &#123; // inddx == 0, 说明thisNode在列表中最小, 当前client获得锁 doSomething(); &#125; else &#123; // 获得排名比thisPath前1位的节点 this.waitPath = \"/\" + groupNode + \"/\" + childrenNodes.get(index - 1); // 在waitPath上注册监听器, 当waitPath被删除时, zookeeper会回调监听器的process方法 zk.getData(waitPath, true, new Stat()); &#125; &#125; &#125; private void doSomething() throws Exception &#123; try &#123; System.out.println(\"gain lock: \" + thisPath); Thread.sleep(2000); // do something &#125; finally &#123; System.out.println(\"finished: \" + thisPath); // 将thisPath删除, 监听thisPath的client将获得通知 // 相当于释放锁 zk.delete(this.thisPath, -1); &#125; &#125; public static void main(String[] args) throws Exception &#123; for (int i = 0; i &lt; 2; i++) &#123; Thread t = new Thread(() -&gt; &#123; try &#123; DistributeLock dl = new DistributeLock(); dl.connectZookeeper(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;); t.start(); &#125; Thread.sleep(Long.MAX_VALUE); &#125;&#125; 7 ZooKeeper原理 Zookeeper虽然在配置文件中并没有指定master和slave 但是，zookeeper工作时，是有一个节点为leader，其他则为follower Leader是通过内部的选举机制临时产生的 7.1 ZooKeeper的选举机制（全新集群paxos）以一个简单的例子来说明整个选举的过程.假设有五台服务器组成的zookeeper集群,它们的id从1-5,同时它们都是最新启动的,也就是没有历史数据,在存放数据量这一点上,都是一样的.假设这些服务器依序启动,来看看会发生什么. 1) 服务器1启动,此时只有它一台服务器启动了,它发出去的报没有任何响应,所以它的选举状态一直是LOOKING状态 2) 服务器2启动,它与最开始启动的服务器1进行通信,互相交换自己的选举结果,由于两者都没有历史数据,所以id值较大的服务器2胜出,但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3),所以服务器1,2还是继续保持LOOKING状态. 3) 服务器3启动,根据前面的理论分析,服务器3成为服务器1,2,3中的老大,而与上面不同的是,此时有三台服务器选举了它,所以它成为了这次选举的leader. 4) 服务器4启动,根据前面的分析,理论上服务器4应该是服务器1,2,3,4中最大的,但是由于前面已经有半数以上的服务器选举了服务器3,所以它只能接收当小弟的命了. 5) 服务器5启动,同4一样,当小弟. 7.2 非全新集群的选举机制(数据恢复)那么，初始化的时候，是按照上述的说明进行选举的，但是当zookeeper运行了一段时间之后，有机器down掉，重新选举时，选举过程就相对复杂了。需要加入数据id、leader id和逻辑时钟。数据id：数据新的id就大，数据每次更新都会更新id。Leader id：就是我们配置的myid中的值，每个机器一个。逻辑时钟：这个值从0开始递增,每次选举对应一个值,也就是说: 如果在同一次选举中,那么这个值应该是一致的 ; 逻辑时钟值越大,说明这一次选举leader的进程更新.选举的标准就变成： 1、逻辑时钟小的选举结果被忽略，重新投票 2、统一逻辑时钟后，数据id大的胜出 3、数据id相同的情况下，leader id大的胜出根据这个规则选出leader。","tags":[{"name":"高并发","slug":"高并发","permalink":"https://seawaylee.github.io/tags/高并发/"},{"name":"大数据基础","slug":"大数据基础","permalink":"https://seawaylee.github.io/tags/大数据基础/"}]},{"title":"Linux基本操作","date":"2017-03-22T12:09:06.000Z","path":"2017/03/22/大数据/大数据基础/Linux环境基本操作/","text":"配置三台Linux服务器，为Hadoop集群准备硬件资源。 1 设备条件 性能: 由一台32G的8Core服务器虚拟出3台(4G 4Core)CentOS7服务器,内存固定分配，Core根据使用状态动态调节 查看CPU信息 cat /proc/cpuinfo 查看内存信息 free -h IP地址: 10.5.151.241 10.5.151.242 10.5.151.243 VM网络配置使用的桥接模式，使用真实的网络地址，Bridge适合远程操作；而NAT模式适合将虚拟机装在本机，使用虚拟网关，如果是在笔记本电脑中虚拟机器可以使用此方式。 2 操作2.1 Vim基本操作一般模式 a 在光标后一位开始插入 A 在该行的最后插入 I 在该行的最前面插入 gg 直接跳到文件的首行 G 直接跳到文件的末行 dd 删除行，如果 5dd ，则一次性删除光标后的5行 yy 复制当前行, 复制多行，则 3yy，则复制当前行附近的3行 p 粘贴 v 进入字符选择模式，选择完成后，按y复制，按p粘贴 ctrl+v 进入块选择模式，选择完成后，按y复制，按p粘贴 shift+v 进入行选择模式，选择完成后，按y复制，按p粘贴 命令模式 查找并替换（在底行中输入） %s/sad/666 查找文件中所有sad，666 /you 查找文件中出现的you，并定位到第一个找到的地方，按n可以定位到下一个匹配位置（按N定位到上一个） 2.2 日常操作 pwd 显示当前目录 date 查看当前时间 who 查看当前登录到服务器的人 last 查看最近的登陆历史记录 2.3 文件权限操作 chmod g-rw haha.dat 表示将haha.dat对所属组的rw权限取消 chmod o-rw haha.dat 表示将haha.dat对其他人的rw权限取消 chmod u+x haha.dat 表示将haha.dat对所属用户的权限增加x 也可以用数字的方式来修改权限 chmod 664 haha.dat 就会修改成 rw-rw-r– 如果要将一个文件夹的所有内容权限统一修改，则可以-R参数 chmod -R 770 aaa/ chown angela:angela aaa/ &lt;只有root能执行&gt; 目录没有执行权限的时候普通用户不能进入文件只有读写权限的时候普通用户是可以删除的(删除文件不是修改它,是操作父及目录),只要父级目录有执行和修改的权限 2.4 用户管理 useradd hadoop 添加用户hadoop passwd hadoop 修改用户密码 为hadoo用户配置sudo的使用权限 vim /etc/sudoers hadoop ALL=(ALL) ALL 2.5 系统管理操作 hostname 查看主机名 hostname hadoop 修改主机名,重启后无效 vim /etc/sysconfig/network 修改主机名，重启后永久生效 ifconfig eth0 192.168.1.100 修改IP，重启后无效 vim /etc/sysconfig/network-scripts/ifcfg-eth0 修改IP 重启网卡后永久生效 挂载外置磁盘 mkdir /mnt/udisk 创建一个目录用来挂载 mount -t iso1234 ro /dev/sd4 /mnt/udisk 将设备/dev/sd4挂在到/mnt/udisk中 umount /mnt/udisk 解除挂载 du -sh /var 查看文件夹大小sh run df -h 查看磁盘空间 halt 关机 reboot 重启 uname -a 查看系统信息 2.6 查看文件操作 cat somefile 一次性将文件内容全部输出（控制台） more somefile 可以翻页查看, 下翻一页(空格) 上翻一页（b） 退出（q） less somefile 可以翻页查看,下翻一页(空格) 上翻一页（b），上翻一行(↑) 下翻一行（↓） 可以搜索关键字（/keyword） tail -10 install.log 查看文件尾部的10行 tail -f install.log 小f跟踪文件的唯一inode号，就算文件改名后，还是跟踪原来这个inode表示的文件 tail -F install.log 大F按照文件名来跟踪 head -10 install.log 查看文件头部的10行 2.7 后台服务管理 service network status 查看指定服务的状态 service network stop 停止指定服务 service network start 启动指定服务 service network restart 重启指定服务 service –status-all 查看系统中所有的后台服务 2.8 系统启动级别管理vi /etc/inittab 2.9 设置后台服务的自启配置 chkconfig 查看所有服务器自启配置 chkconfig iptables off 关掉指定服务的自动启动 chkconfig iptables on 开启指定服务的自动启动 12345678910# Default runlevel. The runlevels used are:# 0 - halt (Do NOT set initdefault to this)# 1 - Single user mode# 2 - Multiuser, without NFS (The same as 3, if you do not have networking)# 3 - Full multiuser mode# 4 - unused# 5 - X11# 6 - reboot (Do NOT set initdefault to this)#id:3:initdefault: 2.10 压缩解压缩 打包解包 gzip access.log 压缩文件 gzip -d access.log.gz tar -cvf mytarball.tar aaa/ 打包aaa文件夹 tar -xvf mytarball.tar 解包 tar -zcvf my.tar.gz aaa/ 压缩并打包 tar -zxvf my.tar.gz 解包解压缩 2.11 wget 使用wget –no-check-certificate https:xxxx/mysql-5.7.17-linux-glibc2.5-x86_64.tar.gz 下载https路径的文件 2.12 rpm 使用 rpm -qa | grep mysql 查看系统中安装的rpm包 rpm -ivh perl* 安装perl依赖 《可能会提示有包冲突，解决： rpm -e 冲突包名 –nodeps 》 rpm -ivh MySQL-server-5.5.48-1.linux2.6.x86_64.rpm 安装server rpm -ivh MySQL-client-5.5.48-1.linux2.6.x86_64.rpm 安装clent 2.13 后台服务管理 chkconfig 查看所有服务器自启配置 chkconfig iptables off 关掉指定服务的自动启动 chkconfig iptables on 开启指定服务的自动启动 3 配置3.1 配置hostname12345vim /etc/hosts10.5.151.241 hadoop110.5.151.242 hadoop210.5.151.241 hadoop3 3.2 配置SSH免密登陆若 A 需要登录 B A上操作 ssh-keygen 生成密钥对 ssh-copy-id B 将A的公钥复制并追加到B的授权列表文件 authorized_keys 中 3.3 Clone Linux 遇到的问题 从一台配置好的linux克隆多个系统时，由于物理网卡地址重复，因此生成了一块新的网卡eth1，原来了eth0不见了 直接修改ifcfg-eth0 、删除UUID和HWARE、配置静态地址 rm -rf /etc/udev/rules.d/70-persisitent-net.rules 或 将eth1的无礼地址复制给eth0 reboot 4 安装Mysql 卸载自带mysql rpm -qa | grep mysql* rpm -qa | grep maria* rpm -e 以上查询结果 yum list installed mysql* yum erase 以上查询结果 find / -name mysql 逐一删除 下载mysql wget –no-check-certificate https://cdn.mysql.com//Downloads/MySQL-5.6/MySQL-5.6.35-1.linux_glibc2.5.x86_64.rpm-bundle.tar tar -xvf 下载结果 rpm -ivh mysql-server 和 mysql-client 启动、连接 systemctl start mysql 启动服务 cat /root/.mysql_secret 查找默认密码 mysql_secure_installation 修改默认密码 mysql -uroot -pxxxxxx 连接mysql use mysql; select host,user from user; 查看用户 update user set host = ‘%’ where user = ‘root’; 设置允许远程登录 flush privileges; 刷新 5 iptables防火墙设置 systemctl status/start/stop/restart firewalld 查看/启动/关闭/重启防火墙服务 查看 iptables -L -n –line-numbers 查看所有规则 iptables -L -n -t nat 列出iptables nat表规则（默认是filter表） 防火墙规则 -A, –append chain 追加到规则的最后一条 -D, –delete chain [rulenum] Delete rule rulenum (1 = first) from chain -I, –insert chain [rulenum] Insert in chain as rulenum (default 1=first) 添加到规则的第一条 -p, –proto proto protocol: by number or name, eg. ‘tcp’,常用协议有tcp、udp、icmp、all -j, –jump target 常见的行为有ACCEPT、DROP和REJECT三种，但一般不用REJECT，会带来安全隐患 修改 iptables -F 清除默认规则（注意默认是filter表，如果对nat表操作要加-t nat） service iptables save 保存配置 实例 iptables -A INPUT -p tcp –dport 22 -j DROP 禁止ssh登陆（若果服务器在机房，一定要小心） iptables -D INPUT -p tcp –dport 22 -j DROP 删除规则 iptables -A INPUT -p tcp -i eth0 -s 192.168.33.0 -j DROP 禁止192.168.33.0网段从eth0网卡接入 iptables -A INPUT -p tcp –dport 22 -i eth0 -s 192.168.33.61 -j ACCEPT iptables -A INPUT ! -s 192.168.10.10 -j DROP 禁止ip地址非192.168.10.10的所有类型数据接入 iptables -I INPUT -p icmp –icmp-type 8 -s 192.168.50.100 -j DROP 禁止ip地址非192.168.10.100的ping请求 iptables -I INPUT -p tcp –dport 22:80 -j DROP 匹配端口范围 iptables -I INPUT -p tcp -m multiport –dport 22,80,3306 -j ACCEPT 匹配多个端口 iptables -I OUTPUT -p tcp –sport 80 -j DROP 不允许源端口为80的数据流出 扩展匹配：1.隐式扩展 2.显示扩展隐式扩展 -p tcp –sport PORT 源端口 –dport PORT 目标端口 显示扩展：使用额外的匹配规则 -m EXTENSTION –SUB-OPT -p tcp –dport 22 与 -p tcp -m tcp –dport 22功能相同 state：状态扩展，接口ip_contrack追踪会话状态 NEW：新的连接请求 ESTABLISHED：已建立的连接请求 INVALID：非法连接 RELATED：相关联的连接 6 自动化shell脚本6.1 自动配置免密登陆123456789101112131415161718192021222324252627282930SERVERS=&quot;node-3.itcast.cn node-4.itcast.cn&quot;PASSWORD=123456BASE_SERVER=172.16.203.100auto_ssh_copy_id() &#123; expect -c &quot;set timeout -1; spawn ssh-copy-id $1; expect &#123; *(yes/no)* &#123;send -- yes\\r;exp_continue;&#125; *assword:* &#123;send -- $2\\r;exp_continue;&#125; eof &#123;exit 0;&#125; &#125;&quot;;&#125;ssh_copy_id_to_all() &#123; for SERVER in $SERVERS do auto_ssh_copy_id $SERVER $PASSWORD done&#125;ssh_copy_id_to_allfor SERVER in $SERVERSdo scp install.sh root@$SERVER:/root ssh root@$SERVER /root/install.shdone 6.2 自动下载部署tomcat脚本12345678BASE_SERVER=mini4yum install -y wgetwget $BASE_SERVER/soft/jdk-7u45-linux-x64.tar.gztar -zxvf jdk-7u45-linux-x64.tar.gz -C /usr/localcat &gt;&gt; /etc/profile &lt;&lt; EOFexport JAVA_HOME=/usr/local/jdk1.7.0_45export PATH=\\$PATH:\\$JAVA_HOME/binEOF","tags":[{"name":"Linux","slug":"Linux","permalink":"https://seawaylee.github.io/tags/Linux/"},{"name":"大数据","slug":"大数据","permalink":"https://seawaylee.github.io/tags/大数据/"}]},{"title":"Java Collections Framework - ArrayList","date":"2017-03-22T02:34:41.000Z","path":"2017/03/22/Java基础/集合/Java Collections Framework - ArrayList/","text":"定义 ArrayList底层以数组实现，允许重复，默认第一次插入元素时创建数组的大小为10，超出限制时会增加50%的容量，每次扩容都底层采用System.arrayCopy()复制到新的数组，初始化时最好能给出数组大小的预估值。 12345678package java.util;public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable &#123; private static final int DEFAULT_CAPACITY = 10; private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;; private transient Object[] elementData; private int size; //其余省略&#125; 概述按数组下标访问元素—get(i)/set(i,e) 的性能很高，这是数组的基本优势。 1234567891011public E get(int index) &#123; rangeCheck(index); return elementData(index);&#125;public E set(int index, E element) &#123; rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125; 直接在数组末尾加入元素—add(e)的性能也高，但如果按下标插入、删除元素—add(i,e), remove(i), remove(e)，则要用System.arraycopy()来移动部分受影响的元素，性能就变差了，这是劣势。 ArrayList中有一个方法trimToSize()用来缩小elementData数组的大小，这样可以节约内存： 123456public void trimToSize() &#123; modCount++; if (size &lt; elementData.length) &#123; elementData = Arrays.copyOf(elementData, size); &#125; &#125; 考虑这样一种情形，当某个应用需要，一个ArrayList扩容到比如size=10000，之后经过一系列remove操作size=15，在后面的很长一段时间内这个ArrayList的size一直保持在&lt;100以内，那么就造成了很大的空间浪费，这时候建议显式调用一下trimToSize()这个方法，以优化一下内存空间。 或者在一个ArrayList中的容量已经固定，但是由于之前每次扩容都扩充50%，所以有一定的空间浪费，可以调用trimToSize()消除这些空间上的浪费。 RandomAccess这个接口有什么用？实现RandomAccess接口的集合有：ArrayList, AttributeList, CopyOnWriteArrayList, RoleList, RoleUnresolvedList, Stack, Vector等。在RandomAccess接口的注释中有这么一段话： 1234567for (int i=0, n=list.size(); i &lt; n; i++) &#123; list.get(i);&#125;runs faster than this loop:for (Iterator i=list.iterator(); i.hasNext(); ) &#123; i.next();&#125; 说明实现了RandomAccess接口的集合，在数据量很大的情况下，采用迭代器遍历比较慢。 和LinkedList的区别1、ArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。2、对于随机访问get和set，ArrayList觉得优于LinkedList，因为LinkedList要移动指针。3、对于新增和删除操作add和remove（不是在尾部添加删除），LinkedList比较占优势，因为ArrayList要移动数据。 和Vector的区别1、Vector和ArrayList几乎是完全相同的,唯一的区别在于Vector是同步类(synchronized)，属于强同步类。因此开销就比ArrayList要大，访问要慢。正常情况下,大多数的Java程序员使用ArrayList而不是Vector,因为同步完全可以由程序员自己来控制。2、Vector每次扩容请求其大小的2倍空间，而ArrayList是1.5倍。3、Vector还有一个子类Stack.","tags":[{"name":"Java基础","slug":"Java基础","permalink":"https://seawaylee.github.io/tags/Java基础/"}]},{"title":"剑指Offer_7_用两个栈实现一个队列","date":"2017-03-21T14:03:45.000Z","path":"2017/03/21/面试/剑指Offer/Q_7_用两个栈实现一个队列/","text":"题目描述 使用两个栈实现一个队列的push和pop方法 解题思路 入栈时直接将数据存储到s1中，栈顶为最新元素； 出栈时查看s2中是否有数据，如果没有，则将s1中的数据弹出并压入s2，弹出s2栈顶元素为最旧元素，弹出即可； 当s1将所有数据压入s2后，s2仍然为空，则表示队列空； 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class Solution&#123; Stack&lt;Integer&gt; stack1 = new Stack&lt;Integer&gt;(); Stack&lt;Integer&gt; stack2 = new Stack&lt;Integer&gt;(); public void push(int x) &#123; stack1.push(x); &#125; public int pop() &#123; if (stack2.isEmpty()) &#123; while (stack1.size() &gt; 0) stack2.push(stack1.pop()); &#125; if (stack2.isEmpty()) &#123; try &#123; throw new Exception(\"没有数据了\"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; return stack2.pop(); &#125; public static void main(String[] args) &#123; Solution solution = new Solution(); for (int i = 0; i &lt; 5; i++) &#123; solution.push(i); &#125; System.out.println(solution.pop()); System.out.println(solution.pop()); System.out.println(solution.pop()); System.out.println(solution.pop()); System.out.println(solution.pop()); System.out.println(solution.pop()); &#125;&#125; 补充 使用两个队列实现一个栈。解法类似。","tags":[{"name":"剑指Offer","slug":"剑指Offer","permalink":"https://seawaylee.github.io/tags/剑指Offer/"}]},{"title":"剑指Offer_6_重建二叉树","date":"2017-03-16T11:56:11.000Z","path":"2017/03/16/面试/剑指Offer/Q_6_重建二叉树/","text":"题目描述 输入某二叉树的前序遍历和中序遍历，请重建出该二叉树。假设输入的 先序遍历和中序遍历的结果中不含重复的数字。例如输入先序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建出图2.6所示的二叉树并输出它的头结点。 解题思路 先序遍历的结果第一个节点一定是根节点 根据先序遍历的第一个节点，即根节点，确定中序遍历中根节点的位置记为index 中序遍历中根节点的左边都是左子树上的节点，右边都是右子数上的节点 先序遍历中，前面index长度的节点为左子树，之后的都为右子树的节点 递归构建左右子树 详细解释 在二叉树的先序遍历中，第一个数字总是树的根节点的值。但在中序遍历中，根节点的值在序列中间，左子树的节点值位于根节点的值的左边，而右子树的节点的值位于根节点的右边。因此我们需要扫描中序遍历序列，才能找到根节点的值。 由于在中序遍历序列中，有n个数字是左子树节点的值，因此左子树总共有n个左子节点。同样，在先序遍历中，根节点后边的n个数字就是n个左子树节点的值右子树节点的值。这样我们就在先序和中序遍历的两个序列中，分别找到了左右子树对应的子序列。 既然我们已经分别找到了左右子树的前序遍历序列和中序遍历序列，我们可以用同样的方法分别取构建左右子树。 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121public class MakeTree&#123; /** * 构建一颗随机值的二叉树 * * @Author NikoBelic * @Date 16/03/2017 20:36 */ public static void buildRandomTree(TreeNode root, int deep) &#123; int type = new Random().nextInt(100) % 3; //int type = 0; // 构建满二叉树 if (deep &gt; 0) &#123; if (type == 0) // 左+右 &#123; root.left = new TreeNode(new Random().nextInt(10)); root.right = new TreeNode(new Random().nextInt(10)); System.out.print(\"left-\" + root.left.val); System.out.println(\"right-\" + root.right.val); buildRandomTree(root.left, deep - 1); buildRandomTree(root.right, deep - 1); &#125; else if (type == 1) &#123; root.left = new TreeNode(new Random().nextInt(10)); System.out.println(\"left-\" + root.left.val); buildRandomTree(root.left, deep - 1); &#125; else &#123; root.right = new TreeNode(new Random().nextInt(10)); System.out.println(\"right-\" + root.right.val); buildRandomTree(root.right, deep - 1); &#125; &#125; else return; &#125; /** * 广度优先遍历二叉树 * * @Author NikoBelic * @Date 16/03/2017 20:36 */ public static void bfs(TreeNode node) &#123; Deque&lt;TreeNode&gt; q = new ArrayDeque&lt;&gt;(); q.add(node); while (!q.isEmpty()) &#123; node = q.pop(); System.out.println(node.val); if (node.left != null) q.addLast(node.left); if (node.right != null) q.addLast(node.right); &#125; &#125; /** * 按照中序、先序遍历结果，重新构建二叉树 * * @Author SeawayLee * @Date 2017/03/21 19:17 */ public static TreeNode constructCore(int[] preVisit, int[] midVisit) &#123; if (preVisit == null || midVisit == null) return null; // 创建root节点 int rootValue = preVisit[0]; TreeNode root = new TreeNode(rootValue); // 只有一个节点 if (preVisit.length == 1) &#123; if (midVisit.length == 1 &amp;&amp; preVisit[0] == midVisit[0]) return root; else System.out.println(\"Invalid Input...\"); &#125; // 在中序遍历中找到根节点 int rootIndex = 0; while (rootIndex &lt;= midVisit.length - 1 &amp;&amp; midVisit[rootIndex] != rootValue) &#123; rootIndex++; &#125; // 构建左子树 int[] preLeft = Arrays.copyOfRange(preVisit, 1, rootIndex + 1); int[] midLeft = Arrays.copyOfRange(midVisit, 0, rootIndex); if (preLeft.length &gt; 0) &#123; root.left = constructCore(preLeft, midLeft); &#125; // 构建右子树 int[] preRight = Arrays.copyOfRange(preVisit, rootIndex + 1, preVisit.length); int[] midRight = Arrays.copyOfRange(midVisit, rootIndex + 1, midVisit.length); if (preRight.length &gt; 0) &#123; root.right = constructCore(preRight, midRight); &#125; return root; &#125; public static void main(String[] args) throws InterruptedException &#123; //TreeNode root = new TreeNode(0); //buildRandomTree(root, 3); TreeNode root = constructCore(new int[]&#123;1, 2, 4, 7, 3, 5, 6, 8&#125;, new int[]&#123;4, 7, 2, 1, 5, 3, 8, 6&#125;); bfs(root); &#125;&#125;","tags":[{"name":"剑指Offer","slug":"剑指Offer","permalink":"https://seawaylee.github.io/tags/剑指Offer/"}]},{"title":"剑指Offer_5_逆序打印链表值","date":"2017-03-15T13:31:17.000Z","path":"2017/03/15/面试/剑指Offer/Q_5_逆序打印链表/","text":"题目描述 输入一个链表的头结点，从未到头反过来打印出每个节点的值。 解题思路 可以先将正序输出结果压入栈或双向队列，再从栈顶输出。FILO 使用递归，从结尾开始输出 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class PrintListReversingly&#123; /** * 递归实现 * @Author NikoBelic * @Date 15/03/2017 21:25 */ private static void printReversingly(SimpleLinkedList head) &#123; if (head == null) return; if (head.next != null) printReversingly(head.next); System.out.print(head.val + \" \"); &#125; /** * 栈或队列实现 * @Author NikoBelic * @Date 15/03/2017 21:28 */ private static void printByQueue(SimpleLinkedList head) &#123; Deque queue = new LinkedBlockingDeque(); while (head != null) &#123; queue.addFirst(head.val); head = head.next; &#125; while (queue.size() &gt; 0) &#123; System.out.print(queue.pop() + \" \"); &#125; &#125; public static void main(String[] args) &#123; SimpleLinkedList head = new SimpleLinkedList(0, null); SimpleLinkedList p; p = head; for (int i = 1; i &lt;= 10; i++) &#123; p.next = new SimpleLinkedList(i, null); p = p.next; System.out.print(i + \" \"); &#125; System.out.println(\"\\n递归逆序输出结果\"); printReversingly(head.next); System.out.println(\"\\n栈逆序输出结果\"); printByQueue(head.next); &#125;&#125;","tags":[{"name":"剑指Offer","slug":"剑指Offer","permalink":"https://seawaylee.github.io/tags/剑指Offer/"}]},{"title":"String类深入解析","date":"2017-03-15T08:54:15.000Z","path":"2017/03/15/Java基础/String类深入解析/","text":"众所周知，String是由字符组成的串，在程序中使用频率很高。Java中的String是一个类，而并非基本数据类型。 不过她却不是普通的类哦！！！ 1 String对象的创建 关于类对象的创建，很普通的一种方式就是利用构造器，String类也不例外：String s=new String(&quot;Hello world&quot;);问题是参数”Hello world”是什么东西，也是字符串对象吗?莫非用字符串对象创建一个字符串对象? 当然，String类对象还有一种大家都很喜欢的创建方式：String s=&quot;Hello world&quot;;但是有点怪呀，怎么与基本数据类型的赋值操作（int i=1）很像呀? 在开始解释这些问题之前，我们先引入一些必要的知识 1.1 Java class文件结构 和常量池我们都知道，Java程序要运行，首先需要编译器将源代码文件编译成字节码文件(也就是.class文件)。然后在由JVM解释执行。 class文件是8位字节的二进制流 。这些二进制流的涵义由一些紧凑的有意义的项 组成。比如class字节流中最开始的4个字节组成的项叫做魔数 (magic)，其意义在于分辨class文件(值为0xCAFEBABE)与非class文件。class字节流大致结构如下图左侧。 其中，在class文件中有一个非常重要的项——常量池。这个常量池专门放置源代码中的符号信息(并且不同的符号信息放置在不同标志的常量表中)。如上图右侧是HelloWorld代码中的常量表（HelloWorld代码如下），其中有四个不同类型的常量表(四个不同的常量池入口)。关于常量池的具体细节，请参照我的博客《Class文件内容及常量池 》 12345public class HelloWorld&#123; void hello()&#123; System.out.println(\"Hello world\"); &#125; &#125; 通过上图可见，代码中的”Hello world”字符串字面值被编译之后，可以清楚的看到存放在了class常量池中的字符串常量表中(上图右侧红框区域)。 1.2 JVM运行class文件源代码编译成class文件之后，JVM就要运行这个class文件。它首先会用类装载器加载进class文件。然后需要创建许多内存数据结构来存放class文件中的字节数据。比如class文件对应的类信息数据、常量池结构、方法中的二进制指令序列、类方法与字段的描述信息等等。当然，在运行的时候，还需要为方法创建栈帧等。这么多的内存结构当然需要管理，JVM会把这些东西都组织到几个“运行时数据区”中。这里面就有我们经常说的“方法区”、“堆”、“Java栈”等。详细请参见我的博客《Java 虚拟机体系结构》。 上面我们提到了，在Java源代码中的每一个字面值字符串，都会在编译成class文件阶段，形成标志号 为8(CONSTANT_String_info)的常量表 。当JVM加载 class文件的时候，会为对应的常量池建立一个内存数据结构，并存放在方法区中。同时JVM会自动为CONSTANT_String_info常量表中 的字符串常量字面值 在堆中 创建 新的String对象(intern字符串 对象 ，又叫拘留字符串对象)。然后把CONSTANT_String_info常量表的入口地址转变成这个堆中String对象的直接地址(常量池解 析)。 这里很关键的就是这个拘留字符串对象 。源代码中所有相同字面值的字符串常量只可能建立唯一一个拘留字符串对象。实际上JVM是通过一个记录了拘留字符串引用的内部数据结构来维持这一特性的。在Java程序中，可以调用String的intern()方法来使得一个常规字符串对象成为拘留字符串对象。我们会在后面介绍这个方法的。 1.3 操作码助忆符指令有了上面阐述的两个知识前提，下面我们将根据二进制指令来区别两种字符串对象的创建方式：(1) String s=new String(“Hello world”);编译成class文件后的指令123450 new java.lang.String [15] //在堆中分配一个String类对象的空间，并将该对象的地址堆入操作数栈。 3 dup //复制操作数栈顶数据，并压入操作数栈。该指令使得操作数栈中有两个String对象的引用值。 4 ldc &lt;String &quot;Hello world&quot;&gt; [17] //将常量池中的字符串常量&quot;Hello world&quot;指向的堆中拘留String对象的地址压入操作数栈 6 invokespecial java.lang.String(java.lang.String) [19] //调用String的初始化方法，弹出操作数栈栈顶的两个对象地址，用拘留String对象的值初始化new指令创建的String对象，然后将这个对象的引用压入操作数栈 9 astore_1 [s] // 弹出操作数栈顶数据存放在局部变量区的第一个位置上。此时存放的是new指令创建出的，已经被初始化的String对象的地址。 事实上，在运行这段指令之前，JVM就已经为”Hello world”在堆中创建了一个拘留字符串( 值得注意的是：如果源程序中还有一个”Hello world”字符串常量，那么他们都对应了同一个堆中的拘留字符串)。然后用这个拘留字符串的值来初始化堆中用new指令创建出来的新的String对象，局部变量s实际上存储的是new出来的堆对象地址。大家注意了，此时在JVM管理的堆中，有两个相同字符串值的String对象：一个是拘留字符串对象，一个是new新建的字符串对象。如果还有一条创建语句String s1=new String(“Hello world”)；堆中有几个值为”Hello world”的字符串呢? 答案是3个，大家好好想想为什么吧！ (2)将String s=”Hello world”;编译成class文件后的指令: 120 ldc &lt;String &quot;Hello world&quot;&gt; [15]//将常量池中的字符串常量&quot;Hello world&quot;指向的堆中拘留String对象的地址压入操作数栈 2 astore_1 [str] // 弹出操作数栈顶数据存放在局部变量区的第一个位置上。此时存放的是拘留字符串对象在堆中的地址 和上面的创建指令有很大的不同，局部变量s存储的是早已创建好的拘留字符串的堆地址。大家好好想想，如果还有一条穿件语句String s1=”Hello word”；此时堆中有几个值为”Hello world”的字符串呢?答案是1个。那么局部变量s与s1存储的地址是否相同呢？ 呵呵, 这个你应该知道了吧。 1.4 小结 String类型脱光了其实也很普通。真正让她神秘的原因就在于CONSTANT_String_info常量表 和 拘留字符串对象 的存在。现在我们可以解决江湖上的许多纷争了。 【 纷争1】关于字符串相等关系的争论 12345678//代码1 String sa=new String(&quot;Hello world&quot;); String sb=new String(&quot;Hello world&quot;); System.out.println(sa==sb); // false //代码2 String sc=&quot;Hello world&quot;; String sd=&quot;Hello world&quot;; System.out.println(sc==sd); // true 代码1中局部变量sa,sb中存储的是JVM在堆中new出来的两个String对象的内存地址。虽然这两个String对象的值(char[]存放的字符序列)都是”Hello world”。因此”==”比较的是两个不同的堆地址。代码2中局部变量sc,sd中存储的也是地址，但却都是常量池中”Hello world”指向的堆的唯一的那个拘留字符串对象的地址 。自然相等了。 【纷争2】 字符串“+”操作的内幕12345678910//代码1 String sa = &quot;ab&quot;; String sb = &quot;cd&quot;; String sab=sa+sb; String s=&quot;abcd&quot;; System.out.println(sab==s); // false //代码2 String sc=&quot;ab&quot;+&quot;cd&quot;; String sd=&quot;abcd&quot;; System.out.println(sc==sd); //true 代码1中局部变量sa,sb存储的是堆中两个拘留字符串对象的地址。而当执行sa+sb时，JVM首先会在堆中创建一个StringBuilder类，同时用sa指向的拘留字符串对象完成初始化，然后调用append方法完成对sb所指向的拘留字符串的合并操作，接着调用StringBuilder的toString()方法在堆中创建一个String对象，最后将刚生成的String对象的堆地址存放在局部变量sab中。而局部变量s存储的是常量池中”abcd”所对应的拘留字符串对象的地址。 sab与s地址当然不一样了。这里要注意了，代码1的堆中实际上有五个字符串对象：三个拘留字符串对象、一个String对象和一个StringBuilder对象。代码2中”ab”+”cd”会直接在编译期就合并成常量”abcd”， 因此相同字面值常量”abcd”所对应的是同一个拘留字符串对象，自然地址也就相同。 2 String三姐妹(String,StringBuffer,StringBuilder)String扒的差不多了。但他还有两个妹妹StringBuffer,StringBuilder长的也不错哦！我们也要下手了： String(大姐，出生于JDK1.0时代) 不可变字符序列 StringBuffer(二姐，出生于JDK1.0时代) 线程安全的可变字符序列 StringBuilder(小妹，出生于JDK1.5时代) 非线程安全的可变字符序列 2.1 StringBuffer与String的可变性问题我们先看看这两个类的部分源代码： 12345678910111213141516171819//String public final class String &#123; private final char value[]; public String(String original) &#123; // 把原字符串original切分成字符数组并赋给value[]; &#125; &#125; //StringBuffer public final class StringBuffer extends AbstractStringBuilder &#123; char value[]; //继承了父类AbstractStringBuilder中的value[] public StringBuffer(String str) &#123; super(str.length() + 16); //继承父类的构造器，并创建一个大小为str.length()+16的value[]数组 append(str); //将str切分成字符序列并加入到value[]中 &#125; &#125; 很显然，String和StringBuffer中的value[]都用于存储字符序列。但是, (1) String中的是常量(final)数组，只能被赋值一次。比如：new String(“abc”)使得value[]={‘a’,’b’,’c’}，之后这个String对象中的value[]再也不能改变了。这也正是大家常说的，String是不可变的原因 。 注意：这个对初学者来说有个误区，有人说String str1=new String(“abc”); str1=new String(“cba”);不是改变了字符串str1吗？那么你有必要先搞懂对象引用和对象本身的区别。这里我简单的说明一下，对象本身指的是存放在堆空间中的该对象的实例数据(非静态非常量字段)。而对象引用指的是堆中对象本身所存放的地址，一般方法区和Java栈中存储的都是对象引用，而非对象本身的数据。 (2) StringBuffer中的value[]就是一个很普通的数组，而且可以通过append()方法将新字符串加入value[]末尾。这样也就改变了value[]的内容和大小了。比如：new StringBuffer(“abc”)使得value[]={‘a’,’b’,’c’,’’,’’…}(注意构造的长度是str.length()+16)。如果再将这个对象append(“abc”)，那么这个对象中的value[]={‘a’,’b’,’c’,’a’,’b’,’c’,’’….}。这也就是为什么大家说 StringBuffer是可变字符串 的涵义了。从这一点也可以看出，StringBuffer中的value[]完全可以作为字符串的缓冲区功能。其累加性能是很不错的，在后面我们会进行比较。 总结，讨论String和StringBuffer可不可变。本质上是指对象中的value[]字符数组可不可变，而不是对象引用可不可变。 2.2 StringBuffer与StringBuilder的线程安全性问题StringBuffer和StringBuilder可以算是双胞胎了，这两者的方法没有很大区别。但在线程安全性方面，StringBuffer允许多线程进行字符操作。这是因为在源代码中StringBuffer的很多方法都被关键字synchronized 修饰了，而StringBuilder没有。 有多线程编程经验的程序员应该知道synchronized。这个关键字是为线程同步机制 设定的。我简要阐述一下synchronized的含义： 每一个类对象都对应一把锁，当某个线程A调用类对象O中的synchronized方法M时，必须获得对象O的锁才能够执行M方法，否则线程A阻塞。一旦线程A开始执行M方法，将独占对象O的锁。使得其它需要调用O对象的M方法的线程阻塞。只有线程A执行完毕，释放锁后。那些阻塞线程才有机会重新调用M方法。这就是解决线程同步问题的锁机制。 了解了synchronized的含义以后，大家可能都会有这个感觉。多线程编程中StringBuffer比StringBuilder要安全多了 ，事实确实如此。如果有多个线程需要对同一个字符串缓冲区进行操作的时候，StringBuffer应该是不二选择。 注意：是不是String也不安全呢？事实上不存在这个问题，String是不可变的。线程对于堆中指定的一个String对象只能读取，无法修改。试问：还有什么不安全的呢？ 2.3 String和StringBuffer的效率问题（这可是个热门话题呀!）首先说明一点：StringBuffer和StringBuilder可谓双胞胎，StringBuilder是1.5新引入的，其前身就是StringBuffer。StringBuilder的效率比StringBuffer稍高，如果不考虑线程安全，StringBuilder应该是首选。另外，JVM运行程序主要的时间耗费是在创建对象和回收对象上。 我们用下面的代码运行1W次字符串的连接操作，测试String,StringBuffer所运行的时间。 123456789101112//测试代码 public class RunTime&#123; public static void main(String[] args)&#123; //测试代码位置1 long beginTime=System.currentTimeMillis(); for(int i=0;i&lt;10000;i++)&#123; // 测试代码位置2 &#125; long endTime=System.currentTimeMillis(); System.out.println(endTime-beginTime); &#125; &#125; 2.3.1 String常量与String变量的”+”操作比较▲测试①代码： (测试代码位置1) String str=&quot;&quot;; (测试代码位置2) str=&quot;Heart&quot;+&quot;Raid&quot;; [耗时： 0ms] ▲测试②代码 (测试代码位置1) String s1=&quot;Heart&quot;; String s2=&quot;Raid&quot;; String str=&quot;&quot;; (测试代码位置2) str=s1+s2; [耗时： 15—16ms] 结论：String常量的“+连接” 稍优于 String变量的“+连接”。 原因： 测试①的”Heart”+”Raid”在编译阶段就已经连接起来，形成了一个字符串常量”HeartRaid”，并指向堆中的拘留字符串对象。运行时只需要将”HeartRaid”指向的拘留字符串对象地址取出1W次，存放在局部变量str中。这确实不需要什么时间。 测试②中局部变量s1和s2存放的是两个不同的拘留字符串对象的地址。然后会通过下面三个步骤完成“+连接”： StringBuilder temp=new StringBuilder(s1)， temp.append(s2); str=temp.toString(); 我们发现，虽然在中间的时候也用到了append()方法，但是在开始和结束的时候分别创建了StringBuilder和String对象。可想而知：调用1W次，是不是就创建了1W次这两种对象呢？不划算。 但是，String变量的”+连接”操作比String常量的”+连接”操作使用的更加广泛。 这一点是不言而喻的。 2.3.2 String对象的”累+”连接操作与StringBuffer对象的append()累和连接操作比较。▲测试①代码： (代码位置1) String s1=&quot;Heart&quot;; String s=&quot;&quot;; (代码位置2) s=s+s1; [耗时： 4200—4500ms] ▲测试②代码 (代码位置1) String s1=”Heart”; StringBuffer sb=new StringBuffer(); (代码位置2) sb.append(s1); [耗时： 0ms(当循环100000次的时候，耗时大概16—31ms)] 结论：大量字符串累加时，StringBuffer的append()效率远好于String对象的”累+”连接 原因： 测试① 中的s=s+s1，JVM会利用首先创建一个StringBuilder，并利用append方法完成s和s1所指向的字符串对象值的合并操作，接着调用StringBuilder的 toString()方法在堆中创建一个新的String对象，其值为刚才字符串的合并结果。而局部变量s指向了新创建的String对象。 因为String对象中的value[]是不能改变的，每一次合并后字符串值都需要创建一个新的String对象来存放。循环1W次自然需要创建1W个String对象和1W个StringBuilder对象，效率低就可想而知了。 测试②中sb.append(s1);只需要将自己的value[]数组不停的扩大来存放s1即可。循环过程中无需在堆中创建任何新的对象。效率高就不足为奇了。 2.4 小结 (1) 在编译阶段就能够确定的字符串常量，完全没有必要创建String或StringBuffer对象。直接使用字符串常量的”+”连接操作效率最高。 (2) StringBuffer对象的append效率要高于String对象的”+”连接操作。 (3) 不停的创建对象是程序低效的一个重要原因。那么相同的字符串值能否在堆中只创建一个String对象那。显然拘留字符串能够做到这一点，除了程序中的字符串常量会被JVM自动创建拘留字符串之外，调用String的intern()方法也能做到这一点。当调用intern()时，如果常量池中已经有了当前String的值，那么返回这个常量指向拘留对象的地址。如果没有，则将String值加入常量池中，并创建一个新的拘留字符串对象。 转自：http://hxraid.iteye.com/blog/522167附加一篇比较浅显易懂的http://blog.csdn.net/exterminator/article/details/6055297再附加一篇更加浅显易懂的http://blog.csdn.net/zhangjg_blog/article/details/18319521","tags":[{"name":"Java基础","slug":"Java基础","permalink":"https://seawaylee.github.io/tags/Java基础/"}]},{"title":"剑指Offer_4Extra_合并两个有序数组","date":"2017-03-14T13:51:15.000Z","path":"2017/03/14/面试/剑指Offer/Q_4Extra_合并有序数组/","text":"题目描述 有两个排序的数组A1和A2，内存在A1的末尾有足够多的空余空间容纳A2。请事先一个函数，把A2中的所有数字插入到A1zhong并且所有的数字是排序的。 解题思路 和前面的例题一样，很多人首先想到的办法是在A1中从头到尾复制数字，但是这样会出现多次复制一个数字的情况。更好的办法是从尾到头比较A1和A2中的数字，并把较大的数字复制到A1的合适位置。注意边界条件的控制，如果A1已经移动结束，但是A2还没有结束，需要将A2移动到A1的其实位置而A1的指针不动 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class MergeSortedArray&#123; public static int[] merge(int[] a, int[] b) &#123; if (a == null) return b; if (b == null) return a; int p1 = a.length - 1 - b.length; int p2 = b.length - 1; while (p2 &gt;= 0) &#123; if (p1 &gt;= 0 &amp;&amp; a[p1] &gt; b[p2]) &#123; a[p1 + p2 + 1] = a[p1]; p1--; &#125; else &#123; a[p1 + p2 + 1] = b[p2]; p2--; &#125; &#125; return a; &#125; public static void main(String[] args) &#123; //int[] a = &#123;1, 5, 7, 9, 15, 0, 0, 0&#125;; //int[] a = &#123;9, 15, 0, 0, 0&#125;; //int[] a = &#123;1,9, 15, 0, 0, 0, 0,0&#125;; int[] a = &#123;1,3,5,7,8,9,11,12,13,14,15,0,0,0,0,0&#125;; int[] b = &#123;2, 4, 6, 10,16&#125;; merge(a, b); for (int i : a) &#123; System.out.print(i + \" \"); &#125; System.out.println(\"\"); &#125;&#125;","tags":[{"name":"剑指Offer","slug":"剑指Offer","permalink":"https://seawaylee.github.io/tags/剑指Offer/"}]},{"title":"剑指Offer_4_替换空格","date":"2017-03-14T12:57:44.000Z","path":"2017/03/14/面试/剑指Offer/Q_4_空格字符替换/","text":"题目描述 请实现一个函数，把字符串中的每个空格替换成 %20 。例如输入”We are happy”，则输出”We%20are%20happy”不能用java的一些api，完全用C语言的方式完成本题，不然会被鄙视。 解题思路虽然我们可以使用遍历的方式，创建一个新的空数组来完成这道题，时间复杂度也只是O(n)，但是浪费了很多空间。虽然我们可以正序遍历数组，每碰到一个空格，就将后边的所有元素向后移动，这样虽然不用开辟冗余的空间，但是时间复杂度较高，最后一部分元素需要被移动 N*空格数 次。 如果我们采用倒序的方式遍历和移动数组，那么时间复杂的和空间复杂度都是最低的。 先遍历一次字符串，统计空格出现的次数，每出现一个空格，对改数组额外开辟2位。 从字符串的后面开始复制和替换。 指针P1指向原始字符串末尾，指针P2指向开辟空间后字符串末尾 P1向前移动，逐个把字符复制到P2，直到碰到空格为止 碰到空格后，P1向前移动一位，P2向前移动三位的同时将%20填充 当P2追赶上P1时，结束复制，表示所有空格已经替换完毕 代码实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class CharReplace&#123; public static char[] replaceSpace(char[] str,int length) &#123; if (str == null) return null; // 计算总共有多少个空格 int spaceCount = 0; for (int i = 0; i &lt; str.length; i++) &#123; if (str[i] == ' ') spaceCount++; &#125; // 扩展字符数组长度,每个 ' ' 替换为 %20 需要额外扩展2个长度 for (int i = 0; i &lt; spaceCount; i++) &#123; str = (String.valueOf(str) + \" \").toCharArray(); &#125; // 两个指针，P1指向元数组结尾，P2指向新数组结尾 int p1 = length - 1; int p2 = str.length - 1; // 从p1向前，逐步将字符复制到p2 while (p1 != p2) &#123; // 如果p1为空格，则p1向前移动1位，p2向前移动三位并填充%20 if (str[p1] == ' ') &#123; p1--; str[p2--] = '0'; str[p2--] = '2'; str[p2--] = '%'; &#125; else &#123; str[p2--] = str[p1--]; &#125; System.out.println(p1 + \",\" + p2 + \" \" + String.valueOf(str)); &#125; return str; &#125; public static void main(String[] args) &#123; char[] str = \"We are happy ~~~\".toCharArray(); System.out.println(replaceSpace(str,str.length)); &#125;&#125;","tags":[{"name":"剑指Offer","slug":"剑指Offer","permalink":"https://seawaylee.github.io/tags/剑指Offer/"}]},{"title":"剑指Offer_3_二维数组中的查找","date":"2017-03-14T11:47:58.000Z","path":"2017/03/14/面试/剑指Offer/Q_3_二维数组查找元素/","text":"题目描述 在一个二位数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下的递增顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 解题思路发现规律： 1.首先选取数组中右上角的数字。如果该数字等于key，查找过程结束2.如果该数字大于key，则删除这个数字所在的列（因为该列所有元素都比key大）3.如果该数字小于key，则删除这个数字所在的行（因为该行所有元素都比key小） 也就是说如果要查找的数字不在数组的右上角，则每一次都在数组的查找范围中删除一行或一列，这样每一步都可以缩小查找范围，知道找到要查找的数字，或者查找范围为空。 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class ArraySearch&#123; public static boolean findElementInArray(int[][] array, int key) &#123; if (array == null) return false; int startCol = array[0].length - 1; for (int i = 0; i &lt; array.length; i++) &#123; for (int j = startCol; j &gt; 0; j--) &#123; System.out.println(\"查询位置:(\" + i + \",\" + j + \")\"); // 如果当前数字大于key，则删除此列 if (array[i][j] &gt; key) &#123; // 删除过得列，不需要再查找，下次循环时从新的起始列开始查 startCol -= 1; continue; &#125; // 如果当前数字小于key，则删除此行 else if (array[i][j] &lt; key) break; else &#123; return true; &#125; &#125; &#125; return false; &#125; public static void main(String[] args) &#123; //Integer[][] array = new Integer[row+1][col+1]; // 这里为什么要多一个 不然会报错？ int[][] array = new int[][]&#123; &#123;1, 2, 8, 9&#125;, &#123;2, 4, 9, 12&#125;, &#123;4, 7, 10, 13&#125;, &#123;6, 8, 11, 15&#125; &#125;; for (int i = 0; i &lt; array.length; i++) &#123; for (int j = 0; j &lt; array[i].length; j++) &#123; System.out.print(array[i][j] + \" \"); &#125; System.out.println(\"\"); &#125; System.out.println(\"===================\"); System.out.println(\"Result:\" + findElementInArray(array, 15)); &#125;&#125; 补充同样，我们可有选取左下角的数字。但是不能选择左上角和右上角，因为既不能删除行，也不能删除列。","tags":[{"name":"剑指Offer","slug":"剑指Offer","permalink":"https://seawaylee.github.io/tags/剑指Offer/"}]},{"title":"剑指Offer_2_实现Singleton模式","date":"2017-03-14T07:41:16.000Z","path":"2017/03/14/面试/剑指Offer/Q_2_单例模式/","text":"不好的解法： 方法一：仅支持单线程的单例模式，只有if判断。1234567891011121314151617181920212223public class NotSafeSingleton&#123; private static NotSafeSingleton instance = null; private NotSafeSingleton()&#123;&#125; public static NotSafeSingleton getInstance() &#123; if (instance == null) &#123; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; instance = new NotSafeSingleton(); &#125; return instance; &#125;&#125; 方法二：支持多线程，但是效率很低的同步方法块。 可行的解法： 方法三：外层if(减少同步代码块被访问的次数) + 同步代码块(防止多线程访问) + 内层if(保证单例) 12345678910111213141516171819202122public class Singleton&#123; private static Singleton instance = null; private static final Object LOCKOBJ = new Object(); private Singleton()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; synchronized (LOCKOBJ) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 推荐的解法: 方法四：饿汉式：线程安全，但是实例创建过早,当调用其中的某个静态方法时，实例就会被创建，但可能我们并不想创建这个实例 12345678910111213public class HungrySingleton&#123; private static final HungrySingleton instance = new HungrySingleton(); private HungrySingleton() &#123; &#125; public static HungrySingleton getInstance() &#123; return instance; &#125;&#125; 方法五：静态内部类：内部内部类会在被调用的时候 初始化外部类的静态实例，有效防止了过早创建实例的问题，且线程安全 123456789101112131415public class InnerSingleton&#123; private InnerSingleton()&#123;&#125; public static InnerSingleton getInstance() &#123; return SingletonHolder.instance; &#125; private static class SingletonHolder &#123; private static final InnerSingleton instance = new InnerSingleton(); &#125;&#125; 方法六：枚举方法（jdk1.5以后，推荐方法）","tags":[{"name":"剑指Offer","slug":"剑指Offer","permalink":"https://seawaylee.github.io/tags/剑指Offer/"}]},{"title":"Solr学习笔记","date":"2017-03-13T05:42:47.000Z","path":"2017/03/13/搜索/Solr学习笔记/","text":"1 Solr基本概念1.1 什么是Solr Solr是基于Lucene的全文检索服务器 Solr需要部署在Web容器中 使用POST方法向Solr服务器发送一个描述Field及其内容的XML文档，Solr根据xml文档添加、删除、更新索引。 1.2 Solr与Lucene的区别Lucene是一个开元全文检索引擎工具包，并不是一个全文检索引擎。Solr的目的是打造一款企业级引擎系统，他是一个搜索引擎服务，可以独立运行，通过Solr可以非常快速的构建企业的搜索引擎。 2 Solr配置2.1 Solr目录结构 bin: solr运行脚本 contrib: solr的一些贡献软件，用于增强solar的功能 dist: war、jar、相关依赖文件 docs: API文档 example: solr工程例子目录: solr: 包含了默认配置信息的Solr和Core目录 multicore: 包含了在Solr的multicore中设置的多个Core目录 webapps: 包括一个solr.war,该war可作为solr的运行实例工程 2.2 Solr整合Tomcat2.2.1 SolrHome与SolrCore创建一个SolrHome目录，SolrHome是Solr运行的主目录，目录中包括了运行Solr实例所有的配置文件和数据文件,Solr实例就是SolrCore，一个SolrHome可以包括多个SolrCore（Solr实例），每个SolrCore提供单独的搜索和索引服务。example/solr是一个标准的SolrHome目录，其中example/solr/collection1是一个SolrCore目录一个SolrCore对外单独提供索引和搜索接口。 2.2。2 整合步骤 安装tomcat。D:\\temp\\apache-tomcat-7.0.53 把solr的war包复制到tomcat 的webapp目录下。把\\solr-4.10.3\\dist\\solr-4.10.3.war复制到D:\\temp\\apache-tomcat-7.0.53\\webapps下。改名为solr.war solr.war解压。使用压缩工具解压或者启动tomcat自动解压。解压之后删除solr.war 把\\solr-4.10.3\\example\\lib\\ext目录下的所有的jar包添加到solr工程中 配置solrHome和solrCore。 创建一个solrhome（存放solr所有配置文件的一个文件夹）。\\solr-4.10.3\\example\\solr目录就是一个标准的solrhome。 把\\solr-4.10.3\\example\\solr文件夹复制到D:\\temp\\0108路径下，改名为solrhome，改名不是必须的，是为了便于理解。 在solrhome下有一个文件夹叫做collection1这就是一个solrcore。就是一个solr的实例。一个solrcore相当于mysql中一个数据库。Solrcore之间是相互隔离。 在solrcore中有一个文件夹叫做conf，包含了索引solr实例的配置信息。 在conf文件夹下有一个solrconfig.xml。配置实例的相关信息。如果使用默认配置可以不用做任何修改。Xml的配置信息：Lib：solr服务依赖的扩展包，默认的路径是collection1\\lib文件夹，如果没有就创建一个dataDir：配置了索引库的存放路径。默认路径是collection1\\data文件夹，如果data文件夹，会自动创建。 告诉solr服务器配置文件也就是solrHome的位置。修改web.xml使用jndi的方式告诉solr服务器。Solr/home名称必须是固定的。(apache下solr项目的web.xml) 12345&lt;env-entry&gt; &lt;env-entry-name&gt;solr/home&lt;/env-entry-name&gt; &lt;env-entry-value&gt;/Users/lixiwei-mac/app/solr-4.10.3/solrhome&lt;/env-entry-value&gt; &lt;env-entry-type&gt;java.lang.String&lt;/env-entry-type&gt;&lt;/env-entry&gt; 启动tomcat，访问http://localhost:8080/solr 3 使用SolrJ管理索引库3.1 什么是SolrJsolrj是访问Solr服务的java客户端，提供索引和搜索的请求方法，SolrJ通常在嵌入在业务系统中，通过SolrJ的API接口操作Solr服务，如下图： 3.2 依赖1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798&lt;!-- Solr --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.solr&lt;/groupId&gt; &lt;artifactId&gt;solr-solrj&lt;/artifactId&gt; &lt;version&gt;4.10.3&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpcore&lt;/artifactId&gt; &lt;version&gt;4.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpmime&lt;/artifactId&gt; &lt;version&gt;4.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.noggit&lt;/groupId&gt; &lt;artifactId&gt;noggit&lt;/artifactId&gt; &lt;version&gt;0.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.codehaus.woodstox&lt;/groupId&gt; &lt;artifactId&gt;wstx-asl&lt;/artifactId&gt; &lt;version&gt;3.2.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.6&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!--Solr Ext--&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jul-to-slf4j&lt;/artifactId&gt; &lt;version&gt;1.7.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.6&lt;/version&gt; &lt;/dependency&gt; 3.3 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201package lucene;import org.apache.commons.io.FileUtils;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.document.*;import org.apache.lucene.index.*;import org.apache.lucene.queryparser.classic.MultiFieldQueryParser;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.*;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import org.apache.lucene.util.Version;import org.junit.Test;import org.wltea.analyzer.lucene.IKAnalyzer;import java.io.File;import java.io.IOException;/** * @author NikoBelic * @create 07/03/2017 13:05 */public class LuceneTest&#123; /** * 创建索引 * * @Author NikoBelic * @Date 07/03/2017 19:01 */ @Test public void testIndexCreate() throws IOException &#123; // 指定文档和索引的存储目录 Directory indexDir = FSDirectory.open(new File(\"/Users/lixiwei-mac/Documents/DataSet/lucene/index\")); // 标准分词器(英文效果好,中文单字分词) Analyzer analyzer = new IKAnalyzer(); IndexWriterConfig config = new IndexWriterConfig(Version.LATEST, analyzer); IndexWriter indexWriter = new IndexWriter(indexDir, config); // 采集文档中的数据放入Lucene中 File sourceDir = new File(\"/Users/lixiwei-mac/Documents/DataSet/lucene/searchsource\"); for (File f : sourceDir.listFiles()) &#123; String fileName = f.getName(); String fileContent = FileUtils.readFileToString(f); String filePath = f.getPath(); long fileSize = FileUtils.sizeOf(f); // (field_name,field_value,need_stored?) Field fileNameField = new TextField(\"filename\", fileName, Field.Store.YES); Field fileContentField = new TextField(\"content\", fileContent, Field.Store.NO); Field filepPthField = new StoredField(\"path\", filePath); Field fileSizeField = new LongField(\"size\", fileSize, Field.Store.YES); Document document = new Document(); document.add(fileNameField); document.add(fileContentField); document.add(filepPthField); document.add(fileSizeField); // 这里会自动创建索引 indexWriter.addDocument(document); &#125; indexWriter.close(); &#125; /** * 使用索引搜索 * * @Author NikoBelic * @Date 07/03/2017 19:01 */ @Test public void testIndexSearch() throws IOException, ParseException &#123; // 指定索引库存放路径 Directory indexDir = FSDirectory.open(new File(\"/Users/lixiwei-mac/Documents/DataSet/lucene/index\")); // 创建索引Reader、Searcher对象 IndexReader indexReader = DirectoryReader.open(indexDir); IndexSearcher indexSearcher = new IndexSearcher(indexReader); // 创建查询 // 方法1 Query query = new TermQuery(new Term(\"content\", \"java\")); // 执行查询,(查询对象,查询结果返回最大值) // 方法2 // 创建分词器(必须和创建索引所用分词器一致) Analyzer analyzer = new IKAnalyzer(); // 默认搜索域作用:如果搜索语法中没有指定域名,则使用默认域名搜索 QueryParser queryParser = new QueryParser(\"filename\", analyzer); // 查询语法:域名:搜索关键字 Query query2 = queryParser.parse(\"apache\"); TopDocs topDocs = indexSearcher.search(query2, 5); System.out.println(\"查询结果的总条数:\" + topDocs.totalHits); // 遍历查询结果 for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; // scoreDoc.doc = 自动生成的文档ID Document document = indexSearcher.doc(scoreDoc.doc); System.out.println(document.get(\"filename\")); System.out.println(scoreDoc.toString()); System.out.println(\"======================================================\"); &#125; indexReader.close(); &#125; @Test public void testDelIndex() throws IOException &#123; Analyzer analyzer = new IKAnalyzer(); Directory indexDir = FSDirectory.open(new File(\"/Users/lixiwei-mac/Documents/DataSet/lucene/index\")); IndexWriterConfig config = new IndexWriterConfig(Version.LATEST, analyzer); IndexWriter indexWriter = new IndexWriter(indexDir, config); // 删除所有 //indexWriter.deleteAll(); // Term 词源,(域名,删除含有这些关键词的数据) indexWriter.deleteDocuments(new Term(\"filename\", \"apache\")); indexWriter.commit(); indexWriter.close(); &#125; /** * 更新就是按照传入的Term进行搜索,如果找到结果那么删除,将更新的内容重新生成一个Document对象 * 如果没有搜索到结果,那么将更新的内容直接添加一个新的Document对象 * * @Author NikoBelic * @Date 07/03/2017 20:57 */ @Test public void testUpdateIndex() throws IOException &#123; Analyzer analyzer = new IKAnalyzer(); Directory indexDir = FSDirectory.open(new File(\"/Users/lixiwei-mac/Documents/DataSet/lucene/index\")); IndexWriterConfig config = new IndexWriterConfig(Version.LATEST, analyzer); IndexWriter indexWriter = new IndexWriter(indexDir, config); Document doc = new Document(); doc.add(new TextField(\"filename\", \"更新检索测试.txt\", Field.Store.YES)); doc.add(new TextField(\"content\", \"文件内容测试\", Field.Store.NO)); doc.add(new LongField(\"size\", 100L, Field.Store.YES)); indexWriter.updateDocument(new Term(\"filename\", \"检索\"), doc); indexWriter.commit(); indexWriter.close(); &#125; /** * 根据索引查询,多种查询 * @Author NikoBelic * @Date 08/03/2017 13:02 */ @Test public void testSearch() throws IOException, ParseException &#123; Directory indexDir = FSDirectory.open(new File(\"/Users/lixiwei-mac/Documents/DataSet/lucene/index\")); IndexReader indexReader = DirectoryReader.open(indexDir); IndexSearcher indexSearcher = new IndexSearcher(indexReader); // 根据文本查询 Query termQuery = new TermQuery(new Term(\"filename\", \"apache\")); // 根据数字范围查询 Query numQuery = NumericRangeQuery.newLongRange(\"size\", 100L, 800L, true, true); // Bool查询 BooleanQuery boolQuery = new BooleanQuery(); boolQuery.add(termQuery, BooleanClause.Occur.MUST); // 独自使用MUST_NOT没有任何意义 boolQuery.add(numQuery, BooleanClause.Occur.MUST); // 查询所有文档 MatchAllDocsQuery matchAllDocsQuery = new MatchAllDocsQuery(); // 多个域的查询,或 关系 String[] fields = &#123;\"filename\", \"content\"&#125;; MultiFieldQueryParser multiFieldQueryParser = new MultiFieldQueryParser(fields, new IKAnalyzer()); Query multiFieldQuery = multiFieldQueryParser.parse(\"apache\"); //TopDocs topDocs = indexSearcher.search(boolQuery, 10); //TopDocs topDocs = indexSearcher.search(matchAllDocsQuery, 10); TopDocs topDocs = indexSearcher.search(multiFieldQuery, 10); System.out.println(\"符合条件的文档数:\" + topDocs.totalHits); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document document = indexSearcher.doc(scoreDoc.doc); System.out.println(document.get(\"filename\")); System.out.println(document.get(\"size\")); System.out.println(\"======================================================================\"); &#125; &#125;&#125; 3.4 应用系统架构","tags":[{"name":"搜索","slug":"搜索","permalink":"https://seawaylee.github.io/tags/搜索/"}]},{"title":"阿里巴巴常考面试题及汇总答案","date":"2017-03-10T13:46:21.000Z","path":"2017/03/10/面经/阿里面经_2/","text":"阿里巴巴常考面试题及汇总答案","tags":[{"name":"面经","slug":"面经","permalink":"https://seawaylee.github.io/tags/面经/"}]},{"title":"阿里巴巴-软件工程师面经（一）","date":"2017-03-10T13:27:56.000Z","path":"2017/03/10/面经/阿里面经_1/","text":"1.hashmap、concurrenthashmap底层实现和区别 2.spring框架的原理 3.如何写一个orm框架 4.hibernate一级缓存和二级缓存，hibernate其他缓存 5.hibernate事务传播行为种类 6.springmvc原理 7.restful的好处 8.restful有几种请求，表单如何提交put请求 9.web中安全性问题的考虑，如何防止 10.web系统整体架构 11.hibernate如何实现声明式事务 12.java并发包 13.volatile 14.平常都看哪些书 15.spring底层数据结构 16.如何进行反射，如何提高反射的性能 17.如何实现java的代理，为什么需要实现接口 18.TCP协议三次握手 19.springmvc用过哪些注解 20.springAOP可以使用哪些代理，有什么区别 21.为什么要分三层这面没有准备好，广度很深度压力很大，建议大家多看看三大框架源码、原理。","tags":[{"name":"面经","slug":"面经","permalink":"https://seawaylee.github.io/tags/面经/"}]},{"title":"豆瓣读书论坛-大规模数据爬取与分析-Java版","date":"2017-03-09T17:06:02.000Z","path":"2017/03/10/爬虫/Java/豆瓣读书爬虫/","text":"1 前言前些天看到了@Brian Way 同学做了一个知乎的用户信息爬虫，想到了自己很久以前也做过类似的知乎爬虫，在今年后半年写了一个豆瓣读书论坛爬虫，爬了很多数据，各种加起来大概有1亿1000万左右。@路人甲 等大牛经常发布一些Python爬虫，也让我学习了不少，就此篇文章向各位大神致敬。 因为最近在写小论文，相关的算法需要预先将数据转换才能用，所以代码中可能有些东西与爬虫无关。如果大家有需要（请用赞告诉我！哈哈哈哈），我后续会将 朴素贝叶斯分类器 对数据的处理，和基于Spark的协同过滤推荐系统 相关介绍和代码整理后公布出来，并详细介绍自己的研究思路。 2 思路 通过 豆瓣图书标签 获取所有标签的地址，将所有标签记录 通过将标签拼接到图书列表的url地址，获取某个标签下的图书信息，通过这个页面可以获得当前页的图书基本信息。 豆瓣图书标签: 小说 进入到某个图书详情页面，爬取更详细的图书信息（如果有需求的话） 解忧杂货店 (豆瓣) 通过获取到的所有图书的bookno可以爬取所有图书的短评、书评等信息。 解忧杂货店 短评 通过这个评论列表我们可以进入到每个用户的个人主页，爬取他们的个人信息 通过这个页面我还获取到了用户 想读、在读、读过的 UserNo - BookNo 对应关系，以后做推荐系统有大用处！ 3 爬虫架构 爬虫架构采用国产开源Java爬虫框架WebMagic，为了能够分布式爬取，项目中还使用了Redis作为统一URL队列管理器，使用Mysql持久化数据，Velocity + Echarts做的数据可视化，整体项目架构是Spring+SpringMVC+Mybatis。 4 数据分析 热门标签下的图书数量 Top10 发表评论最多的用户Top10 大家看看有没有自己认识的人啊哈哈哈，第一名评论了3000本书，真牛逼。 用户发表的评分值分布 其中-1分表示没有给出评价星级，但是做了评论。看来还是好评比较多啊有250万5星！ 用户地域分布，看来北上广相信读书啊！ 其中有28W左右用户在个人信息中没有写长居地址，所以没放进来展示。 每年书评数量分布 1970乱入什么鬼。。2016年大概是半年多一点的数据，其实肯定要比2015高的。 最后看一下评论数大于5W的评分值Top50的图书吧 大家是不是都读过 5 总结 爬一些数据做分析挺有意思的，虽然我做的这些分析都非常辣鸡吧（我TM也想做牛逼的分析啊，我不会啊）。最后的数据图书短评大概有不到800W，用户信息130W，图书信息4W5，用户想读、在读、读过 这类的用户-图书关系大概有9000W+（大数据推荐系统用，所以爬了好多）。豆瓣反爬虫机制几乎没有，所以我爬的也很爽。。。。 国际惯例，附上源码 项目地址 Github - doubanWebSpider 代码写的不规范，非常乱，还请各位见谅。 后续如果大家需要（赞我赞我赞我），我会将基于朴素贝叶斯分类器的中文文本情感分析相关程序和基于Spark的分布式协同过滤推荐算法相关程序陆续发布出来（起这么长的名字是不是听起来很傻b…可是做论文评审的人可不这么觉得…）。其实如果大家懂机器学习和Spark就能知道，我这俩东西很low的，纯粹为了骗论文。。。。 2016-12-23 更新，数据可视化地址和查询图书功能 图书查询功能是模糊查询，结果会按照评分由高到低显示结果。 地址 豆瓣超级无敌爬虫^.^","tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://seawaylee.github.io/tags/爬虫/"},{"name":"Java爬虫","slug":"Java爬虫","permalink":"https://seawaylee.github.io/tags/Java爬虫/"}]},{"title":"Django1.10静态文件不能成功读取解决方案","date":"2017-03-09T16:50:53.000Z","path":"2017/03/10/PythonWeb/Django静态文件问题/","text":"问题：初学Django，按照官网配置settings.py中static相关部分后不能成功读取到配置文件。使用Chrome调试工具查看后发现没有报404错误，而是返回200.Console 提示信息Resource interpreted as Stylesheet but transferred with MIME type text/html: &quot;http://localhost:8000/bbs/index/static/css/common.css&quot;. 解决方案 1.settings中静态文件配置部分如下123456789101112SITE_ROOT = os.path.join(os.path.abspath(os.path.dirname(__file__)),'..') STATIC_URL = '/static/' STATIC_ROOT = os.path.join(SITE_ROOT,'static') STATICFILES_DIRS = ( # os.path.join(BASE_DIR,STATIC_URL), ('css',os.path.join(STATIC_ROOT,'css').replace('\\\\','/') ), ('js',os.path.join(STATIC_ROOT,'js').replace('\\\\','/') ), ('images',os.path.join(STATIC_ROOT,'images').replace('\\\\','/') ), ('upload',os.path.join(STATIC_ROOT,'upload').replace('\\\\','/') ), ) 2.目录结构3.HTML引用静态文件1234567891011121314151617181920212223242526&lt;!DOCTYPE html&gt; &#123;% load static %&#125; &#123;#&#123;% load staticfiles %&#125;#&#125; &lt;html lang=\"en\" &gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;BBS社区&lt;/title&gt; &lt;/head&gt; &lt;link rel=\"stylesheet\" href=\"&#123;% static 'css/common.css' %&#125;\" &gt; &lt;body&gt; &lt;div class=\"pg-header\"&gt; &lt;div class=\"header-menu\"&gt; &lt;img src=\"/static/images/logo.png\" href=\"/bbs/index/\" class=\"digg-logo\"&gt;&lt;/img&gt; &lt;a href=\"/bbs/index/\" class=\"tb active\"&gt;全部&lt;/a&gt; &lt;a href=\"/bbs/index/\" class=\"tb\"&gt;TabA&lt;/a&gt; &lt;a href=\"/bbs/index/\" class=\"tb\"&gt;TabB&lt;/a&gt; &lt;a href=\"/bbs/index/\" class=\"tb\"&gt;TabC&lt;/a&gt; &lt;/div&gt; &lt;div class=\"header-search\"&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class=\"pg-body\"&gt;&lt;/div&gt; &lt;div class=\"pg-bottom\"&gt;&lt;/div&gt; &#123;# &lt;img src=\"&#123;% static 'images/test.png' %&#125;\"&gt;#&#125; &lt;/body&gt; &lt;/html&gt; 4.重启server再试试吧，good luck.","tags":[{"name":"PythonWeb","slug":"PythonWeb","permalink":"https://seawaylee.github.io/tags/PythonWeb/"}]},{"title":"数据库编程入门(一)-PL/SQL快速入门","date":"2017-03-09T16:47:43.000Z","path":"2017/03/10/Oracle/PLSQL快速入门/","text":"1.什么是PL/SQL1.1 PL/SQL含义Procedure Language / SQL 是Oracle对过程化语言的扩展，针对CRUD的过程处理语句，使得SQL语句具有过程处理能力。 1.2 为什么要学习PL/SQL 比如要给员工按职位不同增加不同的工资，虽然可以用java等编程语言操作数据库进行实现，但是效率远不如使用Oralce原生的变成语言实现。 为了学习存储过程和触发器打基础 Procedure Language / SQL 是Oracle对过程化语言的扩展，针对CRUD的过程处理语句，使得SQL语句具有过程处理能力。 2.最简单的PL/SQL程序我用的是Oracle11g,客户端连接工具是SQLDeveloper。 1234567set serveroutput on; — 打开输出开关(默认是关闭的)declare — 说明部分 (变量，光标，例外)begin -- 程序体 DBMS_OUTPUT.PUT_LINE('Hello World');end;/ -- 表示此段程序执行结束 3.使用desc查看表结构、视图结构、包结构 4.基本数据类型1234567891011121314151617char,varchar2,date,number,boolean,longdeclarepnumber number(10,2);pname varchar2(200);pdate date;begin pnumber := 1; DBMS_OUTPUT.PUT_LINE(pnumber); pname := 'NikoBelic'; DBMS_OUTPUT.PUT_LINE(pname); pdate := sysdate; DBMS_OUTPUT.PUT_LINE(pdate); pdate := pdate + 1; DBMS_OUTPUT.PUT_LINE(pdate);end;/ 5.if else then的使用123456789101112131415161718192021/* 判断用户从键盘输入的数字 1.如何使用if语句 2.接收一个键盘输入（字符串）*/-- 接受一个键盘输入-- num:地址值，含义是：在改地址上保存了输入的值accept num PROMPT '请输入一个数字'declare -- 定义变量保存用户输入的数字 pnum number := &amp;num;begin if pnum = 0 then dbms_output.put_line('您输入的是0'); elsif pnum = 1 then dbms_output.put_line('您输入的是1'); elsif pnum = 2 then dbms_output.put_line('您输入的是2'); else dbms_output.put_line('其他数字'); end if;end;/ 6.循环的使用6.1 while loop 循环12345678declarenum number := 1;begin while num &lt;= 10 loop dbms_output.put_line(num); num := num + 1; end loop;end; 6.2 loop循环123456789declare num number := 1;begin loop exit when num &gt; 10; dbms_output.put_line(num); num := num + 1; end loop;end; 6.3 for loop循环12345678declare num number;begin for i in 1..10 loop dbms_output.put_line(i); end loop;end; 7.游标/光标7.1什么是游标游标可以理解为一个指向数据集合的指针cursor c_user is select * from tb_user; – c_user就是指向user表中所有数据的集合 7.2游标的属性 found: 光标有值 notfound:光标无 isopen:光标是否打开 rowcount:光标影响的行数 7.3查看和和修改游标限制游标数的限制：默认情况下，oracle数据库只允许在同一个会话中打开300个游标。查看和修改游标限制数: 123sqlplus sys/password@192.168.1.100:1521/orcl as sysdba;show user;show parameter cursor; 12345alter system set open_cursors=400 scope=both;-- scope的取值有三个: both 配置文件和当前实例都更改 memory 只更改当前实例，不更改配置文件 spfile 只更改参数文件，不更改当前实例，需要重启数据库 7.4 游标应用示例示例1 1234567891011121314151617181920declare -- 定义一个光标 cursor c_user is select name,salary from tb_user; -- 为光标定义对应的变量 uname TB_USER.NAME%type; usalary TB_USER.SALARY%type; user tb_user%rowtype;begin -- 打开光标 open c_user; loop -- 取一条记录 fetch c_user into uname,usalary; exit when c_user%notfound; -- 光标的属性found和notfound dbms_output.put_line(uname || '的薪资是' || usalary); end loop; -- 关闭光标 close c_user;end;/ 示例2 1234567891011121314151617181920declare -- 定义一个光标 cursor c_user is select * from tb_user; -- 为光标定义对应的变量 uname TB_USER.NAME%type; usalary TB_USER.SALARY%type; user tb_user%rowtype;begin -- 打开光标 open c_user; loop -- 取一条记录 fetch c_user into user; exit when c_user%notfound; -- 光标的属性found和notfound dbms_output.put_line(user.name || '的薪资是' || user.salary); end loop; -- 关闭光标 close c_user;end;/ 员工涨工资程序 12345678910111213141516171819DECLARECURSOR CUR_USER IS SELECT * FROM TB_USER;USER TB_USER%ROWTYPE;BEGIN OPEN CUR_USER; LOOP FETCH CUR_USER INTO USER; EXIT WHEN CUR_USER%NOTFOUND; IF USER.JOB = 'CEO' THEN UPDATE TB_USER SET SALARY = SALARY + 100000 WHERE NO = USER.NO; ELSIF USER.JOB = 'MANAGER' THEN UPDATE TB_USER SET SALARY = SALARY + 50000 WHERE NO = USER.NO; ELSE UPDATE TB_USER SET SALARY = SALARY + 10000 WHERE NO = USER.NO; END IF; END LOOP; CLOSE CUR_USER; COMMIT;END; (此图有误，但是结果是正确的，懒得再截图了) 8.系统例外(异常)8.1 NO_DATA_FOUND12345678910111213DECLARECURSOR CUR_USER(NUM NUMBER) IS SELECT * FROM TB_USER WHERE NO &gt; NUM;USER TB_USER%ROWTYPE;BEGINOPEN CUR_USER(2);LOOP FETCH CUR_USER INTO USER; EXIT WHEN CUR_USER%NOTFOUND; DBMS_OUTPUT.PUT_LINE(USER.NAME);END LOOP;CLOSE CUR_USER;END; 8.2 TOO_MANY_ROWS12345678DECLAREUNAME TB_USER.NAME%TYPE;BEGIN SELECT NAME INTO UNAME FROM TB_USER WHERE NO &gt; 1; EXCEPTION WHEN TOO_MANY_ROWS THEN DBMS_OUTPUT.PUT_LINE('TOO FUCKING MANY ROWS!!!!!!');END; 8.3 ZERO_DIVIDE12345678910DECLAREPNUM NUMBER;BEGIN PNUM := 1 / 0; EXCEPTION WHEN ZERO_DIVIDE THEN DBMS_OUTPUT.PUT_LINE('FUCKING ZERO CANNOT BE DIVIDE!!!!!'); DBMS_OUTPUT.PUT_LINE('UNDERSTAND????SILLY BOY!!!'); WHEN OTHERS THEN DBMS_OUTPUT.PUT_LINE('OTHER EXCEPTIONS...');END; 8.4 VALUE_ER12345678DECLAREUNO NUMBER;BEGIN UNO := 'ABC'; EXCEPTION WHEN VALUE_ERROR THEN DBMS_OUTPUT.PUT_LINE('FUCKING DATA TRANSFER ERROR!!!!');END; 8.5自定义异常 1234567891011121314DECLARECURSOR CUR_USER IS SELECT * FROM TB_USER WHERE NO = 100;USER TB_USER%ROWTYPE;USER_NOT_FOUND EXCEPTION;BEGIN OPEN CUR_USER; FETCH CUR_USER INTO USER; IF CUR_USER%NOTFOUND THEN RAISE USER_NOT_FOUND; END IF; EXCEPTION WHEN USER_NOT_FOUND THEN DBMS_OUTPUT.PUT_LINE('THERE IS NO FUCKING USER DATA!!!'); WHEN OTHERS THEN DBMS_OUTPUT.PUT_LINE('OTHER EXCEPTION'); CLOSE CUR_USER; -- 关闭光标这里由于出现了异常所以不会被执行，但是oracle自动启动pomon(process monitor)来自动回收cursorEND; 9.案例9.1按照入职年份统计员工数量 1234567891011121314151617181920212223242526272829DECLARECURSOR CUR_DATE IS SELECT TO_CHAR(HIREDATE,'YYYY') FROM TB_USER;UDATE VARCHAR2(4);COUNT10 NUMBER := 0;COUNT11 NUMBER := 0;COUNT12 NUMBER := 0;COUNT13 NUMBER := 0;COUNT14 NUMBER := 0;COUNT15 NUMBER := 0;COUNT16 NUMBER := 0;BEGIN OPEN CUR_DATE; LOOP FETCH CUR_DATE INTO UDATE; EXIT WHEN CUR_DATE%NOTFOUND; IF UDATE = '2016' THEN COUNT16 := COUNT16 + 1; ELSIF UDATE = '2010' THEN COUNT10 := COUNT10 + 1; ELSIF UDATE = '2011' THEN COUNT11 := COUNT11 + 1; ELSIF UDATE = '2012' THEN COUNT12 := COUNT12 + 1; ELSIF UDATE = '2013' THEN COUNT13 := COUNT13 + 1; ELSIF UDATE = '2014' THEN COUNT14 := COUNT14 + 1; ELSIF UDATE = '2015' THEN COUNT15 := COUNT15 + 1; END IF; END LOOP; CLOSE CUR_DATE; DBMS_OUTPUT.PUT_LINE('TOTAL:'||(COUNT10 + COUNT11 + COUNT12 + COUNT13 + COUNT14 + COUNT15 + COUNT16)); DBMS_OUTPUT.PUT_LINE('2011:'||COUNT11);END; 9.2员工涨工资从薪资最低的员工开始，每人涨10%的工资，要求工资总数不能超过50000 1234567891011121314151617181920212223242526DECLARECURSOR CUR_USER IS SELECT NO,SALARY FROM TB_USER ORDER BY SALARY;UNO NUMBER;USALARY NUMBER;TOTALMONEY NUMBER;TOTALCOUNT NUMBER := 0;INCREAMENTMONEY NUMBER;TMP NUMBER;BEGIN OPEN CUR_USER; SELECT SUM(SALARY) INTO TOTALMONEY FROM TB_USER; LOOP FETCH CUR_USER INTO UNO,USALARY; EXIT WHEN CUR_USER%NOTFOUND; INCREAMENTMONEY := USALARY * 0.1; TMP := TOTALMONEY + INCREAMENTMONEY; EXIT WHEN TMP &gt;= 50000; TOTALMONEY := TOTALMONEY + INCREAMENTMONEY; TOTALCOUNT := TOTALCOUNT + 1; UPDATE TB_USER SET SALARY = SALARY + INCREAMENTMONEY WHERE NO = UNO; END LOOP; CLOSE CUR_USER; DBMS_OUTPUT.PUT_LINE('TOTALCOUNT:'||TOTALCOUNT||' TOTALMONEY:'||TOTALMONEY); COMMIT;END; 9.3 按部门统计员工工资区间和部门总工资创建部门表 12345create table tb_dept(deptno number,name varchar(200),location varchar(200)); 插入数据 1234insert into TB_DEPT(DEPTNO,NAME,LOCATION) values(10,'会计部门','北京');insert into TB_DEPT(DEPTNO,NAME,LOCATION) values(20,'人力部门','上海');insert into TB_DEPT(DEPTNO,NAME,LOCATION) values(30,'研发部门','广州');insert into TB_DEPT(DEPTNO,NAME,LOCATION) values(40,'销售部门','深圳'); 创建统计结果表 12345678create table tb_msg( deptno number, count1 number, count2 number, count3 number, saltotal number); 123456789101112131415161718192021222324252627282930313233343536373839404142DECLARECURSOR CUR_DEPT IS SELECT DEPTNO FROM TB_DEPT;CURSOR CUR_USER(DNO NUMBER) IS SELECT * FROM TB_USER WHERE DEPTNO = DNO;DEPTNO TB_DEPT.DEPTNO%TYPE;USER TB_USER%ROWTYPE;COUNT1 NUMBER := 0;COUNT2 NUMBER := 0;COUNT3 NUMBER := 0;DEPT_TOTAL_SALARY NUMBER := 0;BEGIN OPEN CUR_DEPT; LOOP FETCH CUR_DEPT INTO DEPTNO; EXIT WHEN CUR_DEPT%NOTFOUND; COUNT1 := 0; COUNT2 := 0; COUNT3 := 0; DEPT_TOTAL_SALARY := 0; OPEN CUR_USER(DEPTNO); LOOP FETCH CUR_USER INTO USER; EXIT WHEN CUR_USER%NOTFOUND; DEPT_TOTAL_SALARY := DEPT_TOTAL_SALARY + USER.SALARY; IF USER.SALARY &lt;= 3000 THEN COUNT1 := COUNT1 + 1; ELSIF USER.SALARY &lt;= 6000 THEN COUNT2 := COUNT2 + 1; ELSE COUNT3 := COUNT3 + 1; END IF; END LOOP; CLOSE CUR_USER; DBMS_OUTPUT.PUT_LINE('DEPTNO='||DEPTNO||' DEPT_TOTAL_SALARY='||DEPT_TOTAL_SALARY ||' COUNT1='||COUNT1||' COUNT2='||COUNT2||' COUNT3='||COUNT3); INSERT INTO TB_MSG(DEPTNO,COUNT1,COUNT2,COUNT3,SALTOTAL) VALUES(DEPTNO,COUNT1,COUNT2,COUNT3,NVL(DEPT_TOTAL_SALARY,0)); END LOOP; CLOSE CUR_DEPT; COMMIT; END; 9.4按系名分段统计成绩(85) 大学物理课程各个分数段的学生人数，以及各系学生的平均成绩。各个表的创建和数据插入sql 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213drop table sc;drop table course;drop table student;drop table teacher;drop table dep;CREATE TABLE DEP (DNO NUMBER(2), DNAME VARCHAR2(30), DIRECTOR NUMBER(4), TEL VARCHAR2(8));CREATE TABLE TEACHER (TNO NUMBER(4), TNAME VARCHAR2(10), TITLE VARCHAR2(20), HIREDATE DATE, SAL NUMBER(7,2), BONUS NUMBER(7,2), MGR NUMBER(4), DEPTNO NUMBER(2));CREATE TABLE student (sno NUMBER(6), sname VARCHAR2(8), sex VARCHAR2(2), birth DATE, passwd VARCHAR2(8), dno NUMBER(2)); CREATE TABLE course (cno VARCHAR2(8), cname VARCHAR2(20), credit NUMBER(1), ctime NUMBER(2), quota NUMBER(3)); CREATE TABLE sc (sno NUMBER(6), cno VARCHAR2(8), grade NUMBER(3)); alter table dep add (constraint pk_deptno primary key(dno));alter table dep add(constraint dno_number_check check(dno&gt;=10 and dno&lt;=50));alter table dep modify(tel default 62795032);alter table student add (constraint pk_sno primary key(sno));alter table student add(constraint sex_check check(sex='男' or sex='女'));alter table student modify(birth default sysdate);alter table course add (constraint pk_cno primary key(cno));alter table sc add (constraint pk_key primary key(cno,sno));alter table teacher add (constraint pk_tno primary key(tno));alter table sc add (FOREIGN KEY(cno) REFERENCES course(cno));alter table sc add (FOREIGN KEY(sno) REFERENCES student(sno));alter table student add (FOREIGN KEY(dno) REFERENCES dep(dno));alter table teacher add (FOREIGN KEY(deptno) REFERENCES dep(dno)); INSERT INTO DEP VALUES (10, '计算机系', 9469 , '62785234');INSERT INTO DEP VALUES (20,'自动化系', 9581 , '62775234');INSERT INTO DEP VALUES (30,'无线电系', 9791 , '62778932');INSERT INTO DEP VALUES (40,'信息管理系', 9611, '62785520');INSERT INTO DEP VALUES (50,'微纳电子系', 2031, '62797686');INSERT INTO TEACHER VALUES(9468,'CHARLES','PROFESSOR','17-12月-2004',8000,1000,NULL,10);INSERT INTO TEACHER VALUES(9469,'SMITH','PROFESSOR','17-12月-2004',5000,1000 ,9468,10);INSERT INTO TEACHER VALUES(9470,'ALLEN','ASSOCIATE PROFESSOR', '20-2月-2003',4200,500,9469,10);INSERT INTO TEACHER VALUES(9471,'WARD','LECTURER', '22-2月-2004',3000,300,9469,10);INSERT INTO TEACHER VALUES(9581,'JONES','PROFESSOR ', '2-4月-2003',6500,1000,9468,20);INSERT INTO TEACHER VALUES(9582,'MARTIN','ASSOCIATE PROFESSOR ','28-9月-2005',4000,800,9581,20);INSERT INTO TEACHER VALUES(9583,'BLAKE','LECTURER ','1-5月-2006',3000,300,9581,20);INSERT INTO TEACHER VALUES(9791,'CLARK','PROFESSO', '9-6月-2003',5500,NULL,9468,30);INSERT INTO TEACHER VALUES(9792,'SCOTT','ASSOCIATE PROFESSOR ','09-12月-2004',4500,NULL,9791,30);INSERT INTO TEACHER VALUES(9793,'BAGGY','LECTURER','17-11月-2004',3000,NULL,9791,30);INSERT INTO TEACHER VALUES(9611,'TURNER','PROFESSOR ','8-9月-2005',6000,1000,9468,40);INSERT INTO TEACHER VALUES(9612,'ADAMS','ASSOCIATE PROFESSO','12-1月-2004',4800,800,9611,40);INSERT INTO TEACHER VALUES(9613,'JAMES','LECTURER','3-12月-2006',2800,200,9611,40);INSERT INTO TEACHER VALUES(2031,'FORD','PROFESSOR','3-12月-2005',5500,NULL,9468,50);INSERT INTO TEACHER VALUES(2032,'MILLER','ASSOCIATE PROFESSO','23-1月-2005',4300,NULL,2031,50);INSERT INTO TEACHER VALUES(2033,'MIGEAL','LECTURER','23-1月-2006',2900,NULL,2031,50);INSERT INTO TEACHER VALUES(2034,'PEGGY', 'LECTURER', '23-1月-2007',2500,NULL,2031,50);insert into student(birth,sno,sname,sex,passwd,dno) values('01-8月 -10',1,'John','男','123456',10);insert into student(birth,sno,sname,sex,passwd,dno) values('02-8月 -10',2,'Jacob','男','123456',10);insert into student(birth,sno,sname,sex,passwd,dno) values('03-8月 -10',3,'Michael','男','123456',10);insert into student(birth,sno,sname,sex,passwd,dno) values('04-8月 -10',4,'Joshua','男','123456',10);insert into student(birth,sno,sname,sex,passwd,dno) values('05-8月 -10',5,'Ethan','男','123456',10);insert into student(birth,sno,sname,sex,passwd,dno) values('06-8月 -10',6,'Matthew','男','123456',20);insert into student(birth,sno,sname,sex,passwd,dno) values('07-8月 -10',7,'Daniel','男','123456',20);insert into student(birth,sno,sname,sex,passwd,dno) values('08-8月 -10',8,'Chris','男','123456',20);insert into student(birth,sno,sname,sex,passwd,dno) values('09-8月 -10',9,'Andrew','男','123456',30);insert into student(birth,sno,sname,sex,passwd,dno) values('10-8月 -10',10,'Anthony','男','123456',30);insert into student(birth,sno,sname,sex,passwd,dno) values('11-8月 -10',11,'William','男','123456',30);insert into student(birth,sno,sname,sex,passwd,dno) values('12-8月 -10',12,'Joseph','男','123456',40);insert into student(birth,sno,sname,sex,passwd,dno) values('13-8月 -10',13,'Alex','男','123456',40);insert into student(birth,sno,sname,sex,passwd,dno) values('14-8月 -10',14,'David','男','123456',40);insert into student(birth,sno,sname,sex,passwd,dno) values('15-8月 -10',15,'Ryan','男','123456',40);insert into student(birth,sno,sname,sex,passwd,dno) values('16-8月 -10',16,'Noah','男','123456',40);insert into student(birth,sno,sname,sex,passwd,dno) values('17-8月 -10',17,'James','男','123456',40);insert into student(birth,sno,sname,sex,passwd,dno) values('18-8月 -10',18,'Nicholas','男','123456',50);insert into student(birth,sno,sname,sex,passwd,dno) values('19-8月 -10',19,'Tyler','男','123456',50);insert into student(birth,sno,sname,sex,passwd,dno) values('20-8月 -10',20,'Logan','男','123456',50);insert into student(birth,sno,sname,sex,passwd,dno) values('21-8月 -10',21,'Emily','女','123456',10);insert into student(birth,sno,sname,sex,passwd,dno) values('22-8月 -10',22,'Emma','女','123456',10);insert into student(birth,sno,sname,sex,passwd,dno) values('23-8月 -10',23,'Madis','女','123456',10);insert into student(birth,sno,sname,sex,passwd,dno) values('24-8月 -10',24,'Isabe','女','123456',10);insert into student(birth,sno,sname,sex,passwd,dno) values('25-8月 -10',25,'Ava','女','123456',10);insert into student(birth,sno,sname,sex,passwd,dno) values('26-8月 -10',26,'Abigail','女','123456',20);insert into student(birth,sno,sname,sex,passwd,dno) values('27-8月 -10',27,'Olivia','女','123456',20);insert into student(birth,sno,sname,sex,passwd,dno) values('28-8月 -10',28,'Hannah','女','123456',20);insert into student(birth,sno,sname,sex,passwd,dno) values('29-8月 -10',29,'Sophia','女','123456',30);insert into student(birth,sno,sname,sex,passwd,dno) values('30-8月 -10',30,'Samant','女','123456',30);insert into student(birth,sno,sname,sex,passwd,dno) values('31-8月 -10',31,'Elizab','女','123456',30);insert into student(birth,sno,sname,sex,passwd,dno) values('01-7月 -10',32,'Ashley','女','123456',40);insert into student(birth,sno,sname,sex,passwd,dno) values('02-7月 -10',33,'Mia','女','123456',40);insert into student(birth,sno,sname,sex,passwd,dno) values('11-8月 -10',34,'Alexis','女','123456',40);insert into student(birth,sno,sname,sex,passwd,dno) values('12-8月 -10',35,'Sarah','女','123456',40);insert into student(birth,sno,sname,sex,passwd,dno) values('13-8月 -10',36,'Natalie','女','123456',40);insert into student(birth,sno,sname,sex,passwd,dno) values('14-8月 -10',37,'Grace','女','123456',40);insert into student(birth,sno,sname,sex,passwd,dno) values('15-8月 -10',38,'Chloe','女','123456',50);insert into student(birth,sno,sname,sex,passwd,dno) values('16-8月 -10',39,'Alyssa','女','123456',50);insert into student(birth,sno,sname,sex,passwd,dno) values('17-8月 -10',40,'Brianna','女','123456',50); insert into course values('c001','数据结构',3,10,100);insert into course values('c002','Java语言',2,20,100);insert into course values('c003','数字电路',3,30,100);insert into course values('c004','模拟电路',3,40,100);insert into course values('c005','信号与系统',4,50,100);insert into course values('c006','C语言',3,60,100);insert into course values('c007','高等数学',5,70,100);insert into course values('c008','自动原理',1,80,100);insert into course values('c009','数理方程',3,90,100);insert into course values('c010','大学语文',2,61,100);insert into course values('c011','机械制图',3,52,100);insert into course values('c012','微机原理',3,43,100);insert into course values('c013','通信基础',3,74,100);insert into course values('c014','计算机原理',5,35,100);insert into course values('c015','数据库',3,86,100);insert into course values('c016','编译原理',2,97,100);insert into course values('c017','大学物理',2,38,100);insert into course values('c018','统计基础',4,50,100);insert into course values('c019','线性代数',4,70,100);insert into course values('c020','Linux基础',3,60,100);insert into sc values(6,'c002',60);insert into sc values(6,'c015',60);insert into sc values(6,'c010',61);insert into sc values(27,'c010',65);insert into sc values(6,'c001',60);insert into sc values(6,'c011',61);insert into sc values(6,'c018',70);insert into sc values(8,'c007',65);insert into sc values(27,'c020',65);insert into sc values(27,'c015',65); insert into sc values(26,'c015',55); insert into sc values(25,'c015',59); insert into sc values(1,'c017',65);insert into sc values(2,'c017',66);insert into sc values(3,'c017',67);insert into sc values(4,'c017',68);insert into sc values(5,'c017',69);insert into sc values(6,'c017',70);insert into sc values(7,'c017',71);insert into sc values(8,'c017',72);insert into sc values(9,'c017',73);insert into sc values(10,'c017',74);insert into sc values(11,'c017',75);insert into sc values(12,'c017',76);insert into sc values(13,'c017',77);insert into sc values(14,'c017',78);insert into sc values(15,'c017',79);insert into sc values(16,'c017',80);insert into sc values(17,'c017',81);insert into sc values(18,'c017',82);insert into sc values(19,'c017',83);insert into sc values(20,'c017',84);insert into sc values(21,'c017',85);insert into sc values(22,'c017',86);insert into sc values(23,'c017',87);insert into sc values(24,'c017',88);insert into sc values(25,'c017',89);insert into sc values(26,'c017',90);insert into sc values(27,'c017',89);insert into sc values(28,'c017',88);insert into sc values(29,'c017',87);insert into sc values(30,'c017',86);insert into sc values(31,'c017',85);insert into sc values(32,'c017',84);insert into sc values(33,'c017',83);insert into sc values(34,'c017',82);insert into sc values(35,'c017',81);insert into sc values(36,'c017',80);insert into sc values(37,'c017',79);insert into sc values(38,'c017',78);insert into sc values(39,'c017',77);insert into sc values(40,'c017',76);commit; PL/SQL程序 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859DECLARECURSOR CUR_COURSE IS SELECT * FROM COURSE;COS COURSE%ROWTYPE;CURSOR CUR_DEPT IS SELECT DNO,DNAME FROM DEP;DEPART_NO DEP.DNO%TYPE;DEPART_DNAME DEP.DNAME%TYPE;--CURSOR CUR_SC(DEPART_NO DEP.DNO%TYPE,COURSE_NO COURSE.CNO%TYPE) IS SELECT * FROM SC WHERE SNO IN (SELECT SNO FROM STUDENT WHERE DNO = DEPART_NO) AND CNO = COURSE_NO;CURSOR CUR_SC(DEPART_NO DEP.DNO%TYPE,COURSE_NAME COURSE.CNAME%TYPE) IS SELECT * FROM SC WHERE CNO=(SELECT CNO FROM COURSE WHERE CNAME = COURSE_NAME) AND SNO IN (SELECT SNO FROM STUDENT WHERE DNO = DEPART_NO);S_C SC%ROWTYPE;COUNT1 NUMBER := 0;COUNT2 NUMBER := 0;COUNT3 NUMBER := 0;AVGGRADE NUMBER := 0;COURSE_DEPT_TOTAL_GRADE NUMBER := 0;BEGINOPEN CUR_COURSE;LOOP FETCH CUR_COURSE INTO COS; EXIT WHEN CUR_COURSE%NOTFOUND; OPEN CUR_DEPT; LOOP FETCH CUR_DEPT INTO DEPART_NO,DEPART_DNAME; EXIT WHEN CUR_DEPT%NOTFOUND; OPEN CUR_SC(DEPART_NO,COS.CNAME); COUNT1 := 0; COUNT2 := 0; COUNT3 := 0; COURSE_DEPT_TOTAL_GRADE := 0; AVGGRADE := 0; SELECT AVG(GRADE) INTO AVGGRADE FROM SC WHERE CNO = (SELECT CNO FROM COURSE WHERE CNAME = COS.CNAME) AND SNO IN (SELECT SNO FROM STUDENT WHERE DNO = DEPART_NO); LOOP FETCH CUR_SC INTO S_C; EXIT WHEN CUR_SC%NOTFOUND; IF S_C.GRADE &lt; 60 THEN COUNT1 := COUNT1 + 1; ELSIF S_C.GRADE &lt;85 THEN COUNT2 := COUNT2 + 1; ELSE COUNT3 := COUNT3 + 1; END IF; COURSE_DEPT_TOTAL_GRADE := COURSE_DEPT_TOTAL_GRADE + S_C.GRADE; END LOOP; CLOSE CUR_SC;-- IF COUNT1 + COUNT2 + COUNT3 = 0 THEN AVGGRADE := 0;-- ELSE AVGGRADE := COURSE_DEPT_TOTAL_GRADE/(COUNT1+COUNT2+COUNT3);-- END IF;-- INSERT INTO TB_SCORREMSG(CNAME,DNAME,COUNT1,COUNT2,COUNT3,AVGSCORE) VALUES(COS.CNAME,DEPART_DNAME,COUNT1,COUNT2,COUNT3,AVGGRADE); DBMS_OUTPUT.PUT_LINE('课程：'||COS.CNAME||' 系名：'||DEPART_DNAME||' 低于60分：'||COUNT1||' 60~85分：'||COUNT2||' 85分以上:'||COUNT3||' 系总分 ：'||COURSE_DEPT_TOTAL_GRADE||' 平均分:'||AVGGRADE); END LOOP; CLOSE CUR_DEPT;END LOOP;CLOSE CUR_COURSE;COMMIT;END;","tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://seawaylee.github.io/tags/Oracle/"}]},{"title":"数据库编程入门(二)-存储过程和存储函数","date":"2017-03-09T16:45:48.000Z","path":"2017/03/10/Oracle/存储过程/","text":"1.什么是存储过程和存储函数相同点：存储过程和存储函数都是完成特定功能的程序区别： 存储函数可用return语句返回值，存储过程不可以 123CREATE OR REPLACE PROCUDURE [PNAME(PARAMS)]AS -- AS相当于PLSQL的DECLAREPLSQL程序体 2.最简单的存储过程12345678910111213141516171819--SET SERVEROUTPUT ON;CREATE OR REPLACE PROCEDURE HELLOWORLDAS-- 说明部分BEGIN DBMS_OUTPUT.PUT_LINE('HELLO WORLD');END;//*调用存储过程：1.exec HELLOWORLD();2.begin HELLOWORLS(); end;*/exec HELLOWORLD; 3.带参数的存储过程123456789101112131415-- 创建一个带参数的存储过程-- 给指定员工涨工资create or replace procedure raise_salary(uno in tb_user.no%type,usalary in tb_user.salary%type)asold_salary tb_user.salary%type;begin select salary into old_salary from tb_user where no = uno; dbms_output.put_line('Before rasing salary:'||old_salary||' After rasing salary:'|| (500.0 + old_salary)); update tb_user set salary = old_salary + 500 where no = uno;end;/exec raise_salary(1,500); 4.debug存储过程 5.存储函数1234567891011121314151617181920212223CREATE OR REP;ACE FUNCTIOPN 函数名(参数列表)return 函数值类型asplsql程序体-- 根据员工编号查询员工的年薪create or replace function check_year_salary(pno in tb_user.no%type)return tb_user.no%typeasvsalary tb_user.salary%type;begin select salary into vsalary from tb_user where no = pno; return vsalary * 12;end;/begin dbms_output.put_line(CHECK_YEAR_SALARY(1));end; 6.in和out参数我们知道存储过程和存储函数的区别主要在于函数可以return一个结果，那么如何使存储过程和存储函数返回多个值呢？ 存储过程和存储函数都可以有out参数 存储过程和存储函数都可以有多个out参数 存储过程可以通过out参数来实现返回值 原则：如果只有一个返回值，就用存储函数；否则，就用存储过程。 12345678910111213141516171819202122-- out参数：查询某个员工的姓名月薪和职位create or replace procedure get_user_info( pno in number, pname out varchar2, psalary out number, pjob out varchar2)asbegin select name,salary,job into pname,psalary,pjob from tb_user where no = pno;end;/declarepname varchar2(200);psalary number;pjob varchar2(200);begin get_user_info(1,pname,psalary,pjob); dbms_output.put_line(pname||' '||psalary||' '||pjob);end; 7.Java调用存储过程1) 从Oracle客户端找到ojdbc14.jar2) 创建java项目，将jdbc驱动加载到buildpath3) 编写JDBC连接代码和调用存储过程代码 示例 调用上面查询用户信息的存储过程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133import oracle.jdbc.OracleTypes;import org.junit.Test;import java.sql.*;/** * Created by NikoBelic on 2016/11/23. */public class TestProcedure&#123; private static String driver = \"oracle.jdbc.driver.OracleDriver\"; private static String url = \"jdbc:oracle:thin:@192.168.1.176:1521:orcl\"; private static String user = \"learn\"; private static String password = \"learn\"; /** * 注册数据库驱动 * @Author NikoBelic 2016/11/23 15:56 */ static &#123; try &#123; Class.forName(driver); // 反射注册oracle驱动 &#125; catch (ClassNotFoundException e) &#123; throw new ExceptionInInitializerError(e); &#125; &#125; /** * 获取数据库连接 * * @Author NikoBelic 2016/11/23 15:56 */ public static Connection getConnection() &#123; try &#123; return DriverManager.getConnection(url, user, password); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 关闭连接 * @Author NikoBelic 2016/11/23 */ public static void release(Connection conn, Statement stat, ResultSet rs) &#123; try &#123; if (rs != null) rs.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; rs = null; &#125; try &#123; if (stat != null) stat.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; stat = null; &#125; try &#123; if (conn != null) conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; conn = null; &#125; &#125; @Test public void test() &#123; Connection conn = null; CallableStatement stat = null; String sql = \"&#123;call GET_USER_INFO(?,?,?,?)&#125;\"; try &#123; // 获取连接 conn = getConnection(); // 创建Statement stat = conn.prepareCall(sql); // 对in参数赋值 stat.setInt(1,1); // 对out参数声明 stat.registerOutParameter(2, OracleTypes.VARCHAR); stat.registerOutParameter(3, OracleTypes.NUMBER); stat.registerOutParameter(4, OracleTypes.VARCHAR); // 执行调用 stat.execute(); // 取出结果 String name = stat.getString(2); Double salary = stat.getDouble(3); String job = stat.getString(4); System.out.println(name + \" \" + salary + \" \" + job); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; release(conn,stat,null); &#125; &#125;&#125; 结果 8.Java调用存储函数数据库连接方法同上，新增一个Test方法如下 123456789101112131415161718192021222324252627282930/** * 调用存储函数的测试：查询员工的年收入 * @Author NikoBelic 2016/11/23 16:33 */ @Test public void testFunction() &#123; Connection conn = getConnection(); CallableStatement stat = null; String sql = \"&#123;?=call CHECK_YEAR_SALARY(?)&#125;\"; try &#123; stat = conn.prepareCall(sql); stat.registerOutParameter(1,OracleTypes.NUMBER); stat.setInt(2,1); stat.execute(); Double yearSalary = stat.getDouble(1); System.out.println(\"该员工的年薪是 \" + yearSalary); &#125;catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; release(conn,stat,null); &#125; &#125; 结果 9.如何在out参数中使用光标(cursor)场景：加入我需要获取所有员工的信息，总不能传几十个参数到存储过程中的out参数吧？在out参数中使用光标需要以下三个步骤1) 声明一个包结构2) 创建包头3) 创建包体 9.1新建程序包，自动生成包头 12345678CREATE OR REPLACEPACKAGE MYPACKAGE AS /* TODO enter package declarations (types, exceptions, methods etc) here */ type cur_user is ref cursor; -- 自定义一个类型(光标) procedure queryUserList(dno in number,userList out cur_user); -- 声明一个存储过程END MYPACKAGE; 9.2创建包体 12345678910CREATE OR REPLACEPACKAGE BODY MYPACKAGE AS procedure queryUserList(dno in number,userList out cur_user) AS BEGIN -- TODO: procedure MYPACKAGE.queryUserList所需的实施 open userList for select * from tb_user where DEPTNO = dno; END queryUserList;END MYPACKAGE; 使用desc查看程序包结构 1desc MYPACKAGE 9.3调用包中的存储过程(sqldeveloper工具不支持直接调用)1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 调用包中的存储过程，返回一个结果集(光标) * 查询指定部门下的所有员工信息 * @Author NikoBelic 2016/11/23 17:08 */ @Test public void testCursor() &#123; Connection conn = null; CallableStatement stat = null; ResultSet rs = null; String sql = \"&#123;call MYPACKAGE.queryUserList(?,?)&#125;\"; // 第一个参数是in类型部门编号 第二个参数是out类型光标结果集; try &#123; conn = getConnection(); stat = conn.prepareCall(sql); stat.setInt(1,10); // 将out参数这是为Cursor类型 stat.registerOutParameter(2,OracleTypes.CURSOR); stat.execute(); // 这里必须强制类型转换，CallableStatement接口没有getCursor这个方法 rs = ((OracleCallableStatement) stat).getCursor(2); while (rs.next()) &#123; // 获取结果有两种方式 一种是用角标过去rs.getIndex(int),另一种就是通过字段名称获取 String name = rs.getString(\"name\"); Double salary = rs.getDouble(\"salary\"); Date hireDate = rs.getDate(\"hireDate\"); String job = rs.getString(\"job\"); System.out.println(name + \" \" + salary + \" \" + hireDate + \" \" + job); &#125; &#125;catch (Exception e) &#123; &#125;finally &#123; release(conn,stat,rs); &#125; &#125; 结果 10.总结存储过程和存储函数主要区别在于return。但是有了out参数，存储过程完全可以替代存储函数。 那为什么还有存储函数这个垃圾？因为Oracle的版本问题，需要向下兼容。","tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://seawaylee.github.io/tags/Oracle/"}]},{"title":"数据库编程入门(三)-触发器的应用","date":"2017-03-09T16:40:40.000Z","path":"2017/03/10/Oracle/触发器/","text":"1.什么是触发器 触发器是一种特殊的存储过程 每当一个特定的数据操作语句（insert,update,delete）在指定的表上发出时，Oracle自动地执行触发器中定义的语句序列。 2.触发器的应用场景复杂的安全性检查数据确认实现审计功能完成数据的备份和同步 3.最简单的触发器3.0 触发器基本语法123456create or replace trigger 触发器名before | after delete | insert | update [of 列名]on 表名[for each row [when 条件]] -- 行级触发器 或 语句级触发器 3.1 语句级触发器在指定的操作语句操作之前或之后执行一次，不管这条语句影响了多少行 – 针对的是表 3.2行级触发器触发语句作用的每一条记录都被触发。在行级触发器中使用 :old 和 :new 伪记录变量，识别值的状态。– 针对的是行 1234567891011121314create or replace TRIGGER say_hi_to_new_userafter inserton tb_userdeclarebegin dbms_output.put_line('Hello New User...');end;/insert into tb_user(no,name,salary,job,hiredate,deptno) values(10,'NikoBelic',5000,'MANAGER',sysdate,20); 输出 1行已插入。 Hello New User... 4.触发器应用实例4.1 案例一:复杂安全性检查 禁止在非工作时间插入员工1234567891011121314151617create or replace trigger time_checkbefore inserton tb_userdeclarebegin if to_char(sysdate,'day') in ('星期六','星期日') or to_number(to_char(sysdate,'hh24')) not between 9 and 17 then RAISE_APPLICATION_ERROR(-20001, '禁止在非工作时间插入新员工'); end if;end;/insert into tb_user(no,name,salary,job,hiredate,deptno) values(11,'NikoBelic',5000,'MANAGER',sysdate,20); 输出：错误报告 - SQL 错误: ORA-20001: 禁止在非工作时间插入新员工 ORA-06512: 在 &quot;LEARN.TIME_CHECK&quot;, line 7 ORA-04088: 触发器 &#39;LEARN.TIME_CHECK&#39; 执行过程中出错 4.2 案例二：数据的确认1234567891011121314151617181920212223/* 触发器案例2：数据的确认 涨后工资不能少于涨前工资 1. :old ,:new 代表同一条记录 2. :old 表示操作该行之前，这一行的值 :new 表示操作改行之后，这一行的值*/create or replace trigger check_salarybefore updateon tb_userfor each rowbegin -- if 涨后的工资 &lt; 涨前的工资 then if :new.salary &lt; :old.salary then RAISE_APPLICATION_ERROR(-20002, '涨后工资比涨前工资还低?滚!'); end if;end;/update tb_user t set t.salary = t.salary - 100 where t.no = 1; 输出：错误报告 - SQL 错误: ORA-20002: 涨后工资比涨前工资还低?滚! ORA-06512: 在 &quot;LEARN.CHECK_SALARY&quot;, line 3 ORA-04088: 触发器 &#39;LEARN.CHECK_SALARY&#39; 执行过程中出错 4.3案例三：数据库的审计123456789101112131415161718192021/*触发器应用场景三：数据库的审计 --&gt;基于值的审计功能给员工涨工资，当涨后薪资超过6000时，审计该员工信息*/create table tb_audit( infomation varchar2(200));create or replace trigger audit_userafter updateon tb_userfor each rowbegin -- 当涨后工资超过6000时，插入审计信息 if :new.salary &gt; 6000 then insert into tb_audit(infomation) values(:new.name || ' ' || :new.salary); end if;end;update tb_user t set t.salary = t.salary + 2000; 4.4 案例四：数据的备份和同步当给员工涨完工资后，自动备份新的工资到备份表中创建备份表1234567891011121314create table tb_user_back as select * from tb_user;create or replace trigger sync_salaryafter updateon tb_userfor each rowbegin update tb_user_back set salary = :new.salary where no = :new.no;end;/update tb_user t set t.salary = t.salary + 1 where t.no = 1 ; 4.5 案例5, 防止数据重复插入此案例结合存储过程锁，可以有效防止高并发问题。 1234567891011121314151617181920212223create or replace trigger check_repeatbefore inserton tb_userfor each rowdeclarecursor cur_user is select name from tb_user;pname tb_user.name%type;beginopen cur_user; loop fetch cur_user into pname; exit when cur_user%notfound; dbms_output.put_line(pname); if :new.name = pname then RAISE_APPLICATION_ERROR(-20003, '该用户已经被存储过了'); end if; end loop;end;/insert into tb_user(no,name,salary,job,hiredate,deptno) values(12,'Zhangyu',5000,'MANAGER',sysdate,20); 输出在行: 23 上开始执行命令时出错 - insert into tb_user(no,name,salary,job,hiredate,deptno) values(12,&#39;Zhangyu&#39;,5000,&#39;MANAGER&#39;,sysdate,20)错误报告 - SQL 错误: ORA-20003: 该用户已经被存储过了 ORA-06512: 在 &quot;LEARN.CHECK_REPEAT&quot;, line 12 ORA-04088: 触发器 &#39;LEARN.CHECK_REPEAT&#39; 执行过程中出错","tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://seawaylee.github.io/tags/Oracle/"}]},{"title":"json.loads&json.dumps的使用","date":"2017-03-09T16:34:49.000Z","path":"2017/03/10/Python基础/json与dict转换/","text":"每次遇到json loads/dumps始终搞不清方向，写段代码试下： 1234567891011121314151617181920212223242526import json dict_ = &#123;1:2, 3:4, \"55\":\"66\"&#125; # test json.dumps print type(dict_), dict_ json_str = json.dumps(dict_) print \"json.dumps(dict) return:\" print type(json_str), json_str # test json.loads print \"\\njson.loads(str) return\" dict_2 = json.loads(json_str) print type(dict_2), dict_2# 程序结果：&lt;type 'dict'&gt; &#123;'55': '66', 1: 2, 3: 4&#125;json.dump(dict) return:&lt;type 'str'&gt; &#123;\"55\": \"66\", \"1\": 2, \"3\": 4&#125;json.loads(str) return&lt;type 'dict'&gt; &#123;u'55': u'66', u'1': 2, u'3': 4&#125; 总结： json.dumps : dict转成str json.loads:str转成dict 如此简单。","tags":[{"name":"Python基础","slug":"Python基础","permalink":"https://seawaylee.github.io/tags/Python基础/"}]},{"title":"两个字典（dict）合并","date":"2017-03-09T16:31:42.000Z","path":"2017/03/10/Python基础/字典合并/","text":"123dict1=&#123;1:[1,11,111],2:[2,22,222]&#125;dict2=&#123;3:[3,33,333],4:[4,44,444]&#125; 合并两个字典得到类似12&#123;1:[1,11,111],2:[2,22,222],3:[3,33,333],4:[4,44,444]&#125; 方法1： 1dictMerged1=dict(dict1.items()+dict2.items()) 方法2：12dictMerged2=dict(dict1, **dict2) 方法2等同于：123dictMerged=dict1.copy() dictMerged.update(dict2) 或者 123dictMerged=dict1.copy() dictMerged.update(dict2) 方法2比方法1速度快很多，用timeit测试如下123456789$ python -m timeit -s &apos;dict1=dict2=dict((i,i) for i in range(100))&apos; &apos;dictMerged1=dict(dict1.items()+dict2.items())&apos; 10000 loops, best of 3: 20.7 usec per loop $ python -m timeit -s &apos;dict1=dict2=dict((i,i) for i in range(100))&apos; &apos;dictMerged2=dict(dict1,**dict2)&apos; 100000 loops, best of 3: 6.94 usec per loop $ python -m timeit -s &apos;dict1=dict2=dict((i,i) for i in range(100))&apos; &apos;dictMerged3=dict(dict1)&apos; &apos;dictMerged3.update(dict2)&apos; 100000 loops, best of 3: 7.09 usec per loop $ python -m timeit -s &apos;dict1=dict2=dict((i,i) for i in range(100))&apos; &apos;dictMerged4=dict1.copy()&apos; &apos;dictMerged4.update(dict2)&apos; 100000 loops, best of 3: 6.73 usec per loop","tags":[{"name":"Python基础","slug":"Python基础","permalink":"https://seawaylee.github.io/tags/Python基础/"}]},{"title":"字典排序","date":"2017-03-09T16:29:46.000Z","path":"2017/03/10/Python基础/字典排序/","text":"Python 字典（dict）的特点就是无序的，按照键（key）来提取相应值（value），如果我们需要字典按值排序的话，那可以用下面的方法来进行： 按value排序12345678dic = &#123;'a':31, 'bc':5, 'c':3, 'asd':4, 'aa':74, 'd':0&#125;dict= sorted(dic.iteritems(), key=lambda d:d[1], reverse = True)print dict# 输出的结果：[('aa', 74), ('a', 31), ('bc', 5), ('asd', 4), ('c', 3), ('d', 0)] 下面我们分解下代码print dic.iteritems() 得到[(键，值)]的列表。然后用sorted方法，通过key这个参数，指定排序是按照value，也就是第一个元素d[1的值来排序。reverse = True表示是需要翻转的，默认是从小到大，翻转的话，那就是从大到小。 按key排序 123dic = &#123;'a':31, 'bc':5, 'c':3, 'asd':4, 'aa':74, 'd':0&#125;dict= sorted(dic.iteritems(), key=lambda d:d[0]) d[0]表示字典的键print dict","tags":[{"name":"Python基础","slug":"Python基础","permalink":"https://seawaylee.github.io/tags/Python基础/"}]},{"title":"格式化日期与时间戳转换","date":"2017-03-09T16:28:18.000Z","path":"2017/03/10/Python基础/格式化日期/","text":"1234567891011121314151617#设a为字符串import timea = \"2011-09-28 10:00:00\"#中间过程，一般都需要将字符串转化为时间数组time.strptime(a,'%Y-%m-%d %H:%M:%S')&gt;&gt;time.struct_time(tm_year=2011, tm_mon=9, tm_mday=27, tm_hour=10, tm_min=50, tm_sec=0, tm_wday=1, tm_yday=270, tm_isdst=-1)#将\"2011-09-28 10:00:00\"转化为时间戳time.mktime(time.strptime(a,'%Y-%m-%d %H:%M:%S'))&gt;&gt;1317091800.0#将时间戳转化为localtimex = time.localtime(1317091800.0)time.strftime('%Y-%m-%d %H:%M:%S',x)&gt;&gt;2011-09-27 10:50:00#将时间戳转换为日期format_time = time.strftime(\"%Y-%m-%d\", time.localtime( 1317091800.0))&gt;&gt;2011-09-27","tags":[{"name":"Python基础","slug":"Python基础","permalink":"https://seawaylee.github.io/tags/Python基础/"}]},{"title":"Timer与ScheduledThreadPoolExecutor","date":"2017-03-09T16:25:53.000Z","path":"2017/03/10/Java基础/多线程/Timer与ScheduledThreadPoolExecutor/","text":"12345678910111213141516171819202122232425262728293031import java.text.SimpleDateFormat; import java.util.Date; import java.util.concurrent.ScheduledThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class TestScheduledThreadPoolExecutor &#123; public static void main(String[] args) &#123; String time = new SimpleDateFormat(\"HH:mm:ss\").format(new Date()); System.out.println(\"Start time : \" + time); ScheduledThreadPoolExecutor executor = new ScheduledThreadPoolExecutor(5); //创建5个执行线程 Runnable runnable = new Runnable() &#123; @Override public void run() &#123; // TODO Auto-generated method stub String time = new SimpleDateFormat(\"HH:mm:ss\").format(new Date()); System.out.println(\"Now Time : \" + time); &#125; &#125;; executor.scheduleWithFixedDelay(runnable, 2, 3, TimeUnit.SECONDS); &#125; &#125; 在实际应用中，有时候我们需要创建一些个延迟的、并具有周期性的任务，比如，我们希望当我们的程序启动后每隔1小时就去做一次日志记录。在JDK中提供了两种方法去创建延迟周期性任务。 TimerTimer是java.util包下的一个类，在JDK1.3的时候被引入，Timer只是充当了一个执行者的角色，真正的任务逻辑是通过一个叫做TimerTask的抽象类完成的，TimerTask也是java.util包下面的类，它是一个实现了Runnable接口的抽象类，包含一个抽象方法run( )方法，需要我们自己去提供具体的业务实现。Timer类对象是通过其schedule方法执行TimerTask对象中定义的业务逻辑，并且schedule方法拥有多个重载方法提供不同的延迟与周期性服务。 下面是利用Timer去创建的一个延时周期性任务 123456789101112131415161718192021222324252627282930import java.text.SimpleDateFormat; import java.util.Date; import java.util.Timer; import java.util.TimerTask; public class TestTimer &#123; public static void main(String[] args) &#123; String time = new SimpleDateFormat(\"HH:mm:ss\").format(new Date()); System.out.println(\"Start time : \" + time); Timer timer = new Timer(); TimerTask task = new TimerTask() &#123; @Override public void run() &#123; // TODO Auto-generated method stub String time = new SimpleDateFormat(\"HH:mm:ss\").format(new Date()); System.out.println(\"Now Time : \" + time); &#125; &#125;; //end task timer.schedule(task, 2000, 3000); &#125; &#125; 程序的输出：Start time : 21:36:08Now Time : 21:36:10Now Time : 21:36:13Now Time : 21:36:16Now Time : 21:36:19 ScheduledThreadPoolExecutor在JDK1.5的时候在java.util.concurrent并发包下引入了ScheduledThreadPoolExecutor类，引入它的原因是因为Timer类创建的延迟周期性任务存在一些缺陷，ScheduledThreadPoolExecutor继承了ThreadPoolExecutor，并且实现了ScheduledExecutorService接口，ScheduledThreadPoolExecutor也是通过schedule方法执行Runnable任务的。我们用ScheduledThreadPoolExecutor来实现和上述Timer一样的功能 程序的输出：Start time : 22:12:25Now Time : 22:12:27Now Time : 22:12:30Now Time : 22:12:33Now Time : 22:12:36 这样看来Timer和ScheduledThreadPoolExecutor好像没有声明差别，但是ScheduledThreadPoolExecutor的引入正是由于Timer类存在的一些不足，并且在JDK1.5或更高版本中，几乎没有利用继续使用Timer类，下面说明Timer存在的一些缺点。 单线程Timer类是通过单线程来执行所有的TimerTask任务的，如果一个任务的执行过程非常耗时，将会导致其他任务的时效性出现问题。而ScheduledThreadPoolExecutor是基于线程池的多线程执行任务，不会存在这样的问题。这里我们通过让Timer来执行两个TimerTask任务来说明，其中一个TimerTask的执行过程是耗时的，加入需要2秒。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.text.SimpleDateFormat; import java.util.Date; import java.util.Timer; import java.util.TimerTask; public class SingleThreadTimer &#123; public static void main(String[] args) &#123; String time = new SimpleDateFormat(\"HH:mm:ss\").format(new Date()); System.out.println(\"Start time : \" + time); Timer timer = new Timer(); TimerTask task1 = new TimerTask() &#123; @Override public void run() &#123; // TODO Auto-generated method stub String time = new SimpleDateFormat(\"HH:mm:ss\").format(new Date()); System.out.println(\"Task1 time : \" + time); &#125; &#125;; TimerTask task2 = new TimerTask() &#123; @Override public void run() &#123; // TODO Auto-generated method stub try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; String time = new SimpleDateFormat(\"HH:mm:ss\").format(new Date()); System.out.println(\"task2 time : \" + time); &#125; &#125;; timer.schedule(task1, 2000, 1000); timer.schedule(task2, 2000, 3000); &#125; &#125; 这里定义了两个任务，任务1，程序启动2秒后每隔1秒运行一次，任务2，程序启动2秒后，每隔3秒运行1次，然后让Timer同时运行这两个任务程序的输出如下：Start time : 22:22:37Task1 time : 22:22:39task2 time : 22:22:41Task1 time : 22:22:41Task1 time : 22:22:42task2 time : 22:22:44Task1 time : 22:22:44Task1 time : 22:22:45task2 time : 22:22:47Task1 time : 22:22:47Task1 time : 22:22:48 可以分析，无论是任务1还是任务2都没有按照我们设定的预期进行运行，造成这个现象的原因就是Timer类是单线程的。 Timer线程不捕获异常Timer类中是不捕获异常的，假如一个TimerTask中抛出未检查异常（P.S：java中异常分为两类:checked exception(检查异常)和unchecked exception(未检查异常),对于未检查异常也叫RuntimeException(运行时异常). ），Timer类将不会处理这个异常而产生无法预料的错误。这样一个任务抛出异常将会导致整个Timer中的任务都被取消，此时已安排但未执行的TimerTask也永远不会执行了，新的任务也不能被调度（所谓的“线程泄漏”现象）。下面就已常见的RuntimeException，ArrayIndexOutOfBoundsException数组越界异常，来演示这个缺点： 123456789101112131415161718192021222324252627282930313233343536373839404142434445import java.text.SimpleDateFormat; import java.util.Date; import java.util.Timer; import java.util.TimerTask; public class TestTimerTask &#123; public static void main(String[] args) &#123; System.out.println(new SimpleDateFormat(\"HH:mm:ss\").format(new Date())); Timer timer = new Timer(); TimerTask task1 = new TimerTask() &#123; @Override public void run() &#123; System.out.println(\"1: \" + new SimpleDateFormat(\"HH:mm:ss\").format(new Date())); &#125; &#125;; TimerTask task2 = new TimerTask() &#123; @Override public void run() &#123; int[] arr = &#123;1,2,3,4,5&#125;; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; int index = (int)(Math.random()*100); System.out.println(arr[index]); System.out.println(\"2: \" + new SimpleDateFormat(\"HH:mm:ss\").format(new Date())); &#125; &#125;; timer.schedule(task1, 2000, 3000); timer.schedule(task2, 2000, 1000); &#125; &#125; 程序会在运行过程中抛出数组越界异常，并且整个程序都会被终止，原来完好的任务1也被终止了。 基于绝对时间Timer类的调度是基于绝对的时间的，而不是相对的时间，因此Timer类对系统时钟的变化是敏感的，举个例子，加入你希望任务1每个10秒执行一次，某个时刻，你将系统时间提前了6秒，那么任务1就会在4秒后执行，而不是10秒后。在ScheduledThreadPoolExecutor，任务的调度是基于相对时间的，原因是它在任务的内部存储了该任务距离下次调度还需要的时间（使用的是基于System#nanoTime实现的相对时间，不会因为系统时间改变而改变，如距离下次执行还有10秒，不会因为将系统时间调前6秒而变成4秒后执行）。 基于以上3个弊端，在JDK1.5或以上版本中，我们几乎没有理由继续使用Timer类，ScheduledThreadPoolExecutor可以很好的去替代Timer类来完成延迟周期性任务。","tags":[{"name":"多线程","slug":"多线程","permalink":"https://seawaylee.github.io/tags/多线程/"}]},{"title":"CodeReview工具：UpSource搭建与使用","date":"2017-03-09T16:21:57.000Z","path":"2017/03/10/Mac工具/UpSource搭建教程/","text":"1 下载、安装、配置1.1 官网主页下载zip包https://www.jetbrains.com/upsource/features/免费版可以创建10个用户，admin + guest + 8 user 1.2 安装 配置要求：内存建议8G以上 Linux系统配置/etc/security/limits.conffile:（可选操作） 查看linux文件打开上限ulimit -n，ulimit -n 9999999 设置上限 maximum open files to 100000 memory locking and address space limit to unlimited number of processes to 32768 1234memlock unlimitednofile 100000nproc 32768as unlimited unzip upsource-3.5.3616.zip 1.3 命令 启动./upsource.sh start 关闭./upsource.sh stop 重启./upsource.sh restart 1.4 配置Upsource执行启动命令后，访问提示的地址进入WEB管理界面 123456789101112131415[root@sanjiao070 bin]# ./upsource.sh startStarting Upsource...* Configuring JetBrains Upsource 3.5 * Made default base-url &apos;http://sanjiao070.cn:8080/&apos; from hostname &apos;sanjiao070.cn&apos; and listen port &apos;8080&apos; * JetBrains Upsource 3.5 runtime environment is successfully configured * Loading logging configuration from /opt/upsource-3.5.3616/lib/ext/log4j.xml * Redirecting JetBrains Upsource 3.5 logging to /opt/upsource-3.5.3616/logs/internal/services/bundleProcess * Configuring Service-Container[bundleProcess] * Configuring Bundle Backend Service * Configuring Configuration Wizard * Starting Service-Container[bundleProcess] * Starting Bundle Backend Service * Starting Configuration Wizard * JetBrains Upsource 3.5 Configuration Wizard will be available on [http://sanjiao070.cn:8080] after start Upsource is running 基本配置上面的BaseUrl地址配错了，可以在命令行中重新配置（先关闭服务） ./upsource.sh configure --listen-port 8000 --base-url http://10.10.0.70:8000 配置用户认证中心，使用默认的build-in Hub 使用默认的免费licenseFinish之后服务会重启，可以在服务器中查看日志tail -f logs/upsource-stdout.log 2 UpSource关联版本控制工具（本文为SVN）创建svn项目 创建用户、分配角色、分配用户组 将UpSource用户与svn用户关联。 3 测试1.项目的CodeReviewer角色可以创建review信息2.代码的提交者登陆后会看到关于自己代码被的查看的信息解决问题以后标记Resolved 4 Idea整合UpSource下载Idea+UpSource插件 重启Idea后在Tools中 找到UpSource并与服务端测试连接点击TestConnection自动弹出授权Web页面，使用管理员账号登陆后点击Accept 授权成功 在Idea中也可以看到刚才的Review信息了","tags":[{"name":"Mac工具","slug":"Mac工具","permalink":"https://seawaylee.github.io/tags/Mac工具/"}]},{"title":"如何在同一台电脑上使用两个Git账户","date":"2017-03-09T16:18:37.000Z","path":"2017/03/10/Mac工具/同时使用多个Git/","text":"如果你想在一台机器使用两个github账号（比如私人账号和工作用账号）。这个时候怎么指定push到哪个账号的test仓库上去呢 解决方案是两套key，再写个配置文件， 注意生成两个Key时，不要随便输入enter键就就不会覆盖掉老的两个key（假设你已经拥有私有账号且已经OK，现在想使用另一个工作用账号）： 12345678910111213141516171819202122232425262728293031323334353637381：为工作账号生成SSH Key$ ssh-keygen -t rsa -C &quot;your-email-address&quot;#存储key的时候，不要覆盖现有的id_rsa，在生成两个Key时，不要随便输入enter键就就不会覆盖掉老的两个key ,使用一个新的名字，比如id_rsa_work 2：把id_rsa_work.pub加到你的work账号上3：把该key加到ssh agent上。由于不是使用默认的.ssh/id_rsa，所以你需要显示告诉ssh agent你的新key的位置$ ssh-add ~/.ssh/id_rsa_work# 可以通过ssh-add -l来确认结果 4：配置.ssh/config$ vi .ssh/config# 加上以下内容#default githubHost github.com HostName github.com IdentityFile ~/.ssh/id_rsaHost github_work HostName github.com IdentityFile ~/.ssh/id_rsa_work 5：这样的话，你就可以通过使用github.com别名github_work来明确说你要是使用id_rsa_work的SSH key来连接github，即使用工作账号进行操作。#本地建库$ git init$ git commit -am &quot;first commit&apos; #push到github上去$ git remote add origin git@github_work:xxxx/test.git$ git push origin master","tags":[{"name":"Mac工具","slug":"Mac工具","permalink":"https://seawaylee.github.io/tags/Mac工具/"},{"name":"Git","slug":"Git","permalink":"https://seawaylee.github.io/tags/Git/"}]},{"title":"解决高并发环境下数据插入重复问题","date":"2017-03-09T16:11:04.000Z","path":"2017/03/10/高并发/ISTC_高并发数据重复问题/","text":"1.背景描述 应用框架: spring + SpringMVC + hibernate 数据库: Oracle11g 问题: 一家文学网站向我系统推多线程低并发推送数据，我这边观察日志和数据库，发现有一个作者被存储了2次到数据库中。按照程序的编写逻辑，重复的数据是会被判断出来不被存储的。 2.原因分析由于网络原因，客户可能连续推送了两条重复的数据，两条数据时间间隔非常小，因此导致了我们的 12345678910if(用户存在)&#123; xxxxx 存储用户到数据库&#125;else&#123; 重复推送，不采取任何措施&#125; 这个操作还没有执行完毕，第二条拥有相同数据的线程已经进入并通过了if的检验，导致数据库存储了两条相同的数据。后来我自己写了个100并发的多线程测试程序，发现100条相同数据中有40条被插入到了数据库里！天啦噜！！！因此确定了是多线程的并发导致了程序的判断逻辑失效。 3.解决思路：1) 在Author主表中对 身份证号 添加了唯一索引，现在Author主表不会出现重复数据了，如果连续推送客户会收到一条推送失败的提示信息。2) 但是AuthorOrg表(Author与AuthorOrg是一对多关系)依然会出现重复数据，想过添加siteId + userUniqueId的 联合唯一索引来解决，但是想到work表也会出现同样的问题，添加过多索引会导致DB占用空间无限增大，因此不采用。3) 考虑使用synchronized对方法添加同步锁，但是这样会导致其他正常数据的推送线程也被阻塞，影响效率。因此不采用。4) 使用对数据库添加行锁，实验发现还是会出现2条重复数据分析： 理论上的结果应该是1条成功，149条失败。 对数据库的select语句添加行锁必须作用于某条记录，但是第一次报送时，数据库中并没有这条数据，因此行锁根本没有加上，导致第二条数据成功异步使用select语句。 第一次报送成功以后，数据库中有了这条数据，select语句成功的对这条记录添加了行锁，所以后边不会出现重复数据。因此此法不可用。 5) 即想提高效率不对方法添加synchronized，又想保证数据准确性，最后使用synchronized(siteId + uid) 在Controller层加锁(保证了只有重复数据被加锁,在Controller使用的原因是因为事务会在Service调用完毕才被提交，我实验过在Service同步，150并发会出现2条重复数据，因为事务还没来得及提交) 测试结果：测试了3次150并发 不到一秒的时间全部返回，结果1条登记成功，149条返回该作者已登记。 4.提示这种加同步锁的方法在负载均衡下的多台应用服务器会失效！因为就算Spring保证了对象是单例的，但是多台服务器肯定是多个对象！因此synchronized将无效。解决方法是在数据库层对该对接公司的唯一记录加select锁，这样就能保证数据的不重复性，但是会降低该公司推送数据的效率(相当于逐条推送)，但是公司与公司之间还是并行推送的。还有一个方法就是将业务逻辑写入存储过程，然后对存储过程加锁，这种方法太麻烦了，需求有变动就必须去修改存储过程，但是效率要比前者高得多。","tags":[{"name":"高并发","slug":"高并发","permalink":"https://seawaylee.github.io/tags/高并发/"}]},{"title":"SpringInAction读书笔记(十三) Spring JMS","date":"2017-03-09T15:57:12.000Z","path":"2017/03/09/JavaWeb/Spring/Spring_13/","text":"1 maven配置123456789101112131415&lt;!--Spring ActiveMQ--&gt;&lt;spring-activemq.version&gt;5.14.3&lt;/spring-activemq.version&gt;&lt;activemq.version&gt;5.14.3&lt;/activemq.version&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-all&lt;/artifactId&gt; &lt;version&gt;$&#123;activemq.version&#125;&lt;/version&gt;&lt;/dependency&gt; 2 spring整合jms12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:jms=\"http://www.springframework.org/schema/jms\" xsi:schemaLocation=\"http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-3.2.xsd http://www.springframework.org/schema/jms http://www.springframework.org/schema/jms/spring-jms.xsd\"&gt; &lt;!--JMS连接工厂--&gt; &lt;bean id=\"connectionFactory\" class=\"org.apache.activemq.spring.ActiveMQConnectionFactory\"&gt; &lt;property name=\"brokerURL\" value=\"tcp://localhost:61616\"/&gt; &lt;property name=\"trustAllPackages\" value=\"true\"/&gt; &lt;/bean&gt; &lt;!--消息转换器--&gt; &lt;bean id=\"messageConverter\" class=\"org.springframework.jms.support.converter.SimpleMessageConverter\"/&gt; &lt;!--消息队列源--&gt; &lt;bean id=\"queue\" class=\"org.apache.activemq.command.ActiveMQQueue\"&gt; &lt;constructor-arg name=\"name\" value=\"user.queue\"/&gt; &lt;/bean&gt; &lt;!--JMS操作模板--&gt; &lt;bean id=\"jmsTemplate\" class=\"org.springframework.jms.core.JmsTemplate\"&gt; &lt;constructor-arg name=\"connectionFactory\" ref=\"connectionFactory\"/&gt; &lt;property name=\"messageConverter\" ref=\"messageConverter\"/&gt; &lt;property name=\"defaultDestination\" ref=\"queue\"/&gt; &lt;/bean&gt; &lt;!--异步消息处理器--&gt; &lt;bean id=\"userHandler\" class=\"data_persistent.jms.UserHandler\"/&gt; &lt;!--JMS异步监听器--&gt; &lt;jms:listener-container connection-factory=\"connectionFactory\"&gt; &lt;jms:listener destination=\"user.queue\" ref=\"userHandler\" method=\"handleUserAlert\"/&gt; &lt;/jms:listener-container&gt;&lt;/beans&gt; 3 发送消息、获取消息测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140@RunWith(SpringJUnit4ClassRunner.class)//@ContextConfiguration(classes = &#123;JMSConfig.class&#125;)@ContextConfiguration(value = \"classpath*:spring/applicationContext-jms.xml\")public class JMSTest&#123; @Autowired JmsTemplate jmsTemplate; @Autowired ActiveMQQueue queue; /** * 原生JMS发送消息 * @Author NikoBelic * @Date 03/02/2017 16:10 */ @Test public void sendMsg() &#123; ConnectionFactory cf = new ActiveMQConnectionFactory(\"tcp://localhost:61616\"); Connection conn = null; Session session = null; try &#123; conn = cf.createConnection(); session = conn.createSession(false, Session.AUTO_ACKNOWLEDGE); Destination destination = new ActiveMQQueue(\"user.queue\"); MessageProducer producer = session.createProducer(destination); TextMessage message = session.createTextMessage(); message.setText(\"Hello ActiveMq\"); producer.send(message); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (session != null) session.close(); if (conn != null) conn.close(); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 原生JMS接收消息 * @Author NikoBelic * @Date 03/02/2017 16:10 */ @Test public void getMsg() &#123; ConnectionFactory cf = new ActiveMQConnectionFactory(\"tcp://localhost:61616\"); Connection conn = null; Session session = null; try &#123; conn = cf.createConnection(); session = conn.createSession(false, Session.AUTO_ACKNOWLEDGE); Destination destination = new ActiveMQQueue(\"user.queue\"); MessageConsumer consumer = session.createConsumer(destination); Message message = consumer.receive(); TextMessage textMessage = (TextMessage) message; System.out.println(\"Got a message: \" + textMessage.getText()); conn.start(); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (session != null) session.close(); if (conn != null) conn.close(); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125; &#125; // ----------------------华丽的分割线---------------------- /** * 异步发送消息 * @Author NikoBelic * @Date 03/02/2017 18:13 */ @Test public void easySendMsg() &#123; UserObj userObj = new UserObj(1, \"NikoBleic\", \"12345\", \"None\"); // 方法一 //jmsTemplate.send(queue, session -&gt; &#123; // return session.createTextMessage(userObj.toString()); //&#125;); // 方法二 jmsTemplate.convertAndSend(userObj); &#125; /** * 同步接收消息 * @Author NikoBelic * @Date 03/02/2017 18:14 */ @Test public void easyGetMsg() throws JMSException &#123; while (true) &#123; try &#123; //TextMessage message = (TextMessage) jmsTemplate.receive(); //System.out.println(message.getText()); Message message = jmsTemplate.receive(); if (message instanceof ObjectMessage) &#123; System.out.println(\"ObjectMsg Type\"); System.out.println(((ObjectMessage) message).getObject()); &#125; else &#123; System.out.println(\"Other Type...\" + message.getJMSType()); &#125; &#125;catch (Exception e) &#123; System.out.println(e.getMessage()); &#125; &#125; &#125;&#125; 1异步消息监听器收到了一条消息:UserObj&#123;id=1, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125; 由于配置了异步消息监听器，使用测试方法时，jmsTemplate.receive()将会与异步消息监听器争抢消息，谁抢到不一定。看下面示例 123456789101112131415161718192021222324252627@Testpublic void easyGetMsg() throws JMSException&#123; while (true) &#123; try &#123; //TextMessage message = (TextMessage) jmsTemplate.receive(); //System.out.println(message.getText()); Message message = jmsTemplate.receive(); if (message instanceof ObjectMessage) &#123; System.out.println(\"JMS模板接收到了消息:\" + ((ObjectMessage) message).getObject()); System.out.println(\"\"); &#125; else &#123; System.out.println(\"Other Type...\" + message.getJMSType()); &#125; &#125;catch (Exception e) &#123; System.out.println(e.getMessage()); &#125; &#125;&#125; 执行后什么都不会输出，记控制台输出窗口为ConsoleA 12345678910111213141516@Test public void easySendMsg() &#123; for (int i = 0; i &lt; 100; i++) &#123; // 方法二 UserObj userObj = new UserObj(i, \"NikoBleic\", \"12345\", \"None\"); jmsTemplate.convertAndSend(userObj); &#125; // 方法一 //jmsTemplate.send(queue, session -&gt; &#123; // return session.createTextMessage(userObj.toString()); //&#125;); &#125; 执行以后ConsoleA会输出如下，可以看到异步消息监听器与template.receive发生了争抢，但是以下显示的不是全部数据，因为执行发送程序的时候，发送端也会初始化项目中的异步消息监听器，导致总共有2个异步消息监听器和1个template一起争抢消息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798异步消息监听器收到了一条消息:UserObj&#123;id=0, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=1, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=3, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=4, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=6, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=7, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=9, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=10, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=12, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=13, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=15, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=16, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=18, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=19, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=21, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=22, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=24, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=25, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=27, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=28, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=30, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=31, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=33, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=34, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=36, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=37, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=39, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=42, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=40, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=44, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=46, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=47, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=49, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=50, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=52, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=53, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=55, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=56, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=58, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=59, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=61, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=62, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=64, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=65, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=67, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=68, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=70, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=71, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=73, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=74, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=76, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=78, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=79, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=81, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=82, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=84, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=85, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=87, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=88, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=90, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=91, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=93, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=94, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=96, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=97, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;JMS模板接收到了消息:UserObj&#123;id=99, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125; 当前窗口会输出如下 12345678910111213141516171819202122232425262728293031323334异步消息监听器收到了一条消息:UserObj&#123;id=2, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=5, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=8, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=11, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=14, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=17, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=20, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=23, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=26, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=29, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=32, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=35, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=38, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=41, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=43, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=45, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=48, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=51, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=54, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=57, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=60, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=63, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=66, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=69, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=72, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=75, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=77, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=80, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=83, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=86, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=89, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=92, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=95, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125;异步消息监听器收到了一条消息:UserObj&#123;id=98, username=&apos;NikoBleic&apos;, password=&apos;12345&apos;, role=&apos;None&apos;&#125; 4 监控界面字符串消息 消息队列 对象序列化为json字符串","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"},{"name":"Spring","slug":"Spring","permalink":"https://seawaylee.github.io/tags/Spring/"}]},{"title":"SpringInAction读书笔记(十二) Spring Boot","date":"2017-03-09T15:53:27.000Z","path":"2017/03/09/JavaWeb/Spring/Spring_12/","text":"1 最简单的一个SpringBoot应用程序1.1 导入依赖12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;version&gt;1.4.1.RELEASE&lt;/version&gt;&lt;/dependency&gt; 1.2 编写控制器123456789101112131415161718package springboot.controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;/** * @author NikoBelic * @create 31/01/2017 00:48 */@RestControllerpublic class Hi&#123; @RequestMapping(\"/\") String Hi() &#123; return \"Hello SpringBoot\"; &#125;&#125; 1.3 编写SpringBoot启动函数12345678910111213141516171819package springboot;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.context.annotation.ComponentScan;/** * @author NikoBelic * @create 31/01/2017 00:51 */@ComponentScan@EnableAutoConfigurationpublic class App&#123; public static void main(String[] args) throws InterruptedException &#123; SpringApplication.run(App.class,args); &#125;&#125; 1.4 运行结果控制台输出中可以看到相关的默认配置 12345678910111213141516171819202122232425262728293031323334 . ____ _ __ _ _ /\\\\ / ___&apos;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\( ( )\\___ | &apos;_ | &apos;_| | &apos;_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) &apos; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v1.4.1.RELEASE)2017-01-31 00:56:37.331 INFO 79397 --- [ main] springboot.App : Starting App on localhost with PID 79397 (/Users/lixiwei-mac/Documents/IdeaProjects/WebTemplate/target/classes started by NikoBelic in /Users/lixiwei-mac/Documents/IdeaProjects/WebTemplate)2017-01-31 00:56:37.334 INFO 79397 --- [ main] springboot.App : No active profile set, falling back to default profiles: default2017-01-31 00:56:37.485 INFO 79397 --- [ main] ationConfigEmbeddedWebApplicationContext : Refreshing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@635eaaf1: startup date [Tue Jan 31 00:56:37 CST 2017]; root of context hierarchy2017-01-31 00:56:40.355 INFO 79397 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat initialized with port(s): 8080 (http)2017-01-31 00:56:40.372 INFO 79397 --- [ main] o.apache.catalina.core.StandardService : Starting service Tomcat2017-01-31 00:56:40.373 INFO 79397 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet Engine: Apache Tomcat/8.5.52017-01-31 00:56:40.496 INFO 79397 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext2017-01-31 00:56:40.496 INFO 79397 --- [ost-startStop-1] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 3024 ms2017-01-31 00:56:40.797 INFO 79397 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean : Mapping servlet: &apos;dispatcherServlet&apos; to [/]2017-01-31 00:56:40.809 INFO 79397 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: &apos;characterEncodingFilter&apos; to: [/*]2017-01-31 00:56:40.817 INFO 79397 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: &apos;hiddenHttpMethodFilter&apos; to: [/*]2017-01-31 00:56:40.818 INFO 79397 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: &apos;httpPutFormContentFilter&apos; to: [/*]2017-01-31 00:56:40.818 INFO 79397 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: &apos;requestContextFilter&apos; to: [/*]2017-01-31 00:56:41.400 INFO 79397 --- [ main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@635eaaf1: startup date [Tue Jan 31 00:56:37 CST 2017]; root of context hierarchy2017-01-31 00:56:41.511 INFO 79397 --- [ main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped &quot;&#123;[/]&#125;&quot; onto java.lang.String springboot.controller.Hi.Hi()2017-01-31 00:56:41.515 INFO 79397 --- [ main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped &quot;&#123;[/error]&#125;&quot; onto public org.springframework.http.ResponseEntity&lt;java.util.Map&lt;java.lang.String, java.lang.Object&gt;&gt; org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest)2017-01-31 00:56:41.516 INFO 79397 --- [ main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped &quot;&#123;[/error],produces=[text/html]&#125;&quot; onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)2017-01-31 00:56:41.563 INFO 79397 --- [ main] o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]2017-01-31 00:56:41.564 INFO 79397 --- [ main] o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]2017-01-31 00:56:41.643 INFO 79397 --- [ main] o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]2017-01-31 00:56:41.831 INFO 79397 --- [ main] o.s.j.e.a.AnnotationMBeanExporter : Registering beans for JMX exposure on startup2017-01-31 00:56:41.936 INFO 79397 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)2017-01-31 00:56:41.948 INFO 79397 --- [ main] springboot.App : Started App in 5.572 seconds (JVM running for 6.33)2017-01-31 00:56:48.830 INFO 79397 --- [nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring FrameworkServlet &apos;dispatcherServlet&apos;2017-01-31 00:56:48.830 INFO 79397 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : FrameworkServlet &apos;dispatcherServlet&apos;: initialization started2017-01-31 00:56:48.848 INFO 79397 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet : FrameworkServlet &apos;dispatcherServlet&apos;: initialization completed in 18 ms Web页面 有没有一键装机的感觉？ 2 SpringBoot简介SpringBoot提供了四个主要特性 SpringBootStarter：将常用依赖分组进行了整合。即一次性添加所需依赖到项目中。 自动配置：合理的推测应用所需的bean并自动配置他们。 命令行接口（Command-line interface，CLI）：结合自动配置进一步简化开发 Actuator：管理特性。 2.1 Starter依赖比如1中的示例程序，只需要导入一个starter依赖即可自动导入web应用所需的全部依赖。其他类型的starter依赖包官网有详细介绍。 2.2 自动配置之前构建SpringMVC应用程序时，需要配置视图解析器、处理器映射器、资源处理器，而现在全部由SpringBoot自动推测所需的配置完成自动配置。我们所需要做的只是将jsp等视图放到项目中并编写Controller，SpringBoot会自动嗅探到这些模板文件从而为我们配置相应的视图解析器，并自动配置支持SpringMVC的多个bean。 2.3 SpringBoot CLIspring run Hi.groovy目前不知道有啥用。。 2.4 Actuator 管理端点 合理的异常处理以及默认的”/error”映射端点 获取应用信息的”/info”端点 启用SpringSecurity时会有一个审计事件框架 3 使用SpringBoot构建应用3.1 maven配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.edu.ncut&lt;/groupId&gt; &lt;artifactId&gt;WebTemplate&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;WebTemplate Maven Webapp&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;!-- 配置parent以后,我们就不需要再管理其他依赖的版本号了,版本号会从parent中继承得到--&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.4.4.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;!-- 包含了SpringMVC的依赖 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 数据持久化 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.21&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- 日志 --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 试图引擎 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.thymeleaf&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-spring4&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- 这个Maven插件能够生成可执行的Jar文件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 3.2 Model1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package springboot.model;/** * @author NikoBelic * @create 31/01/2017 01:30 */public class Contact&#123; private Long id; private String firstName; private String lastName; private String phoneNumber; private String emailAddress; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public String getFirstName() &#123; return firstName; &#125; public void setFirstName(String firstName) &#123; this.firstName = firstName; &#125; public String getLastName() &#123; return lastName; &#125; public void setLastName(String lastName) &#123; this.lastName = lastName; &#125; public String getPhoneNumber() &#123; return phoneNumber; &#125; public void setPhoneNumber(String phoneNumber) &#123; this.phoneNumber = phoneNumber; &#125; public String getEmailAddress() &#123; return emailAddress; &#125; public void setEmailAddress(String emailAddress) &#123; this.emailAddress = emailAddress; &#125; @Override public String toString() &#123; return \"Contact&#123;\" + \"id=\" + id + \", firstName='\" + firstName + '\\'' + \", lastName='\" + lastName + '\\'' + \", phoneNumber='\" + phoneNumber + '\\'' + \", emailAddress='\" + emailAddress + '\\'' + '&#125;'; &#125;&#125; 3.3 Dao123456789101112131415161718192021222324252627282930313233343536373839package springboot.dao;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.stereotype.Repository;import springboot.model.Contact;import java.util.List;/** * @author NikoBelic * @create 31/01/2017 01:32 */@Repositorypublic class ContactDao&#123; @Autowired private JdbcTemplate jdbcTemplate; public List&lt;Contact&gt; findAll() &#123; return jdbcTemplate.query(\"select * from tb_contact\", (rs, rowNum) -&gt; &#123; Contact contact = new Contact(); contact.setId(rs.getLong(1)); contact.setFirstName(rs.getString(2)); contact.setLastName(rs.getString(3)); contact.setPhoneNumber(rs.getString(4)); contact.setEmailAddress(rs.getString(5)); return contact; &#125;); &#125; public void save(Contact contact) &#123; jdbcTemplate.update(\"insert into tb_contact(firstName,lastName,phoneNumber,emailAddress) values(?,?,?,?)\", contact.getFirstName(), contact.getLastName(), contact.getPhoneNumber(), contact.getEmailAddress()); &#125;&#125; 3.4 Controller123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package springboot.controller;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import springboot.dao.ContactDao;import springboot.model.Contact;import java.util.List;import java.util.Map;/** * @author NikoBelic * @create 31/01/2017 01:31 */@Controller@RequestMapping(value = \"/contact\")public class ContactController&#123; @Autowired private ContactDao contactDao; /** * 联系人主页面,Get处理/contact/,映射到home视图 * @Author NikoBelic * @Date 31/01/2017 01:36 */ @RequestMapping(method = RequestMethod.GET) public String home(Map&lt;String,Object&gt; model) &#123; List&lt;Contact&gt; contacts = contactDao.findAll(); model.put(\"contacts\",contacts); return \"home\"; &#125; /** * 添加联系人后跳转到主页 * @Author NikoBelic * @Date 31/01/2017 01:39 */ @RequestMapping(method = RequestMethod.POST) public String submit(Contact contact) &#123; contactDao.save(contact); return \"redirect:/contact\"; &#125;&#125; 3.5 Thymeleaf视图SpringBoot官方不建议使用jsp 12345678910111213141516171819202122232425262728293031&lt;!DOCTYPE html&gt;&lt;html xmlns:th=\"http://www.thymeleaf.org\"&gt; &lt;head&gt; &lt;title&gt;Spring Boot Contacts&lt;/title&gt; &lt;link rel=\"stylesheet\" th:href=\"@&#123;/style.css&#125;\" /&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;Spring Boot Contacts&lt;/h2&gt; &lt;form method=\"POST\"&gt; &lt;label for=\"firstName\"&gt;First Name:&lt;/label&gt; &lt;input type=\"text\" name=\"firstName\"&gt;&lt;/input&gt;&lt;br/&gt; &lt;label for=\"lastName\"&gt;Last Name:&lt;/label&gt; &lt;input type=\"text\" name=\"lastName\"&gt;&lt;/input&gt;&lt;br/&gt; &lt;label for=\"phoneNumber\"&gt;Phone #:&lt;/label&gt; &lt;input type=\"text\" name=\"phoneNumber\"&gt;&lt;/input&gt;&lt;br/&gt; &lt;label for=\"emailAddress\"&gt;Email:&lt;/label&gt; &lt;input type=\"text\" name=\"emailAddress\"&gt;&lt;/input&gt;&lt;br/&gt; &lt;input type=\"submit\"&gt;&lt;/input&gt; &lt;/form&gt; &lt;ul th:each=\"contact : $&#123;contacts&#125;\"&gt; &lt;li&gt; &lt;span th:text=\"$&#123;contact.firstName&#125;\"&gt;First&lt;/span&gt; &lt;span th:text=\"$&#123;contact.lastName&#125;\"&gt;Last&lt;/span&gt; : &lt;span th:text=\"$&#123;contact.phoneNumber&#125;\"&gt;phoneNumber&lt;/span&gt;, &lt;span th:text=\"$&#123;contact.emailAddress&#125;\"&gt;emailAddress&lt;/span&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/body&gt;&lt;/html&gt; 3.6 测试 3.7 目录结构 4 启动Actuator了解应用内部状况1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-actuator&lt;/artifactId&gt; &lt;/dependency&gt; 添加依赖后，Get访问 /autoconfig 来查看使用自动配置时锁做出的决策；访问/beans查看应用所配置的bean等等。","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"},{"name":"Spring","slug":"Spring","permalink":"https://seawaylee.github.io/tags/Spring/"}]},{"title":"SpringInAction读书笔记(十一) 远程调用","date":"2017-03-09T15:51:58.000Z","path":"2017/03/09/JavaWeb/Spring/Spring_11/","text":"Spring 可以与RMI、Hessian、Burlap、HTTP invoker 等远程调用整合，但是都太麻烦了，在实际中应用很少，本文主要介绍如何使用Spring与WebService整合实现远程调用。几个名词: SOAP:简单对象访问协议 WSDL：WebService描述语言 JaxWs配置1234567891011121314151617181920212223package data_persistent.config;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.remoting.jaxws.SimpleJaxWsServiceExporter;/** * JAX-WS配置 * @author NikoBelic * @create 27/01/2017 16:33 */@Configurationpublic class RMIConfig&#123; @Bean public SimpleJaxWsServiceExporter jaxWsServiceExporter() &#123; System.out.println(\"RMI配置文件初始化\"); SimpleJaxWsServiceExporter exporter = new SimpleJaxWsServiceExporter(); exporter.setBaseAddress(\"http://localhost:8088/services/\"); return exporter; &#125;&#125; WebService层123456789101112131415161718192021222324252627282930313233343536373839404142package data_persistent.webservice;import data_persistent.dao.UserDao;import data_persistent.model.UserObj;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import javax.jws.WebMethod;import javax.jws.WebService;/** * @author NikoBelic * @create 25/01/2017 22:44 */@Component@WebService(serviceName = \"userService\")public class UserWebService&#123; @Autowired private UserDao userDao; //@WebMethod public UserObj addUser(UserObj userObj) &#123; return userDao.addUser(userObj); &#125; //@WebMethod public UserObj findUserById(Integer id) &#123; return userDao.findUserById(id); &#125; //@WebMethod public void updateUserById(Integer id, UserObj userObj) &#123; userDao.updateUserById(id, userObj); &#125; //@WebMethod public void deleteUserById(Integer id) &#123; userDao.deleteUserById(id); &#125;&#125; 发布结果访问http://localhost:8088/services/userService?wsdl 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100This XML file does not appear to have any style information associated with it. The document tree is shown below.&lt;!-- Published by JAX-WS RI (http://jax-ws.java.net). RI's version is JAX-WS RI 2.2.9-b130926.1035 svn-revision#5f6196f2b90e9460065a4c2f4e30e065b245e51e. --&gt;&lt;!-- Generated by JAX-WS RI (http://jax-ws.java.net). RI's version is JAX-WS RI 2.2.9-b130926.1035 svn-revision#5f6196f2b90e9460065a4c2f4e30e065b245e51e. --&gt;&lt;definitions xmlns:wsu=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd\" xmlns:wsp=\"http://www.w3.org/ns/ws-policy\" xmlns:wsp1_2=\"http://schemas.xmlsoap.org/ws/2004/09/policy\" xmlns:wsam=\"http://www.w3.org/2007/05/addressing/metadata\" xmlns:soap=\"http://schemas.xmlsoap.org/wsdl/soap/\" xmlns:tns=\"http://webservice.data_persistent/\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns=\"http://schemas.xmlsoap.org/wsdl/\" targetNamespace=\"http://webservice.data_persistent/\" name=\"userService\"&gt;&lt;types&gt;&lt;xsd:schema&gt;&lt;xsd:import namespace=\"http://webservice.data_persistent/\" schemaLocation=\"http://localhost:8088/services/userService?xsd=1\"/&gt;&lt;/xsd:schema&gt;&lt;/types&gt;&lt;message name=\"addUser\"&gt;&lt;part name=\"parameters\" element=\"tns:addUser\"/&gt;&lt;/message&gt;&lt;message name=\"addUserResponse\"&gt;&lt;part name=\"parameters\" element=\"tns:addUserResponse\"/&gt;&lt;/message&gt;&lt;message name=\"findUserById\"&gt;&lt;part name=\"parameters\" element=\"tns:findUserById\"/&gt;&lt;/message&gt;&lt;message name=\"findUserByIdResponse\"&gt;&lt;part name=\"parameters\" element=\"tns:findUserByIdResponse\"/&gt;&lt;/message&gt;&lt;message name=\"updateUserById\"&gt;&lt;part name=\"parameters\" element=\"tns:updateUserById\"/&gt;&lt;/message&gt;&lt;message name=\"updateUserByIdResponse\"&gt;&lt;part name=\"parameters\" element=\"tns:updateUserByIdResponse\"/&gt;&lt;/message&gt;&lt;message name=\"deleteUserById\"&gt;&lt;part name=\"parameters\" element=\"tns:deleteUserById\"/&gt;&lt;/message&gt;&lt;message name=\"deleteUserByIdResponse\"&gt;&lt;part name=\"parameters\" element=\"tns:deleteUserByIdResponse\"/&gt;&lt;/message&gt;&lt;portType name=\"UserWebService\"&gt;&lt;operation name=\"addUser\"&gt;&lt;input wsam:Action=\"http://webservice.data_persistent/UserWebService/addUserRequest\" message=\"tns:addUser\"/&lt;output wsam:Action=\"http://webservice.data_persistent/UserWebService/addUserResponse\" message=\"tns:addUserResponse\"/&gt;&lt;/operation&gt;&lt;operation name=\"findUserById\"&gt;&lt;input wsam:Action=\"http://webservice.data_persistent/UserWebService/findUserByIdRequest\" message=\"tns:findUserById\"/&gt;&lt;output wsam:Action=\"http://webservice.data_persistent/UserWebService/findUserByIdResponse\" message=\"tns:findUserByIdResponse\"/&gt;&lt;/operation&gt;&lt;operation name=\"updateUserById\"&gt;&lt;input wsam:Action=\"http://webservice.data_persistent/UserWebService/updateUserByIdRequest\" message=\"tns:updateUserById\"/&gt;&lt;output wsam:Action=\"http://webservice.data_persistent/UserWebService/updateUserByIdResponse\" message=\"tns:updateUserByIdResponse\"/&gt;&lt;/operation&gt;&lt;operation name=\"deleteUserById\"&gt;&lt;input wsam:Action=\"http://webservice.data_persistent/UserWebService/deleteUserByIdRequest\" message=\"tns:deleteUserById\"/&gt;&lt;output wsam:Action=\"http://webservice.data_persistent/UserWebService/deleteUserByIdResponse\" message=\"tns:deleteUserByIdResponse\"/&gt;&lt;/operation&gt;&lt;/portType&gt;&lt;binding name=\"UserWebServicePortBinding\" type=\"tns:UserWebService\"&gt;&lt;soap:binding transport=\"http://schemas.xmlsoap.org/soap/http\" style=\"document\"/&gt;&lt;operation name=\"addUser\"&gt;&lt;soap:operation soapAction=\"\"/&gt;&lt;input&gt;&lt;soap:body use=\"literal\"/&gt;&lt;/input&gt;&lt;output&gt;&lt;soap:body use=\"literal\"/&gt;&lt;/output&gt;&lt;/operation&gt;&lt;operation name=\"findUserById\"&gt;&lt;soap:operation soapAction=\"\"/&gt;&lt;input&gt;&lt;soap:body use=\"literal\"/&gt;&lt;/input&gt;&lt;output&gt;&lt;soap:body use=\"literal\"/&gt;&lt;/output&gt;&lt;/operation&gt;&lt;operation name=\"updateUserById\"&gt;&lt;soap:operation soapAction=\"\"/&gt;&lt;input&gt;&lt;soap:body use=\"literal\"/&gt;&lt;/input&gt;&lt;output&gt;&lt;soap:body use=\"literal\"/&gt;&lt;/output&gt;&lt;/operation&gt;&lt;operation name=\"deleteUserById\"&gt;&lt;soap:operation soapAction=\"\"/&gt;&lt;input&gt;&lt;soap:body use=\"literal\"/&gt;&lt;/input&gt;&lt;output&gt;&lt;soap:body use=\"literal\"/&gt;&lt;/output&gt;&lt;/operation&gt;&lt;/binding&gt;&lt;service name=\"userService\"&gt;&lt;port name=\"UserWebServicePort\" binding=\"tns:UserWebServicePortBinding\"&gt;&lt;soap:address location=\"http://localhost:8088/services/userService\"/&gt;&lt;/port&gt;&lt;/service&gt;&lt;/definitions&gt;","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"},{"name":"Spring","slug":"Spring","permalink":"https://seawaylee.github.io/tags/Spring/"}]},{"title":"SpringInAction读书笔记(十)  Spring Cache","date":"2017-03-09T15:50:48.000Z","path":"2017/03/09/JavaWeb/Spring/Spring_10/","text":"缓存对于某些不要求实时获取最新数据的请求非常好用，如果再高并发环境下，数据库成为系统的性能瓶颈，使用缓存能够大幅度提升系统性能。本文以Redis作为缓存容器，结合Spring来模拟一个缓存系统。个人认为，如果你会使用Redis，则完全没有必要将其与Spring整合来实现缓存，自己使用Jedis工具来实现缓存更加灵活。但是如果你不懂Redis，那么使用SpringCache+Redis就可以了。 13.1 Redis配置文件读取配置123456789@Configuration@PropertySource(\"classpath:redis.properties\")public class PropertyConfig&#123; @Bean public static PropertySourcesPlaceholderConfigurer propertySourcesPlaceholderConfigurer() &#123; return new PropertySourcesPlaceholderConfigurer(); &#125;&#125; 13.2 SpringCache配置类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102package data_persistent.config;import org.springframework.beans.factory.annotation.Value;import org.springframework.cache.CacheManager;import org.springframework.cache.annotation.EnableCaching;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.cache.RedisCacheManager;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.data.redis.connection.jedis.JedisConnectionFactory;import org.springframework.data.redis.core.RedisTemplate;import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool;import redis.clients.jedis.JedisPoolConfig;/** * 缓存配置 * @author NikoBelic * @create 24/01/2017 15:47 */@EnableCaching@Configurationpublic class CacheConfig&#123; @Value(value = \"$&#123;host&#125;\") private String host; @Value(value = \"$&#123;port&#125;\") private Integer port; @Value(value = \"$&#123;keynum&#125;\") private Integer keynum; /** * 注入CacheManager * @Author NikoBelic * @Date 24/01/2017 16:05 */ @Bean public CacheManager cacheManager(RedisTemplate redisTemplate) &#123; RedisCacheManager cacheManager = new RedisCacheManager(redisTemplate); cacheManager.setDefaultExpiration(10); return cacheManager; &#125; /** * 注入JedisConnectionFactory * @Author NikoBelic * @Date 24/01/2017 16:03 */ @Bean public JedisConnectionFactory redisConnectionFactory() &#123; JedisConnectionFactory factory = new JedisConnectionFactory(); factory.setHostName(host); factory.setPort(port); factory.setDatabase(keynum); //factory.afterPropertiesSet(); return factory; &#125; /** * 注入RedisTemplate * @Author NikoBelic * @Date 24/01/2017 16:02 */ @Bean public RedisTemplate&lt;String,String&gt; redisTemplate(RedisConnectionFactory redisCF) &#123; RedisTemplate&lt;String,String&gt; redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(redisCF); redisTemplate.afterPropertiesSet(); return redisTemplate; &#125; // *********************** Jedis 配置 *************************** // 以下配置与缓存无关,在本例中可有可无 @Bean public JedisPoolConfig jedisPoolConfig() &#123; JedisPoolConfig jedisPoolConfig = new JedisPoolConfig(); jedisPoolConfig.setMaxWaitMillis(60 * 1000); jedisPoolConfig.setMaxTotal(1000); jedisPoolConfig.setMaxIdle(100); return jedisPoolConfig; &#125; @Bean public JedisPool jedisPool(JedisPoolConfig jedisPoolConfig) &#123; JedisPool jedisPool = new JedisPool(jedisPoolConfig,host,port,999999999); return jedisPool; &#125; @Bean public Jedis jedis(JedisPool jedisPool) &#123; Jedis jedis = jedisPool.getResource(); jedis.select(keynum); return jedis; &#125;&#125; 13.3 Dao层使用注解将结果缓存12345678910111213public interface UserDao&#123; @CachePut(value = \"UserCaching\",key = \"#result.id\") UserObj addUser(UserObj userObj); @Cacheable(\"UserCaching\") UserObj findUserById(Integer id); void updateUserById(Integer id,UserObj userObj); @CacheEvict(\"UserCaaching\") void deleteUserById(Integer id);&#125; 实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package data_persistent.dao;import data_persistent.model.UserObj;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jdbc.core.JdbcOperations;import org.springframework.jdbc.core.PreparedStatementCreator;import org.springframework.jdbc.support.GeneratedKeyHolder;import org.springframework.jdbc.support.KeyHolder;import org.springframework.stereotype.Repository;import javax.inject.Inject;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.SQLException;import java.sql.Statement;/** * @author NikoBelic * @create 22/01/2017 16:01 */@Repositorypublic class UserDaoImpl implements UserDao&#123; @Autowired private JdbcOperations jdbcOperations; /** * 使用JDBCTemplate添加数据 * * @Author NikoBelic * @Date 22/01/2017 16:25 */ public UserObj addUser(UserObj user) &#123; KeyHolder holder = new GeneratedKeyHolder(); int id = jdbcOperations.update(connection -&gt; &#123; PreparedStatement ps = connection.prepareStatement(\"insert into tb_user(username,password,role) values(?,?,?)\", Statement.RETURN_GENERATED_KEYS); ps.setString(1, user.getUsername()); ps.setString(2, user.getPassword()); ps.setString(3, user.getRole()); return ps; &#125;, holder); user.setId(holder.getKey().intValue()); return user; &#125; public UserObj findUserById(Integer id) &#123; System.out.println(\"findUserById si called...\"); return jdbcOperations.queryForObject(\"select * from tb_user t where t.id = ?\", (rs, rowNum) -&gt; &#123; return new UserObj(rs.getInt(\"id\"), rs.getString(\"username\"), rs.getString(\"password\"), rs.getString(\"role\")); &#125;, id); &#125; public void updateUserById(Integer id, UserObj userObj) &#123; jdbcOperations.update(\"update tb_user t set t.username = ? ,t.password = ?,t.role = ? where t.id = ?\", userObj.getUsername(), userObj.getPassword(), userObj.getRole(), id); &#125; @Override public void deleteUserById(Integer id) &#123; //jdbcOperations.update(\"delete from tb_user where id = ?\",id); System.out.println(\"删除数据库中的用户\" + id); &#125;&#125; 10.4 测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = RootConfig.class)public class PersistentTest&#123; @Autowired private UserDao userDao; @Autowired MongoOperations mongo; @Autowired CompanyRepo companyRepo; //@Autowired //Jedis jedis; @Autowired KVDao kvDao; // ***************** JDBCTemplate *******************/ @Test public void addUser() &#123; UserObj user = new UserObj(); user.setUsername(\"蛤蟆皮\"); user.setPassword(\"123456\"); user.setRole(\"ADMIN\"); System.out.println(userDao.addUser(user)); &#125; @Test public void findUser() throws InterruptedException &#123; System.out.println(userDao.findUserById(12)); Thread.sleep(11 * 1000); System.out.println(userDao.findUserById(12)); &#125; @Test public void updateUser() &#123; UserObj userObj = new UserObj(1,\"阿西吧\",\"password\",\"ABC\"); userDao.updateUserById(1,userObj); &#125; // ***************** Redis Caching *******************/ @Test public void redis_conn() &#123; System.out.println(kvDao.get(\"chinese\")); kvDao.set(\"\", \"\"); System.out.println(kvDao.get(\"chinese\")); &#125; @Test public void test_cache() &#123; System.out.println(\"Before Update...\"); System.out.println(userDao.findUserById(1)); UserObj userObj = new UserObj(1,\"Hey\",\"密码\",\"管理员\"); userDao.updateUserById(userObj.getId(),userObj); System.out.println(\"After Update...\"); System.out.println(userDao.findUserById(1)); &#125; @Test public void test_del_cache() &#123; UserObj userObj = new UserObj(9,\"Cache\",\"密码\",\"管理员\"); userObj = userDao.addUser(userObj); System.out.println(userDao.findUserById(userObj.getId())); userDao.deleteUserById(userObj.getId()); System.out.println(userDao.findUserById(userObj.getId())); &#125; 10.5 Redis数据库缓存结果1234127.0.0.1:6379[4]&gt; keys *1) &quot;\\xac\\xed\\x00\\x05sr\\x00\\x11java.lang.Integer\\x12\\xe2\\xa0\\xa4\\xf7\\x81\\x878\\x02\\x00\\x01I\\x00\\x05valuexr\\x00\\x10java.lang.Number\\x86\\xac\\x95\\x1d\\x0b\\x94\\xe0\\x8b\\x02\\x00\\x00xp\\x00\\x00\\x00\\x0b&quot;2) &quot;\\xac\\xed\\x00\\x05sr\\x00\\x11java.lang.Integer\\x12\\xe2\\xa0\\xa4\\xf7\\x81\\x878\\x02\\x00\\x01I\\x00\\x05valuexr\\x00\\x10java.lang.Number\\x86\\xac\\x95\\x1d\\x0b\\x94\\xe0\\x8b\\x02\\x00\\x00xp\\x00\\x00\\x00\\b&quot;3) &quot;UserCaching~keys&quot; UserCaching~keys 是一个ZSet结构，是一个有序集合。 10.6 注解 注解 描述 @Cacheable 表明Spring在调用方法之前，首先应该在缓存中查找方法的返回值，如果这个值能够找到，就会返回缓存的值，否则的话，这个方法就会被调用，返回值会放到缓存之中。 @CachePut 表明Spring应该将方法的返回值放到缓存中，在方法的调用前并不会检查缓存，方法始终都会被调用。 @CacheEvict 表明Spring应该在缓存中清楚一个或多个条目 @Caching 这事一个分组的注解，能够同时应用多个其他的缓存注解","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"},{"name":"Spring","slug":"Spring","permalink":"https://seawaylee.github.io/tags/Spring/"}]},{"title":"SpringInAction读书笔记(九) Spring Data","date":"2017-03-09T15:49:13.000Z","path":"2017/03/09/JavaWeb/Spring/Spring_9/","text":"本章将介绍如何使用Spring的JDBCTemplate、如何使用SpringData整合MongoDB、如何使用SpringData在运行时自动生成Repository。 10.1 配置文件类全局配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package data_persistent.config;import org.springframework.web.servlet.support.AbstractAnnotationConfigDispatcherServletInitializer;/** * @author NikoBelic * @create 22/01/2017 15:46 */public class WebInitializer extends AbstractAnnotationConfigDispatcherServletInitializer&#123; public WebInitializer() &#123; System.out.println(\"全局配置文件初始化\"); &#125; /** * 指定ContextLoaderListener配置类 * @Author NikoBelic * @Date 30/12/2016 10:48 */ @Override protected Class&lt;?&gt;[] getRootConfigClasses() &#123; return new Class&lt;?&gt;[]&#123;RootConfig.class&#125;; &#125; /** * 指定SpringMVC配置类 * @Author NikoBelic * @Date 30/12/2016 10:49 */ @Override protected Class&lt;?&gt;[] getServletConfigClasses() &#123; return new Class&lt;?&gt;[]&#123;WebConfig.class&#125;; &#125; /** * 将DispatcherServlet映射到 \"/\" * @Author * @Date 30/12/2016 10:44 */ @Override protected String[] getServletMappings() &#123; return new String[]&#123;\"/\"&#125;; &#125;&#125; Spring上下文配置、数据源配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package data_persistent.config;import com.mongodb.Mongo;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.FilterType;import org.springframework.data.mongodb.core.MongoFactoryBean;import org.springframework.data.mongodb.core.MongoOperations;import org.springframework.data.mongodb.core.MongoTemplate;import org.springframework.data.mongodb.repository.config.EnableMongoRepositories;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.jdbc.datasource.DriverManagerDataSource;import org.springframework.web.servlet.config.annotation.EnableWebMvc;import javax.sql.DataSource;/** * Spring上下文配置类 * 由ContextLoaderListener加载的bean * @author NikoBelic * @create 09/01/2017 20:30 */@ComponentScan(basePackages = \"data_persistent\",excludeFilters = &#123;@ComponentScan.Filter(type = FilterType.ANNOTATION,value = EnableWebMvc.class)&#125;)@Configuration@EnableMongoRepositories(\"data_persistent.mongodao\") // 开启SpringData自动生成MongoRepository功能public class RootConfig&#123; public RootConfig() &#123; System.out.println(\"RootConfig初始化\"); &#125; /** * 注入Mysql数据源 * @Author NikoBelic * @Date 23/01/2017 11:31 */ @Bean public DataSource dataSource() &#123; DriverManagerDataSource ds = new DriverManagerDataSource(); ds.setDriverClassName(\"com.mysql.jdbc.Driver\"); ds.setUrl(\"jdbc:mysql://localhost:3306/dmes?characterEncoding=UTF-8\"); ds.setUsername(\"root\"); ds.setPassword(\"lxw1993822\"); return ds; &#125; /** * 注入JdbcTemplate * @Author NikoBelic * @Date 23/01/2017 11:30 */ @Bean public JdbcTemplate jdbcTemplate(DataSource dataSource) &#123; return new JdbcTemplate(dataSource); &#125; /** * 注入MongoFactory * @Author NikoBelic * @Date 23/01/2017 11:30 */ @Bean public MongoFactoryBean mongo() &#123; MongoFactoryBean mongo = new MongoFactoryBean(); mongo.setHost(\"localhost\"); return mongo; &#125; /** * 注入MongoTempldate * MongoOperations是一个借口,MongoTemplate实现了它 * @Author NikoBelic * @Date 23/01/2017 11:30 */ @Bean public MongoOperations mongoTemplate(Mongo mongo) &#123; return new MongoTemplate(mongo,\"spring_in_action\"); &#125;&#125; SpringMVC配置(未使用) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package data_persistent.config;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;import org.springframework.web.multipart.MultipartResolver;import org.springframework.web.multipart.support.StandardServletMultipartResolver;import org.springframework.web.servlet.ViewResolver;import org.springframework.web.servlet.config.annotation.DefaultServletHandlerConfigurer;import org.springframework.web.servlet.config.annotation.EnableWebMvc;import org.springframework.web.servlet.config.annotation.WebMvcConfigurerAdapter;import org.springframework.web.servlet.view.InternalResourceViewResolver;import org.springframework.web.servlet.view.JstlView;import java.io.IOException;/** * @author NikoBelic * @create 09/01/2017 20:35 */@EnableWebMvc@ComponentScan(\"data_persistent.controller\")@Configurationpublic class WebConfig extends WebMvcConfigurerAdapter&#123; public WebConfig() &#123; System.out.println(\"SpringMVC配置初始化\"); &#125; /** * 配置视图解析器 * @Author NikoBelic * @Date 09/01/2017 20:37 */ @Bean public ViewResolver viewResolver() &#123; InternalResourceViewResolver resolver = new InternalResourceViewResolver(); resolver.setPrefix(\"/WEB-INF/views/\"); resolver.setSuffix(\".jsp\"); resolver.setExposeContextBeansAsAttributes(true); resolver.setViewClass(JstlView.class); return resolver; &#125; /** * 配置静态资源的处理 * @Author NikoBelic * @Date 09/01/2017 20:37 */ @Override public void configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer) &#123; configurer.enable(); &#125; /** * 配置文件上传解析器 * @Author NikoBelic * @Date 09/01/2017 20:38 */ @Bean public MultipartResolver multipartResolver() throws IOException &#123; return new StandardServletMultipartResolver(); &#125;&#125; 10.2 Sprng + JdbcTemplateModel 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package data_persistent.model;/** * @author NikoBelic * @create 22/01/2017 15:58 */public class UserObj&#123; private Integer id; private String username; private String password; private String role; public UserObj() &#123; &#125; public UserObj(int id, String username, String password) &#123; this.id = id; this.username = username; this.password = password; &#125; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; public String getRole() &#123; return role; &#125; public void setRole(String role) &#123; this.role = role; &#125; @Override public String toString() &#123; return \"UserObj&#123;\" + \"id=\" + id + \", username='\" + username + '\\'' + \", password='\" + password + '\\'' + \", role='\" + role + '\\'' + '&#125;'; &#125;&#125; Dao 12345678910111213package data_persistent.dao;import data_persistent.model.UserObj;/** * @author NikoBelic * @create 22/01/2017 15:59 */public interface UserDao&#123; void addUser(); UserObj findUserById(Integer id);&#125; DaoImpl 12345678910111213141516171819202122232425262728293031323334353637383940414243package data_persistent.dao;import data_persistent.model.UserObj;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jdbc.core.JdbcOperations;import org.springframework.stereotype.Repository;import javax.inject.Inject;/** * @author NikoBelic * @create 22/01/2017 16:01 */@Repositorypublic class UserDaoImpl implements UserDao&#123; @Autowired private JdbcOperations jdbcOperations; /** * 使用JDBCTemplate添加数据 * @Author NikoBelic * @Date 22/01/2017 16:25 */ public void addUser() &#123; UserObj user = new UserObj(); user.setUsername(\"NikoBelic\"); user.setPassword(\"123456\"); user.setRole(\"ROLE_UNKNOWN\"); jdbcOperations.update(\"insert into tb_user(username,password,role) values(?,?,?)\", user.getUsername(), user.getPassword(), user.getRole()); &#125; public UserObj findUserById(Integer id) &#123; return jdbcOperations.queryForObject(\"select * from tb_user t where t.id = ?\" , (rs,rowNum) -&gt;&#123; return new UserObj(rs.getInt(\"id\"),rs.getString(\"username\"),rs.getString(\"password\")); &#125;,id); &#125;&#125; 10.3 SpringData + MongoDB文档类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package data_persistent.document;import org.springframework.data.annotation.Id;import org.springframework.data.mongodb.core.mapping.Document;import java.util.Collection;import java.util.HashSet;/** * @author NikoBelic * @create 22/01/2017 17:38 */@Document(collection = \"companys\")public class CompanyDoc&#123; @Id private Integer cid; private String companyName; private String location; private String financing; Collection&lt;MemberDoc&gt; members = new HashSet&lt;&gt;(); public Integer getCid() &#123; return cid; &#125; public void setCid(Integer cid) &#123; this.cid = cid; &#125; public String getCompanyName() &#123; return companyName; &#125; public void setCompanyName(String companyName) &#123; this.companyName = companyName; &#125; public String getLocation() &#123; return location; &#125; public void setLocation(String location) &#123; this.location = location; &#125; public String getFinancing() &#123; return financing; &#125; public void setFinancing(String financing) &#123; this.financing = financing; &#125; public Collection&lt;MemberDoc&gt; getMembers() &#123; return members; &#125; public void setMembers(Collection&lt;MemberDoc&gt; members) &#123; this.members = members; &#125; @Override public String toString() &#123; return \"CompanyDoc&#123;\" + \"cid=\" + cid + \", companyName='\" + companyName + '\\'' + \", location='\" + location + '\\'' + \", financing='\" + financing + '\\'' + \", members=\" + members + '&#125;'; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package data_persistent.document;/** * @author NikoBelic * @create 22/01/2017 18:00 */public class MemberDoc&#123; private Integer mid; private String name; private String age; private String job; public MemberDoc(Integer mid, String name, String age, String job) &#123; this.mid = mid; this.name = name; this.age = age; this.job = job; &#125; public Integer getMid() &#123; return mid; &#125; public void setMid(Integer mid) &#123; this.mid = mid; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getAge() &#123; return age; &#125; public void setAge(String age) &#123; this.age = age; &#125; public String getJob() &#123; return job; &#125; public void setJob(String job) &#123; this.job = job; &#125; @Override public String toString() &#123; return \"MemberDoc&#123;\" + \"mid='\" + mid + '\\'' + \", name='\" + name + '\\'' + \", age='\" + age + '\\'' + \", job='\" + job + '\\'' + '&#125;'; &#125;&#125; Mongo的持久化采用SpringData根据函数名自动生成Repository、使用Query自定义查询。 12345678910111213141516171819package data_persistent.mongodao;import data_persistent.document.CompanyDoc;import org.springframework.data.mongodb.repository.MongoRepository;import org.springframework.data.mongodb.repository.Query;import java.util.List;/** * @author NikoBelic * @create 23/01/2017 12:57 */public interface CompanyRepo extends MongoRepository&lt;CompanyDoc,Integer&gt;&#123; List&lt;CompanyDoc&gt; findByLocationLike(String location); @Query(\"&#123;'companyName':'犀牛科技'&#125;\") CompanyDoc findWithQuery();&#125; 10.4 测试类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package data_persistent.test;import com.mongodb.Mongo;import data_persistent.config.RootConfig;import data_persistent.dao.UserDao;import data_persistent.document.CompanyDoc;import data_persistent.document.MemberDoc;import data_persistent.mongodao.CompanyRepo;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.mongodb.core.MongoOperations;import org.springframework.data.mongodb.core.query.Criteria;import org.springframework.data.mongodb.core.query.Query;import org.springframework.data.mongodb.repository.MongoRepository;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import java.util.Collection;import java.util.HashSet;import java.util.List;/** * @author NikoBelic * @create 22/01/2017 16:07 */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = RootConfig.class)public class PersistentTest&#123; @Autowired private UserDao userDao; @Autowired MongoOperations mongo; @Autowired CompanyRepo companyRepo; // ***************** JDBCTemplate *******************/ @Test public void addUser() &#123; userDao.addUser(); &#125; @Test public void findUser() &#123; System.out.println(userDao.findUserById(1)); &#125; // ***************** MongoDB *******************/ @Test public void mongo_add() &#123; CompanyDoc company = new CompanyDoc(); company.setCid(1); company.setCompanyName(\"犀牛科技\"); company.setLocation(\"北京工业大学\"); company.setFinancing(\"天使轮\"); Collection&lt;MemberDoc&gt; members = new HashSet&lt;&gt;(); for (int i = 0; i &lt; 3; i++) &#123; members.add(new MemberDoc(i+1,\"员工\",(18+i) + \"\",\"码农\" + (char)(97 + i))); &#125; company.setMembers(members); mongo.insert(company,\"companys\"); &#125; @Test public void mongo_findAll() &#123; //List&lt;CompanyDoc&gt; cList = mongo.findAll(CompanyDoc.class,\"companys\"); //List&lt;CompanyDoc&gt; cList = companyRepo.findAll(); //List&lt;CompanyDoc&gt; cList = companyRepo.findByLocationLike(\"北京\"); CompanyDoc c = companyRepo.findWithQuery(); System.out.println(c); //for (CompanyDoc companyDoc : cList) //&#123; // System.out.println(companyDoc); //&#125; &#125; @Test public void mongo_advance() &#123; List&lt;CompanyDoc&gt; companyDocs = mongo.find(Query.query(Criteria.where(\"companyName\").regex(\"^犀牛*\")), CompanyDoc.class, \"companys\"); System.out.println(companyDocs); &#125;&#125;","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"},{"name":"Spring","slug":"Spring","permalink":"https://seawaylee.github.io/tags/Spring/"}]},{"title":"SpringInAction读书笔记(八) Spring Security","date":"2017-03-09T15:45:34.000Z","path":"2017/03/09/JavaWeb/Spring/Spring_8/","text":"9.1 Spring Security简介Spring Security 是为基于Spring的应用程序提供声明式安全保护的安全性框架。SpringSecurity充分利用了DI和AOP技术。本章将使用SpringSecurity保证Web安全性。 9.1.1 SpringSecurity模块SpringSecurity被分成了11个模块 ACL AccessControlList访问控制列表为域对象提供安全性 切面 当使用SpringSecurity注解时，会使用基于AspectJ的且米娜，而不是使用标准的SpringAOP CAS客户端 提供与Jasig的中心认证服务（Central Authentication Service）进行集成的功能 配置 包含通过XML和Java配置SpringSecurity的功能支持 核心 提供SpringSecurity基本库 加密 提供了加密和密码编码的功能 LDAP 支持基于LDAP进行认证 OpenID 支持使用OpenID进行集中式认证 Remoting 提供了对SpringRemoting的支持 标签库 SpringSecurity的JSP标签库 Web 提供了SpringSecurity基于Filter的Web安全性支持 其中Core和Configuration模块必须包含在应用程序的类路径下。一般为了保护web应用我们还需要Web模块。 9.1.2 过滤Web请求SpringSecurity借助一系列ServletFilter来提供各种安全性功能。DelegatingFilterProxy是一个特殊的ServletFilter。它将工作委托给javax.servlet.Filter实现类，这个实现类作为一个注册在Spring应用上下文中。 在web.xml中配置过滤器（方法1） 1234567&lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;filter&gt; &lt;filter-name&gt;springSecurityFilterChain&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt; &lt;/filter&gt;&lt;/web-app&gt; 在Java中配置（方法2） 12345678910/** * SpringSecurity配置类 * @author NikoBelic * @create 11/01/2017 14:00 */public class SecurityWebInitializer extends AbstractSecurityWebApplicationInitializer&#123; &#125; 不管我们使用web.xml配置还是AbstractSecurityWebApplicationInitializer得子类来配置DelegatingFilterProxy，它都会拦截发往应用中的请求，并将请求委托给ID为springSecurityFilterChain的bean。 配置Security安全细节 1234567891011121314/** * 配置Web安全的细节 * @author NikoBelic * @create 11/01/2017 15:32 */@Configuration@EnableWebSecuritypublic class SecurityConfig extends WebSecurityConfigurerAdapter&#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; http.authorizeRequests().anyRequest().authenticated().and().formLogin().and().httpBasic(); &#125;&#125; 这个简单的默认配置指定了该如何保护HTTP请求，以及客户端认证用户的方案。通过调用authorizeRequest()和anyRequest().authenticated()就会要求所有进入应用的HTTP请求都要进行认证。他也配置SpringSecurity支持基于表单的登陆以及HTTPBasic方式的认证。由于没有配置configure(AuthenticationManagerBuilder)方法，所以没有用户存储支撑认证过程。没有人能够登陆成功。TODO： 配置用户存储 指定哪些请求需要认证，哪些请求不需要认证，以及所需要的权限 提供一个自定义的登陆界面，替代原来简单的默认登录页。 基于安全限制，有选择的在Web视图上显示特定的内容。 9.2 选择查询用户详细信息的服务SpringSecurity能够基于各种数据存储来认证用户。它内置了多种常见的用户存储场景，如内存、关系型数据库以及LDAP。 9.2.1 使用基于内存的用户存储1234567891011/** * 基于内存的用户登陆权限配置 * @Author NikoBelic * @Date 11/01/2017 17:43 */@Overrideprotected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth.inMemoryAuthentication().withUser(\"lxw\").password(\"lxw\").roles(\"USER\").and() .withUser(\"admin\").password(\"admin\").roles(\"USER\",\"ADMIN\");&#125; 9.2.2 基于数据库表进行认证SpringSecurity会内置一些查询语句，当查找用户信息时会自动执行。当然我们也可以自定义查询语句来获取用户信息。如下是相关java配置。将默认的sql查询替换为自定义的设计时，要遵循查询的基本协议。 所有查询都将用户名作为唯一的参数 认证查询会选取用户名、密码以及启用状态信息。 权限查询会选取0行或多行包含该用户名以及其权限信息的数据。 群组权限查询会选取0行或多行数据，每行数据中都会包括群组ID、群组名称以及权限。 12345678910111213141516171819202122232425262728293031323334/** * 配置Web安全的细节 * @author NikoBelic * @create 11/01/2017 15:32 */@Configuration@EnableWebSecuritypublic class SecurityConfig extends WebSecurityConfigurerAdapter&#123; @Autowired DataSource dataSource; @Override protected void configure(HttpSecurity http) throws Exception &#123; http.authorizeRequests().anyRequest().authenticated().and().formLogin().and().httpBasic(); &#125; /** * 用户登陆权限配置 * @Author NikoBelic * @Date 11/01/2017 17:43 */ @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; // 基于内存的用户权限配置 //auth.inMemoryAuthentication().withUser(\"lxw\").password(\"lxw\").roles(\"USER\").and() // .withUser(\"admin\").password(\"admin\").roles(\"USER\",\"ADMIN\"); // 基于JDBC的用户权限配置 auth.jdbcAuthentication().dataSource(dataSource). usersByUsernameQuery(\"select username,password,true from tb_user where username = ?\"). authoritiesByUsernameQuery(\"select username,'ROLE_USER' from tb_user where username = ?\"); &#125;&#125; 其中datasource是通过自动装配注入的，因此我们需要在root中或者xml中先配置一下数据源。 12345678910111213141516171819202122/** * Spring上下文配置类 * 由ContextLoaderListener加载的bean * @author NikoBelic * @create 09/01/2017 20:30 */@Configuration@ComponentScan(basePackages = \"chapter07\",excludeFilters = &#123;@ComponentScan.Filter(type = FilterType.ANNOTATION,value = EnableWebMvc.class)&#125;)public class RootConfig&#123; @Bean public DataSource dataSource() &#123; DriverManagerDataSource ds = new DriverManagerDataSource(); ds.setDriverClassName(\"com.mysql.jdbc.Driver\"); ds.setUrl(\"jdbc:mysql://localhost:3306/dmes?characterEncoding=UTF-8\"); ds.setUsername(\"root\"); ds.setPassword(\"*******\"); return ds; &#125;&#125; 之后再访问web层就可以使用数据库中已经存储的用户去登陆SpringSecurity校验界面了。补充：一般情况下数据库中的密码并不是明文存储的，因此我们需要一个密码转换方法将SpringSecurity从DB中获取到的密码进行解码。举个栗子：DB中username=niko,password=niko_salt(这是密文哦,用户注册后系统加工后的处理结果)。那么通过MyPasswordDecoder类处理后，我们通过SpringSecurity登录验证的输入为:niko,niko就能正常访问了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Configuration@EnableWebSecuritypublic class SecurityConfig extends WebSecurityConfigurerAdapter&#123; @Autowired DataSource dataSource; @Override protected void configure(HttpSecurity http) throws Exception &#123; http.authorizeRequests().anyRequest().authenticated().and().formLogin().and().httpBasic(); &#125; /** * 用户登陆权限配置 * * @Author NikoBelic * @Date 11/01/2017 17:43 */ @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; // 基于内存的用户权限配置 //auth.inMemoryAuthentication().withUser(\"lxw\").password(\"lxw\").roles(\"USER\").and() // .withUser(\"admin\").password(\"admin\").roles(\"USER\",\"ADMIN\"); // 基于JDBC的用户权限配置 auth.jdbcAuthentication().dataSource(dataSource). usersByUsernameQuery(\"select username,password,true from tb_user where username = ?\"). authoritiesByUsernameQuery(\"select username,'ROLE_USER' from tb_user where username = ?\"). passwordEncoder(new MyPasswordDecoder(\"_salt\")); &#125;&#125;/** * 密码解码器 * @Author NikoBelic * @Date 11/01/2017 19:26 */class MyPasswordDecoder implements PasswordEncoder&#123; private String salt; public MyPasswordDecoder(String salt) &#123; this.salt = salt; &#125; @Override public String encode(CharSequence charSequence) &#123; return charSequence.toString() + salt; &#125; @Override public boolean matches(CharSequence rawPassword, String encodePassword) &#123; return (rawPassword + salt).equals(encodePassword); &#125;&#125; 9.2.3 自定义查询用户服务1234567891011121314151617181920/** * 自定义用户权限查询类 * 可以配置为MongoDB、Redis等等 * @Author NikoBelic * @Date 11/01/2017 19:38 */class MyUserService implements UserDetailsService&#123; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; if (username.equals(\"test\")) &#123; List&lt;GrantedAuthority&gt; authorities = new ArrayList&lt;&gt;(); authorities.add(new SimpleGrantedAuthority(\"ROLE_SPITTER\")); return new User(\"test\", \"test\",authorities ); &#125; throw new UsernameNotFoundException(\"没有查询到该用户\"); &#125;&#125; 9.3 拦截请求针对不同的功能进行不同等级的安全认证是项目中经常会用到的。 1234567891011@Override protected void configure(HttpSecurity http) throws Exception &#123; // 对全部web请求进行安全校验 //http.authorizeRequests().anyRequest().authenticated().and().formLogin().and().httpBasic(); // 按照URL路径设置安全校验 http.authorizeRequests().antMatchers(\"/weibo/profile\").authenticated() .antMatchers(HttpMethod.GET,\"/weibo/testSecurity\").authenticated() .anyRequest().permitAll(); &#125; 以上配置指定了部分url禁止访问，匹配url的字符串支持Ant风格(antMatchers)的通配符。也可以在一个matcher中指定多个url链接，用逗号隔开。url也支持正则表达式regexMachers和anyRequest。.regexMachers(&quot;spitters/.*&quot;).authenticated()和/spitters/**所拦截的url是一样的。用来定义如何保护路径的配制方法 access(String) 如果给定的SpEL表达式计算结果为true，就允许访问。 anonymous() 允许匿名用户访问 authenticated() 允许认证过的用户访问 denyAll() 无条件拒绝所有访问 fullyAuthenticated() 如果用户是完整认证的话（不是通过Remember-me功能认证的）就允许访问 hasAnyAuthority(String…) 如果用户具备给定全险种的某一个的话，就允许访问 hasAnyRole(String…) 如果用户具备给定角色中的某一个的话，就允许访问 hasAuthority(String) 如果用户具备给定权限的话，就允许访问 hasIpAddress(String) 如果请求来自给定IP地址的话，就允许访问。 hasRole(String) 如果用户具备给定角色的话，就允许访问 not() 对其他访问方法的结果求反 permitAll() 无条件允许访问 rememberMe() 如果哟过户是通过Remember-me功能认证的，就允许访问 我们可以将任意数量的antMachers、regexMatcgers、anyRequest连接起来，以满足web安应用安全规则的需要。这些规则按照给定的顺序发挥作用，所以必须将最具体（细粒度）的请求路径放在前面，而最不具体的路径（anyRequest）放在最后面。否则后面的路径配置将会被前面的覆盖。 9.3.1 使用Spring表达式进行安全保护使用SpEL表达式可以使安全限制更加灵活强大，以上表格展示了所有限制方法，但是假如我们需要使用一些特殊的安全机制，那么可以使用SpEL表达式。| authentication | 用户的认证对象 || — | — || denyAll | 结果始终为false || hasAnyRole(list of roles) | 如果用户被授予了列表中的人已制定角色，结果为true || hasRole(role) | 如果用户被授予了指定的角色，结果为true || hasIpAddress(IP) | 如果来自指定的IP，结果为true || isAnonymous() | 如果当前用户为匿名用户，结果为true || isAuthenticated() | 如果当前用户进行了认证，结果为true || isFullyuAuthenticated() | 如果当前用户进行了完整认证，结果为true || isRememberMe() | 如果当前用户是通过Remember-me自动认证的，结果为true || permitAll | 结果为true || principal | 用户的principal对象 | .antMachers(“/spitter/me”).access(“hasRole(‘ROLE_SPITTER’) and hasIpAddress(‘192.168.1.2’)”) 9.3.2 强制通道的安全性将访问路径强制转换为https 12345http.authorizeRequests().antMatchers(\"/weibo/profile/*\").hasRole(\"USER\") .antMatchers(HttpMethod.GET,\"/weibo/testSecurity\").authenticated() .and() .requiresChannel() .anyRequest().requiresSecure(); 9.3.3 防止跨站请求伪造SpringSecurity会默认启动CSRF防护，因此我们无法向其他站点提交请求，以防止页面在被篡改的情况下站点被CSRF攻击。我们可以在jsp页面中的form表单中添加csrf隐藏域(后台传过来的)。 1&lt;input type=\"hidden\" name = \"$&#123;__csrf。paramteterName&#125;\" value=\"$&#123;__csrf.token&#125;\"/&gt; 或者在SpringSeccurity中禁用CSRF防护。 123http ... .csrf().disable() 9.4 认证用户上面的安全配制方法，虽然配置了一些用户权限问题，但是没有设置登陆界面，所以你咋让我认证啊？解决方案如下 1234567// 按照URL路径设置安全校验 http.formLogin() // 启动默认的登陆表单 .and() .authorizeRequests()// 授权通过以后 .antMatchers(\"/weibo/profile/*\").hasRole(\"USER\") // 访问指定路径必须拥有USER角色权限 .antMatchers(HttpMethod.GET,\"/weibo/testSecurity\").authenticated() // 指定url只允许GET请求 .anyRequest().permitAll(); //其他请求均可通过 9.4.1 添加自定义的登陆页自己造一个 1form['weibo/login' POST](input(username),input(password),submit()) 注意表单提交位置、输入域名称就可以了，简单的一匹。 1234567http.csrf().disable().formLogin() .loginPage(\"/weibo/showLogin\")// 登陆表单 .and() .authorizeRequests()// 授权通过以后 .antMatchers(\"/weibo/profile/*\").hasRole(\"USER\") // 访问指定路径必须拥有USER角色权限 .antMatchers(HttpMethod.GET,\"/weibo/testSecurity\").authenticated() // 指定url只允许GET请求 .anyRequest().permitAll(); //其他请求均可通过 Controller12345678910/** * 跳转登陆页面 * @Author NikoBelic * @Date 12/01/2017 19:46 */ @RequestMapping(value = \"/showLogin\",method = RequestMethod.GET) public String showLogin() &#123; return \"/weibo/login\"; &#125; 9.4.2 启用HTTP Basic认证对于普通用户来说，展现一个login表单还可以，但是对于应用程序来说就需要另外一种验证方式了。启用HTTP Basic认证的方式很简单。 12345678910// 按照URL路径设置安全校验 http.csrf().disable().formLogin() .loginPage(\"/weibo/showLogin\")// 登陆表单 .and() .httpBasic() .and() .authorizeRequests()// 授权通过以后 .antMatchers(\"/weibo/profile/*\").hasRole(\"USER\") // 访问指定路径必须拥有USER角色权限 .antMatchers(HttpMethod.GET,\"/weibo/testSecurity\").authenticated() // 指定url只允许GET请求 .anyRequest().permitAll(); //其他请求均可通过 模拟Basic认证请求 9.4.3 RememberMe功能和注销功能12345678910111213141516http.csrf().disable().formLogin() .loginPage(\"/weibo/showLogin\")// 登陆表单 .and() .logout() // 注销 清楚remember-me的token .logoutSuccessUrl(\"/weibo/register\") .and() .httpBasic()//启动服务器认证功能 .and() .rememberMe() // 启动记住我功能 .tokenValiditySeconds(10) // 只保存10s的token .key(\"weiboKey\") .and() .authorizeRequests()// 授权通过以后 .antMatchers(\"/weibo/profile/*\").hasRole(\"USER\") // 访问指定路径必须拥有USER角色权限 .antMatchers(HttpMethod.GET,\"/weibo/testSecurity\").authenticated() // 指定url只允许GET请求 .anyRequest().permitAll(); //其他请求均可通过 9.5 小结Spring Security提供了一种简单、灵活且强大的机制来保护Web应用程序。借助一系列Servlet Filter能够控制对Web资源的访问，包括MVC控制器。借助于Java配置模型，能够简洁的声明Web安全性功能。当认证用户是，可以使用基于内存用户库、关系型数据库、LDAP目录服务器来配置认证功能。","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"},{"name":"Spring","slug":"Spring","permalink":"https://seawaylee.github.io/tags/Spring/"}]},{"title":"SpringInAction读书笔记(七) SpringMVC高级技术","date":"2017-03-09T15:44:02.000Z","path":"2017/03/09/JavaWeb/Spring/Spring_7/","text":"7.1 SpringMVC的环境搭建方式Java配置方式 全局配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class SpitterWebInitializer extends AbstractAnnotationConfigDispatcherServletInitializer&#123; /** * Spring上下文配置 * ContextLoaderListener * @Author NikoBelic * @Date 09/01/2017 20:40 */ @Override protected Class&lt;?&gt;[] getRootConfigClasses() &#123; return new Class&lt;?&gt;[]&#123;RootConfig.class&#125;; &#125; /** * SpringMVC上下文配置 * DisparcherServlet * @Author NikoBelic * @Date 09/01/2017 20:41 */ @Override protected Class&lt;?&gt;[] getServletConfigClasses() &#123; return new Class&lt;?&gt;[]&#123;WebConfig.class&#125;; &#125; /** * SpringMVC请求拦截,将DispatcherServlet映射到/ * @Author NikoBelic * @Date 09/01/2017 20:41 */ @Override protected String[] getServletMappings() &#123; return new String[]&#123;\"/\"&#125;; &#125; /** * 配置文件上传限制 * @Author NikoBelic * @Date 09/01/2017 20:40 */ @Override protected void customizeRegistration(ServletRegistration.Dynamic registration) &#123; registration.setMultipartConfig(new MultipartConfigElement(\"/Users/lixiwei-mac/Desktop/tmp\",20971520, 41943040, 0)); &#125;&#125; Spring上下文配置（ContextLoaderListener） 123456789/** * @author NikoBelic * @create 09/01/2017 20:30 *///@Configuration@ComponentScan(basePackages = \"chapter07\",excludeFilters = &#123;@ComponentScan.Filter(type = FilterType.ANNOTATION,value = EnableWebMvc.class)&#125;)public class RootConfig&#123;&#125; SpringMVC配置（DispatcherServlet） 12345678910111213141516171819202122232425262728293031323334353637383940414243@Configuration@EnableWebMvc@ComponentScan(\"chapter07.controller\")public class WebConfig extends WebMvcConfigurerAdapter&#123; /** * 配置视图解析器 * @Author NikoBelic * @Date 09/01/2017 20:37 */ @Bean public ViewResolver viewResolver() &#123; InternalResourceViewResolver resolver = new InternalResourceViewResolver(); resolver.setPrefix(\"/WEB-INF/views/\"); resolver.setSuffix(\".jsp\"); resolver.setExposeContextBeansAsAttributes(true); resolver.setViewClass(JstlView.class); return resolver; &#125; /** * 配置静态资源的处理 * @Author NikoBelic * @Date 09/01/2017 20:37 */ @Override public void configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer) &#123; configurer.enable(); &#125; /** * 配置文件上传解析器 * @Author NikoBelic * @Date 09/01/2017 20:38 */ @Bean public MultipartResolver multipartResolver() throws IOException &#123; return new StandardServletMultipartResolver(); &#125;&#125; XML配置方式（略） 可以参考前几章 7.2 处理Multipart文件上传web相关配置已经包含在7.1中 jsp页面 1234567891011121314151617181920212223242526272829&lt;%@ taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\" %&gt;&lt;%@ taglib prefix=\"sf\" uri=\"http://www.springframework.org/tags/form\" %&gt;&lt;%@ page isELIgnored=\"false\" %&gt;&lt;%@ page session=\"false\" %&gt;&lt;%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;RegisterForm&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Register&lt;/h1&gt;&lt;%--这里Form表单没有设置action,因此他会将请求提交到与展现时相同的URL路径上--%&gt;&lt;form method=\"post\" enctype=\"multipart/form-data\"&gt; Username:&lt;input type=\"text\" name=\"username\"/&gt;&lt;br/&gt; Password:&lt;input type=\"password\" name=\"password\"/&gt;&lt;br/&gt; FirstName:&lt;input type=\"text\" name=\"firstName\"/&gt;&lt;br/&gt; LastName:&lt;input type=\"text\" name=\"lastName\"/&gt;&lt;br/&gt; ProfilePic:&lt;input type=\"file\" name=\"profilePic\" accept=\"image/jpeg,image/png,image/gif\"/&gt;&lt;br/&gt; &lt;input type=\"submit\" value=\"Register\"/&gt;&lt;/form&gt;使用Spring表单绑定标签&lt;%--&lt;sf:form method=\"post\" commandName=\"spitter\"&gt;--%&gt; &lt;%--Username:&lt;sf:input path=\"username\"/&gt;&lt;br/&gt;--%&gt; &lt;%--Password:&lt;sf:password path=\"password\"/&gt;&lt;br/&gt;--%&gt; &lt;%--&lt;input type=\"submit\" value=\"Register\"&gt;--%&gt;&lt;%--&lt;/sf:form&gt;--%&gt;&lt;/body&gt;&lt;/html&gt; 控制器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@Controller@RequestMapping(value = \"weibo\")public class WeiboController&#123; /** * 跳转到注册界面 * @Author NikoBelic * @Date 09/01/2017 22:35 */ @RequestMapping(value = \"/register\",method = RequestMethod.GET) public String showRegister() &#123; return \"weibo/register\"; &#125; /** * 处理注册流程 * @Author NikoBelic * @Date 09/01/2017 22:36 */ @RequestMapping(value = \"/register\",method = RequestMethod.POST) public String processRegister(@RequestPart(\"profilePic\")MultipartFile profilePic, @Valid Spitter spitter, Errors errors, RedirectAttributes model) throws IOException, DuplicateException &#123; profilePic.transferTo(new File(\"/Users/lixiwei-mac/Desktop/data/\" + profilePic.getOriginalFilename())); System.out.println(spitter); if (spitter.getUsername().equals(\"dup\")) throw new DuplicateException(); model.addAttribute(\"username\",spitter.getUsername()); model.addAttribute(\"password\",spitter.getPassword()); model.addFlashAttribute(spitter); return \"redirect:/weibo/profile/&#123;username&#125;\"; &#125; @RequestMapping(value = \"/profile/&#123;username&#125;\",method = RequestMethod.GET) public String profile(@PathVariable(\"username\") String username,String password,Model model) &#123; System.out.println(username); System.out.println(password); System.out.println(model.containsAttribute(\"spitter\")); return \"profile\"; &#125; /** * 控制器异常处理 * @Author NikoBelic * @Date 10/01/2017 08:51 */ //@ExceptionHandler(DuplicateException.class) //@ResponseBody //public String handleDuplicateException() //&#123; // return \"error/duplicate\"; //&#125;&#125; customizeRegistration配置指定了临时文件的存储地址（传输完毕后会自动清除） 7.3 处理异常使用@ExceptionHandler可以处理Controller抛出的异常，异常出现后可以跳转到更加友好的提示界面。@ControllerAdvice作为一个Controller的全局配置，对所有@Controller注解的@RequestMapping方法生效。 123456789@ControllerAdvicepublic class AppWideExceptionHandler&#123; @ExceptionHandler(DuplicateException.class) public String duplicateSpittleHandler() &#123; return \"error/duplicate\"; &#125;&#125; 7.4 跨重定向传递数据redirect：将客户端的请求以get方式重定向到另一个controller，可以用${}路径和?参数的方式传递简单数据，但是无法传输Spitter这样的对象。重定向后模型中的数据将会丢失。forward：将客户端的请求转发到另外一个页面，保留原有的模型数据，但是浏览器地址不会发生改变，因为浏览器不知道自己的请求被转发了。 如果需要使用redirect并想保留model中的数据，可以使用flash属性。使用RedirectAttributes作为model，它会将模型暂存到session中，重定向后可以使用Model取出对应的对象。我们也可以不借助Spring提供的这个方法，自己将对象存储到session中，转发后在从session中取出并清空session。","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"},{"name":"Spring","slug":"Spring","permalink":"https://seawaylee.github.io/tags/Spring/"}]},{"title":"SpringInAction读书笔记(六) 渲染Web试图","date":"2017-03-09T15:42:53.000Z","path":"2017/03/09/JavaWeb/Spring/Spring_6/","text":"我们最常使用的JSP文件是使用InternalResourceViewResolver试图解析器来渲染试图。在第5章中，我们用这个视图解析器进行了配置，为了得到试图的名字，会使用”/WEB-INF/views/“前缀和”.jsp”后缀，从而确定来渲染模型的JSP文件的物理位置。SpringMVC规定以了一个名为ViewResolver的接口，它大致如下所示: 1234public interface ViewResolver&#123; View resolveViewName(String viewName,Locale locale);&#125; 当给resolveViewName()方法传入一个视图名和Locale对象时，他会返回一个View实例。View是另外一个接口，如下所示： 12345public interface View&#123; String getContentType(); void render(Map&lt;String,?&gt; model, HttpServletRequest request, HttpServletResponse response) ;&#125; View接口的任务就是接受模型以及Servlet的request和response对象，并将其输出结果渲染到response中。这看起来非常简单，我们所需要做的就是编写ViewResolver和View的实现，将要渲染的内容放到response中，进而展现到用户的浏览器中。虽然我们可以编写者两个实现类，但是一般情况下，直接使用Spring提供的多个内置的实现就可以了。一般我们用InternalResourceViewResolver，其他类型请自行查阅。","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"},{"name":"Spring","slug":"Spring","permalink":"https://seawaylee.github.io/tags/Spring/"}]},{"title":"SpringInAction读书笔记(五) SpringMVC起步","date":"2017-03-09T15:38:41.000Z","path":"2017/03/09/JavaWeb/Spring/Spring_5/","text":"##5.1 请求在SpringMVC组件之间的流转 请求发送 DispatcherServlet查询一个或多个HandlerMapping来确定请求的下一站。 按照HM查询结果，将请求转发到指定的Controller。 Controller返回ModelAndView给DispatcherServlet。 DispatcherServlet使用视图解析器ViewResolver将逻辑试图名匹配为一个特定的视图实现（一般是JSP）。 将Model传出给视图实现，渲染试图。 将渲染结果通过响应对象（Response）传递给客户端。 ##5.2 使用Java类来搭建SpringMVC传统配置SpringMVC是用web.xml+Spring配置文件结合的方式。但是如果你的容器支持Servlet3.0（Tomcat7及以上） 1234567891011121314151617181920212223242526272829303132333435363738394041424344package chapter05;import org.springframework.web.servlet.support.AbstractAnnotationConfigDispatcherServletInitializer;/** * 配置DispatcherServlet * @author NikoBelic * @create 30/12/2016 10:43 */public class SpittrWebInitializer extends AbstractAnnotationConfigDispatcherServletInitializer&#123; /** * 指定ContextLoaderListener配置类 * @Author NikoBelic * @Date 30/12/2016 10:48 */ @Override protected Class&lt;?&gt;[] getRootConfigClasses() &#123; return new Class&lt;?&gt;[]&#123;RootConfig.class&#125;; &#125; /** * 指定SpringMVC配置类 * @Author NikoBelic * @Date 30/12/2016 10:49 */ @Override protected Class&lt;?&gt;[] getServletConfigClasses() &#123; return new Class&lt;?&gt;[]&#123;WebConfig.class&#125;; &#125; /** * 将DispatcherServlet映射到 \"/\" * @Author * @Date 30/12/2016 10:44 */ @Override protected String[] getServletMappings() &#123; return new String[]&#123;\"/\"&#125;; &#125;&#125; 123456789101112131415161718192021222324252627282930/** * 最小但可用的SpringMVC配置 * @author NikoBelic * @create 30/12/2016 10:50 */@Configuration@EnableWebMvc@ComponentScan(\"chapter05\")public class WebConfig extends WebMvcConfigurerAdapter&#123; @Bean public ViewResolver viewResolver() &#123; InternalResourceViewResolver resolver = new InternalResourceViewResolver(); resolver.setPrefix(\"/WEB-INF/views/\"); resolver.setSuffix(\".jsp\"); resolver.setExposeContextBeansAsAttributes(true); return resolver; &#125; /** * 配置静态资源的处理 * @Author NikoBelic * @Date 30/12/2016 12:15 */ @Override public void configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer) &#123; configurer.enable(); &#125;&#125; 12345678910111213141516171819202122232425262728293031/** * 最小但可用的SpringMVC配置 * @author NikoBelic * @create 30/12/2016 10:50 */@Configuration@EnableWebMvc@ComponentScan(\"chapter05\")public class WebConfig extends WebMvcConfigurerAdapter&#123; @Bean public ViewResolver viewResolver() &#123; InternalResourceViewResolver resolver = new InternalResourceViewResolver(); resolver.setPrefix(\"/WEB-INF/views/\"); resolver.setSuffix(\".jsp\"); resolver.setExposeContextBeansAsAttributes(true); return resolver; &#125; /** * 配置静态资源的处理 * @Author NikoBelic * @Date 30/12/2016 12:15 */ @Override public void configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer) &#123; configurer.enable(); &#125;&#125; Spring上下文的加载方式有两个，一个是使用SpringMVC的DispatcherServlet去加载（控制器、视图解析器、处理器映射器），另一个是用ContextLoaderListener加载（应用中的其他bean，例如Service、Dao）。如果上程序所写，我们配置WebConfig类来加载SpringMVC需要的bean。配置RootConfig来加载剩余的bean。 ##5.2 编写基本的控制器 @Controller其实是包含@Component注解的，二者的功能是相同的，但是为了更加清晰的区分不同的组件，使用@Controller来标识这是一个SpringMVC的组件。@RequestMapping可以加载类上，也可以加载在方法上，有一下几种情况： 类有注解，方法有注解，那么HTTP访问路径为 http://IP:Port/Project/ControllerMapping/MethodMapping 类无注解，方法有注解 http://IP:Port/Project/MethodMapping 类有注解，方法无注解，那么只允许该Controller中只有一个Method，否则启动报错 我们一般使用方法1 12345678910@Controller@RequestMapping(\"home\")public class HomeController&#123; @RequestMapping(value = \"list\",method = RequestMethod.GET) public String home() &#123; return \"home\"; &#125;&#125; 创建一个jsp页面 123456789101112&lt;%@ taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\" %&gt;&lt;%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Spittr&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Spittr&lt;/h1&gt;&lt;a href=\"&lt;c:url value=\"/spittles\"/&gt;\"&gt;Spittles&lt;/a&gt;&lt;a href=\"&lt;c:url value=\"/spitter/register\"/&gt;\"&gt;Spittles&lt;/a&gt;&lt;/body&gt;&lt;/html&gt; 使用Tomcat启动服务后，项目会根据SpittrWebInitializer配置的信息加载相关的配置文件。 以上构建了一个最简单的一个请求结果展示，那么如果我们需要给前台界面返回一些数据，怎么做？ 修改控制器 1234567891011121314151617181920212223242526272829303132333435@Controller@RequestMapping(\"home\")public class HomeController&#123; @RequestMapping(value = \"list\",method = RequestMethod.GET) public String home() &#123; return \"home\"; &#125; @RequestMapping(value = \"spittles\",method = RequestMethod.GET) public ModelAndView getSpittles(ModelAndView mv) &#123; List&lt;Spittle&gt; spittleList = createSpittleList(10); mv.setViewName(\"spittle\"); mv.addObject(\"spittleList\",spittleList); return mv; &#125; /** * 模拟从数据库获取数据 * @Author NikoBelic * @Date 31/12/2016 13:15 */ private List&lt;Spittle&gt; createSpittleList(int count) &#123; List&lt;Spittle&gt; spittles = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; count; i++) &#123; spittles.add(new Spittle(i,\"Spittle \" + i ,new Date(),null,null)); &#125; return spittles; &#125;&#125; 创建JSP页面 1234567891011121314151617181920212223242526272829&lt;%@ taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\" %&gt;&lt;%@ page isELIgnored=\"false\" %&gt;&lt;%@ page session=\"false\" %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Spittr&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Spittles&lt;/h1&gt;&lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;td&gt;Id&lt;/td&gt; &lt;td&gt;Message&lt;/td&gt; &lt;td&gt;Time&lt;/td&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;c:forEach items=\"$&#123;spittleList&#125;\" var=\"spittle\"&gt; &lt;tr&gt; &lt;td&gt;$&#123;spittle.id&#125;&lt;/td&gt; &lt;td&gt;$&#123;spittle.message&#125;&lt;/td&gt; &lt;td&gt;$&#123;spittle.time&#125;&lt;/td&gt; &lt;/tr&gt; &lt;/c:forEach&gt; &lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; ##5.3 接受请求的输入Controller中的方法接收参数的方式有很多以分页请求为例。 Controller 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384@Controller@RequestMapping(\"home\")public class HomeController&#123; private static List&lt;Spittle&gt; spittleList; private static final int PAGE_SIZE = 5; static &#123; spittleList = createSpittleList(Math.max(10, new Random().nextInt(100))); System.out.println(\"随机生成数据:\" + spittleList.size()); &#125; @RequestMapping(value = \"list\", method = RequestMethod.GET) public String home() &#123; return \"home\"; &#125; /** * 获取Spittles * * @Author NikoBelic * @Date 31/12/2016 22:22 */ @RequestMapping(value = \"spittles/&#123;page&#125;\", method = RequestMethod.GET) public ModelAndView getSpittles(@PathVariable int page, ModelAndView mv) &#123; Map&lt;String, Integer&gt; pageInfo = getPageInfo(page, spittleList.size()); List&lt;Spittle&gt; pageSpittleList = spittleList.subList(pageInfo.get(\"start\"), pageInfo.get(\"end\")); mv.setViewName(\"spittle\"); mv.addObject(\"spittleList\", pageSpittleList); mv.addObject(\"totalPage\", pageInfo.get(\"totalPage\")); mv.addObject(\"page\", page); return mv; &#125; /** * 模拟从数据库获取数据 * * @Author NikoBelic * @Date 31/12/2016 13:15 */ private static List&lt;Spittle&gt; createSpittleList(int count) &#123; List&lt;Spittle&gt; spittles = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; count; i++) &#123; spittles.add(new Spittle(i, \"Spittle \" + i, new Date(), null, null)); &#125; return spittles; &#125; /** * 分页工具 * * @Author NikoBelic * @Date 31/12/2016 23:39 */ private Map&lt;String, Integer&gt; getPageInfo(int page, int totalRecord) &#123; Map&lt;String, Integer&gt; pageInfo = new HashMap&lt;&gt;(); int start = (page - 1) * PAGE_SIZE; // 当前页起始index int end; int totalPage = (int) Math.ceil(totalRecord / (float) PAGE_SIZE); if (page != totalPage) end = start + PAGE_SIZE; else &#123; if (totalRecord % PAGE_SIZE != 0) end = (totalPage - 1) * PAGE_SIZE + totalRecord % PAGE_SIZE; else end = start + PAGE_SIZE; &#125; pageInfo.put(\"start\", start); pageInfo.put(\"end\", end); pageInfo.put(\"page\", page); pageInfo.put(\"totalPage\", totalPage); System.out.println(\"起始:\" + start + \",结束:\" + end + \",当前页:\" + page + \",总页数:\" + totalPage + \",总数据:\" + totalRecord); return pageInfo; &#125;&#125; JSP 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;%@ taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\" %&gt;&lt;%@ page isELIgnored=\"false\" %&gt;&lt;%@ page session=\"false\" %&gt;&lt;%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Spittr&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Spittles&lt;/h1&gt;&lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;td&gt;Id&lt;/td&gt; &lt;td&gt;Message&lt;/td&gt; &lt;td&gt;Time&lt;/td&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;c:forEach items=\"$&#123;spittleList&#125;\" var=\"spittle\"&gt; &lt;tr&gt; &lt;td&gt;$&#123;spittle.id&#125;&lt;/td&gt; &lt;td&gt;$&#123;spittle.message&#125;&lt;/td&gt; &lt;td&gt;$&#123;spittle.time&#125;&lt;/td&gt; &lt;/tr&gt; &lt;/c:forEach&gt; &lt;/tbody&gt;&lt;/table&gt;&lt;span&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/home/spittles/1\"&gt;首页&lt;/a&gt;&lt;/span&gt;&lt;span&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/home/spittles/&lt;c:out value='$&#123;page-1&#125;'/&gt;\" &lt;c:if test=\"$&#123;page &lt;= 1&#125;\"&gt; hidden=\"hidden\" &lt;/c:if&gt;&gt;上一页&lt;/a&gt;&lt;/span&gt;&lt;span&gt;当前页:$&#123;page&#125;&lt;/span&gt;&lt;span&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/home/spittles/&lt;c:out value='$&#123;page+1&#125;'/&gt;\" &lt;c:if test=\"$&#123;page &gt;= totalPage&#125;\"&gt; hidden=\"hidden\" &lt;/c:if&gt;&gt;下一页&lt;/a&gt;&lt;/span&gt;&lt;span&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/home/spittles/$&#123;totalPage&#125;\"&gt;尾页&lt;/a&gt;&lt;/span&gt;&lt;/body&gt;&lt;/html&gt; ##5.4 处理表单我们模拟一个用户注册情景用户通过浏览器请求得到一个注册页面，通过注册页面完成注册后自动跳转到个人信息页面。Controller 12345678910111213141516171819202122232425262728@Controller@RequestMapping(\"spitter\")public class SpitterController&#123; @Autowired private SpitterDao spitterDao; @RequestMapping(value = \"register\",method = RequestMethod.GET) public String showRegisterForm() &#123; return \"register\"; &#125; @RequestMapping(value =\"register\",method = RequestMethod.POST) public String processRegister(Spitter spitter) &#123; spitterDao.save(spitter); return \"redirect:/spitter/\" + spitter.getUsername(); &#125; @RequestMapping(value =\"&#123;username&#125;\" ,method = RequestMethod.GET) public String showSpitterProfile(@PathVariable String username,Model model) &#123; Spitter spitter = spitterDao.findSpitterByUsername(username); model.addAttribute(\"spitter\",spitter); return \"profile\"; &#125;&#125; 注册表单信息 1234567891011121314151617181920&lt;%@ taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\" %&gt;&lt;%@ page isELIgnored=\"false\" %&gt;&lt;%@ page session=\"false\" %&gt;&lt;%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;RegisterForm&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Register&lt;/h1&gt;&lt;%--这里Form表单没有设置action,因此他会将请求提交到与展现时相同的URL路径上--%&gt;&lt;form method=\"post\"&gt; Username:&lt;input type=\"text\" name=\"username\"/&gt;&lt;br/&gt; Password:&lt;input type=\"password\" name=\"password\"/&gt;&lt;br/&gt; FirstName:&lt;input type=\"text\" name=\"firstName\"/&gt;&lt;br/&gt; LastName:&lt;input type=\"text\" name=\"lastName\"/&gt;&lt;br/&gt; &lt;input type=\"submit\" value=\"Register\"/&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 个人信息 1234567891011121314&lt;%@ taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\" %&gt;&lt;%@ page isELIgnored=\"false\" %&gt;&lt;%@ page session=\"false\" %&gt;&lt;%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Spittr&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Your Profile&lt;/h1&gt;$&#123;spitter.username&#125;&lt;br/&gt;$&#123;spitter.firstName&#125; $&#123;spitter.lastName&#125;&lt;/body&gt;&lt;/html&gt; Model 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class Spitter&#123; String username; String firstName; String lastName; String password; public Spitter() &#123; &#125; public Spitter(String username, String firstName, String lastName, String password) &#123; this.username = username; this.firstName = firstName; this.lastName = lastName; this.password = password; &#125; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public String getFirstName() &#123; return firstName; &#125; public void setFirstName(String firstName) &#123; this.firstName = firstName; &#125; public String getLastName() &#123; return lastName; &#125; public void setLastName(String lastName) &#123; this.lastName = lastName; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125;&#125; Dao 1234567891011121314151617181920212223242526@Repositorypublic class SpitterDao&#123; private List&lt;Spitter&gt; spitterList; public SpitterDao() &#123; this.spitterList = new ArrayList&lt;&gt;(); System.out.println(spitterList); &#125; public void save(Spitter spitter) &#123; spitterList.add(spitter); &#125; public Spitter findSpitterByUsername(String username) &#123; for (Spitter spitter : spitterList) &#123; if (spitter.getUsername().equals(username)) return spitter; &#125; return null; &#125;&#125; 校验表单 注解 描述 @AssertFalse 所注解的元素必须是Boolean型，并且值为false @AssertTrue 所注解的元素必须是Boolean型，并且值为true @DecimalMax 必须是数字，且小于指定值 @DecimalMin 必须是数字，且大于指定值 @Digits 必须是数字，并且值必须有指定的位数 @Future 必须是将来的一个日期 @Max 必须是数字，且小于等于指定值 @Min 必须是数字，且大于等于指定值 @NotNull 值不能为null @Null 值必须为null @Past 必须是过去的一个日期 @Pattern 值必须匹配给定的正则表达式 @Size 值必须是String、集合、数组，并且长度必要符合给定的范围","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"},{"name":"Spring","slug":"Spring","permalink":"https://seawaylee.github.io/tags/Spring/"}]},{"title":"SpringInAction读书笔记(四) 面向切面的Spring","date":"2017-03-09T15:35:55.000Z","path":"2017/03/09/JavaWeb/Spring/Spring_4/","text":"4.1AOP所要解决的问题将横切关注点（包裹在业务代码外层）与业务逻辑相分离，AOP实现将横切关注点与他们所影响的对象之间的解耦。除此之外，AOP还还会在声明式事务、安全和缓存进行应用。AOP用于重用通用功能，传统方式最常见的是继承和委托。继承会造成对象体系非常脆弱，委托会对委托对象进行复杂的对象，AOP提供了另一种可选方案。我们通过声明式的方式定义这个功能要以何种方式在何处应用，而无需求改受影响的类。横切关注点可以被模块化为特殊的类，这些类被称为切面（aspect）。优点： 1.每个关注点都集中于一个地方，而不是分散到代码中 2.服务模块更简洁，只包含核心代码，关注点的代码都被转移到切面中 4.2 相关术语 通知（advice） 切点（pointcut） 连接点（joinpoint） 4.2.1 通知切面的工作被称为通知。 前置通知（Before）：在目标方法被调用之前调用通知功能 后置通知（After）：在目标方法完成之后调用通知，此时不会关心方法的输出是什么 返回通知（After-returning）：在目标方法成功执行之后调用通知 异常通知（After-throwing）：在目标方法抛出异常后调用通知 环绕通知（Around）：通知包裹了被通知的方法，在被通知的方法调用之前和调用之后执行自定义的行为 4.2.2 连接点连接点是可以插入切面的一个点（时间点）。例如方法执行前、方法执行后、抛出异常时…等等。 4.2.3 切点定义切面在何处（位置点）进行通知。 4.2.4 切面切面是通知和切点的结合。 4.2.5 引入引入允许我们向现有的类添加新的方法或属性。 4.2.6 织入织入是把切面应用到目标对象并创建新的代理对象的过程。切面在指定的连接点被织入到目标对象中。在目标对象的生命周期里有多个点可以进行织入： 编译期：切面在目标类编译时被织入。例如AspectJ的织入编译器。 类加载期：切面在目标被加载到JVM时被织入。例如AspectJ5. 运行期：切面在应用运行的某个时刻被织入。一般情况啊下，在织入切面时，AOP容器会为目标对象动态地创建一个代理对象。SpringAOP就是以这种方式织入切面的。（学习下反射和动态代理哦） 4.3 切点表达式语言知道AspectJ是一种切点表达式语言就够啦。用起来超级简单。以下注释部分是配置是说明如何通过AspectJ表达式来匹配该方法作为切点。 创建一个表演类，我们要在表演前后做一些动作 123456789101112131415161718@Componentpublic class Performance&#123; /** * 执行该方法的切点表达是语言 execution(* chapter04.Performace.peforne(...)) * execution:表示执行该表达式语言 * * 表示任意返回类型 * chapter04.Performance.performe 分别是 包名、类名、方法名 * ...表示使用任意参数 * @Author NikoBelic * @Date 28/12/2016 19:04 */ public void perform() &#123; System.out.println(\"我表演啦\"); //return; &#125;&#125; 创建观众类作为切面 12345678910111213141516171819202122232425262728293031323334@Aspectpublic class Audience&#123; // 定义命名的切点 @Pointcut(\"execution(* *.perform(..))\") public void performance() &#123;&#125; @Before(\"performance()\") public void silenceCellPhones() &#123; System.out.println(\"观众们请关闭手机铃声...\"); &#125; @Before(\"performance()\") public void takSeats() &#123; System.out.println(\"大家请坐...\"); &#125; @AfterReturning(\"performance()\") public void applause() &#123; System.out.println(\"大家请鼓掌...\"); &#125; @AfterThrowing(\"performance()\") public void deamanRefund() &#123; System.out.println(\"表演失败了,大家鼓励一下...\"); &#125;&#125; 基于类的Spring配置 1234567891011@Configuration@EnableAspectJAutoProxy@ComponentScanpublic class ConcertConfig&#123; @Bean public Audience audience() &#123; return new Audience(); &#125;&#125; 测试方法 12345678910111213@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = ConcertConfig.class)public class Main&#123; @Autowired Performance performance; @Test public void aopTest() &#123; performance.perform(); &#125;&#125; 输出 观众们请关闭手机铃声... 大家请坐... 我表演啦 大家请鼓掌... 使用XML配置 123456789101112131415&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xsi:schemaLocation=\"http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-3.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd\"&gt; &lt;!--配置自动扫包--&gt; &lt;context:component-scan base-package=\"chapter04\"/&gt; &lt;!--启动AspectJ自动代理--&gt; &lt;aop:aspectj-autoproxy/&gt;&lt;/beans&gt; 测试 1234567891011121314@RunWith(SpringJUnit4ClassRunner.class)//@ContextConfiguration(classes = ConcertConfig.class)@ContextConfiguration(locations = \"classpath:spring/c4-applicationContext.xml\")public class Main&#123; @Autowired Performance performance; @Test public void aopTest() &#123; performance.perform(); &#125;&#125; 环绕通知修改切面类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Aspectpublic class Audience&#123; // 定义命名的切点 @Pointcut(\"execution(* *.perform(..))\") public void performance() &#123;&#125; @Around((\"performance()\")) public void watchPerformance(ProceedingJoinPoint joinPoint) &#123; try &#123; System.out.println(\"环绕-手机静音\"); System.out.println(\"环绕-就坐\"); joinPoint.proceed(); System.out.println(\"环绕-牛b牛b!再来一个!\"); &#125; catch (Throwable throwable) &#123; System.out.println(\"环绕-垃圾!退钱!\"); &#125; &#125; //@Before(\"performance()\") public void silenceCellPhones() &#123; System.out.println(\"观众们请关闭手机铃声...\"); &#125; //@Before(\"performance()\") public void takSeats() &#123; System.out.println(\"大家请坐...\"); &#125; //@AfterReturning(\"performance()\") public void applause() &#123; System.out.println(\"大家请鼓掌...\"); &#125; //@AfterThrowing(\"performance()\") public void deamanRefund() &#123; System.out.println(\"表演失败了,大家鼓励一下...\"); &#125;&#125; 其他不变，测试结果环绕-手机静音 环绕-就坐 我表演啦 环绕-牛b牛b!再来一个! 带参数的通知 我们通过编写切面来统计CD磁盘中的某个歌曲被播放了几次播放器类 12345678@Componentpublic class CDPlayer&#123; public void play(int songNum) &#123; System.out.println(\"正在播放CD磁盘中第\" + songNum + \"首歌曲\"); &#125;&#125; 切面类 123456789101112131415161718192021@Aspectpublic class PlayCounter&#123; private HashMap&lt;Integer,Integer&gt; trackCounts = new HashMap&lt;&gt;(); @Pointcut(\"execution(* *.play(int)) \" + \"&amp;&amp; args(songNum)\") public void trackPlayed(int songNum)&#123;&#125; @Before(\"trackPlayed(songNum)\") public void countSong(int songNum) &#123; int currentCount = getPlayCount(songNum); trackCounts.put(songNum,currentCount + 1); System.out.println(\"该歌曲播放过\" + currentCount + \"次\\n\"); &#125; public int getPlayCount(int songNum) &#123; return trackCounts.containsKey(songNum)?trackCounts.get(songNum):0; &#125;&#125; 配置类 123456789101112131415161718@Configuration@EnableAspectJAutoProxy/*启动AspectJ自动代理*/@ComponentScanpublic class ConcertConfig&#123; // 声明Audience bean @Bean public Audience audience() &#123; return new Audience(); &#125; @Bean public PlayCounter playCounter() &#123; return new PlayCounter(); &#125;&#125; 测试 12345678@Testpublic void aopArgsTest()&#123; for (int i = 0; i &lt; 10; i++) &#123; cdPlayer.play(new Random().nextInt(3)); &#125;&#125; 输出 1234567891011121314151617181920212223242526272829该歌曲播放过0次正在播放CD磁盘中第2首歌曲该歌曲播放过1次正在播放CD磁盘中第2首歌曲该歌曲播放过0次正在播放CD磁盘中第0首歌曲该歌曲播放过2次正在播放CD磁盘中第2首歌曲该歌曲播放过3次正在播放CD磁盘中第2首歌曲该歌曲播放过1次正在播放CD磁盘中第0首歌曲该歌曲播放过2次正在播放CD磁盘中第0首歌曲该歌曲播放过4次正在播放CD磁盘中第2首歌曲该歌曲播放过3次正在播放CD磁盘中第0首歌曲该歌曲播放过0次正在播放CD磁盘中第1首歌曲 4.4 在XML中声明切面当你需要声明切面，但又不能为通知类添加注解的时候，那么就必须转向XML配置了。 AOP配置元素 用途 定义AOP通知器 定义AOP后置通知（不管被通知的方法是否执行成功） 定义AOP返回通知 定义AOP异常通知 定义AOP环绕通知 定义一个切面 启动@AspectJ注解驱动的切面 定义一个AOP前置通知 顶层的AOP配置元素。大多数的元素必须包含在元素内 以透明的方式呗通知的对象引入额外的接口 定义一个切点 将切面的注解去掉 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Componentpublic class Audience&#123; // 定义命名的切点 //@Pointcut(\"execution(* *.perform(..))\") //public void performance() //&#123;&#125; //@Around((\"performance()\")) public void watchPerformance(ProceedingJoinPoint joinPoint) &#123; try &#123; System.out.println(\"环绕-手机静音\"); System.out.println(\"环绕-就坐\"); joinPoint.proceed(); System.out.println(\"环绕-牛b牛b!再来一个!\"); &#125; catch (Throwable throwable) &#123; System.out.println(\"环绕-垃圾!退钱!\"); &#125; &#125; //@Before(\"performance()\") public void silenceCellPhones() &#123; System.out.println(\"观众们请关闭手机铃声...\"); &#125; //@Before(\"performance()\") public void takSeats() &#123; System.out.println(\"大家请坐...\"); &#125; //@AfterReturning(\"performance()\") public void applause() &#123; System.out.println(\"大家请鼓掌...\"); &#125; //@AfterThrowing(\"performance()\") public void deamanRefund() &#123; System.out.println(\"表演失败了,大家鼓励一下...\"); &#125;&#125; 配置xml 12345678910111213141516171819202122232425262728293031&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xsi:schemaLocation=\"http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-3.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd\"&gt; &lt;!--配置自动扫包--&gt; &lt;context:component-scan base-package=\"chapter04\"/&gt; &lt;!--启动AspectJ自动代理--&gt; &lt;aop:aspectj-autoproxy/&gt; &lt;aop:config&gt; &lt;!--切面--&gt; &lt;aop:aspect ref=\"audience\"&gt; &lt;!--切点--&gt; &lt;aop:pointcut id=\"dosth\" expression=\"execution(* *.perform(..))\"/&gt; &lt;!--前置通知--&gt; &lt;aop:before method=\"silenceCellPhones\" pointcut-ref=\"dosth\"/&gt; &lt;aop:before method=\"takSeats\" pointcut-ref=\"dosth\"/&gt; &lt;!--后置通知--&gt; &lt;aop:after-returning method=\"applause\" pointcut-ref=\"dosth\"/&gt; &lt;!--异常通知--&gt; &lt;aop:after-throwing method=\"deamanRefund\" pointcut-ref=\"dosth\"/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 注意 修改以下Main测试类的配置读取方式，将Java配置类改为配置文件配置。 12345@RunWith(SpringJUnit4ClassRunner.class)//@ContextConfiguration(classes = ConcertConfig.class)@ContextConfiguration(locations = \"classpath:spring/c4-applicationContext.xml\")public class Main&#123;...&#125; 5 小结本章讲解了通过切面类和配置文件配置实现Spring的AOP功能，使模块之间进行解耦，有效减少了代码冗余，让我们只关注类自身的功能。通过这几章我们学习了Spring和核心功能，DI和AOP。但是我们现在只停留在了会用的阶段，要深入其原理还需要看看源码，下一章开始学习SpringMVC。开始构建真正的Web应用。","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"},{"name":"Spring","slug":"Spring","permalink":"https://seawaylee.github.io/tags/Spring/"}]},{"title":"SpringInAction读书笔记(三) 高级装备","date":"2017-03-09T15:34:08.000Z","path":"2017/03/09/JavaWeb/Spring/Spring_3/","text":"3.1 环境与Profile一般情况下，开发环境所需要的spring配置文件，例如dataSource数据源肯定与生产环境是不一样的，开发环境到生产环境或测试换几个时，我们都需要手动的去替换大量配置文件，非常麻烦，Spring的Profile为我们解决了这个问题，通过激活不同的profile来控制创建哪些bean。 下面举一个最简单的例子创建两个不同的类，假设他们是不同的配置文件 123456789101112131415public class Animal&#123; public Animal() &#123; System.out.println(\"动物对象被创建了...\"); &#125;&#125;public class Student&#123; public Student() &#123; System.out.println(\"学生对象被实例化了\"); &#125;&#125; Spring配置文件 123456789101112131415&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-3.2.xsd\"&gt; &lt;beans profile=\"dev\"&gt; &lt;bean id=\"object\" class=\"chapter03.Student\"/&gt; &lt;/beans&gt; &lt;beans profile=\"prod\"&gt; &lt;bean id=\"object\" class=\"chapter03.Animal\"/&gt; &lt;/beans&gt;&lt;/beans&gt; 指定profile进行测试 1234567891011@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:spring/c3-applicationContext.xml\")@ActiveProfiles(\"prod\")public class Main&#123; @Test public void profileTest() &#123; System.out.println(\"测试\"); &#125;&#125; 输出动物对象被创建了... 测试 修改 1@ActiveProfiles(\"dev\") 输出学生对象被实例化了 测试在web开发中，我们可以使用以下方式切换和设置profile需要依赖两个独立的属性：spring.profiles.active spring.profiles.default如果设置了active，指定的profile会被激活。如果没有设置active，指定的default会被激活。如果都没有设置，只会创建没有在profile范围内的bean。有多钟方式来设置这两个属性： 作为DispatcherServlet的初始化参数； 作为Web应用的上下文参数； 作为JNDI条目； 作为环境变量； 作为JVM的系统属性； 在集成测试类上，使用@ActiveProfiles注解设置； 3.2 条件化的bean如果有这种需求：只有某个特定的环境变量设置之后，才会创建某个bean。那么我们就需要这个功能。一般情况下不会用到，因此不做详细描述。 1234567891011121314151617181920212223242526public class MagicBeanFactory&#123; @Bean @Conditional(MagicExistsCondition.class) public MagicBean magicBean() &#123; return new MagicBean(); &#125;&#125;class MagicBean&#123; public MagicBean() &#123; System.out.println(\"MagicBean 被初始化了...\"); &#125;&#125;class MagicExistsCondition implements Condition&#123; public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) &#123; Environment env = conditionContext.getEnvironment(); return env.containsProperty(\"magic\"); &#125;&#125; 3.3 处理自动装配的歧义性假设有一个Animal接口，Dog和Cat都是Animal接口的实现类，当我们使用@Autowired注入Animal时，Spring会因为不知道注入Dog还是Cat而报错。 解决方案： 123public interface Animal&#123;&#125; 123456@Component//@Qualifier(\"cat\")@Primarypublic class Cat implements Animal&#123;&#125; 123456@Component//@Qualifier(\"dog\")//@Primarypublic class Dog implements Animal&#123;&#125; 3.4 运行时值注入12345678910111213@Component@PropertySource(\"classpath:db.properties\")public class MyProperties&#123; @Autowired Environment env; public void show() &#123; System.out.println(env.getProperty(\"username\") + \",\" + env.getProperty(\"password\")); &#125;&#125; 123456789101112131415161718192021@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = \"classpath:spring/c3-applicationContext.xml\")//@ActivePcrofiles(\"dev\")public class Main&#123; @Autowired MagicBeanFactory factory; @Autowired @Qualifier(\"dog\") Animal animal; @Autowired private MyProperties properties; @Test public void propertyTest() &#123; properties.show(); &#125;&#125; 3.5 小结本章是一些装配Bean的高级特性。包括使用profile初始化不同的beans；在spring4中使用@Conditional更加灵活细化的按条件初始化bean；使用@Qualifier解决@Autowired的歧义性；使用@PropertySource等一些列注解读取配置文件；其实还有SpEl表达式，用来动态的初始化bean，本文没有做讲解，在以后学习Spring-Security的时候会详细说明。","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"},{"name":"Spring","slug":"Spring","permalink":"https://seawaylee.github.io/tags/Spring/"}]},{"title":"SpringInAction读书笔记(二) 装备Bean","date":"2017-03-09T14:17:00.000Z","path":"2017/03/09/JavaWeb/Spring/Spring_2/","text":"2.1 Sprig装配Bean的方式Spring装配Bean的方案有三种，任选其一。 在XML中进行显示配置 在Java中进行显示配置 隐式的bean发现机制和自动装配 自动化装配BeanSpring从两个角度来实现自动化装备： 组件扫描component scanning：Spring自动发现应用上下文所创建的bean 自动装配autowiring：Spring自动满足bean之间的依赖 2.2 Spring自动扫描Bean我们不使用上一章那种XML的方式将bean配置，而是采用基于Java的配置方法。 创建一个接口 1234public interface CompactDisc&#123; void play();&#125; 创建实现类，@Component注解将该类标识为由Spring管理的组件 1234567891011@Componentpublic class Jay implements CompactDisc&#123; private String title = &quot;七里香&quot;; private String artist = &quot;周杰伦&quot;; public void play() &#123; System.out.println(&quot;播放器正在播放 &quot; + title + &quot;-&quot; + artist); &#125;&#125; 创建Spring的Java配置类，其中ComponentScan默认扫描当前包下的所有被标示为@Component（其实还包括@Server、@Repository等等），ComponentScan可以配置扫描路径，如果不写就只扫描当前包。 12345@Configuration@ComponentScanpublic class CDPlayerConfig&#123;&#125; 如果不采用上面的Java配置方法，我们可以使用XML配置要扫描的路径。 12&lt;!--自动扫描--&gt;&lt;context:component-scan base-package=&quot;chapter02&quot;/&gt; 然后我们就可以使用测试一下是否注入成功了有两种读取配置的方式可以选择，被注释的部分是读取Java配置类，后者是读取xml配置文件。 1234567891011121314@RunWith(SpringJUnit4ClassRunner.class)//@ContextConfiguration(classes = CDPlayerConfig.class)@ContextConfiguration(locations = &quot;classpath:spring/applicationContext.xml&quot;)public class CDPlyaerTest&#123; @Autowired private CompactDisc cd; @Test public void componentTest() &#123; cd.play(); &#125;&#125; 输出播放器正在播放 七里香-周杰伦 2.3 Spring自动装配Bean@Autowired自动装配注解可以用在类的属性、setter方法、其他方法上。不过一般来说我们都是用在属性上，如上面的装配CD的例子。当然，如果不愿意使用Spring特有的@Autowired来注解，也可以使用@Inject（Java依赖注入规范），来注解。在我们声明类为组件时使用@Component来注解，也可以使用Java提供的@Named来注解。推荐使用Spring特有的注解。","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"},{"name":"Spring","slug":"Spring","permalink":"https://seawaylee.github.io/tags/Spring/"}]},{"title":"SpringInAction读书笔记(一) Spring之旅","date":"2017-03-09T14:10:00.000Z","path":"2017/03/09/JavaWeb/Spring/Spring_1/","text":"本章简要介绍一下Spring的核心模块：依赖注入、切面。并介绍一下Spring的模块组件，对Spring有一个简要的了解。在后续章节将详细学习Spring的DI、AOP等核心组件。 1.依赖注入（DI）传统的代码编写方式，例如一个Student类中创建了Arm、Leg、Body等对象，那么我们需要创建Student对象时，他就会按照类中写死的那样去创建固定类型的Arm（麒麟臂）、Leg（飞毛腿）、Body（大胸肌）这些对象，如果我们想动态的给这个Student换零件儿的话就要重写或新增一个其他类型的Student。但是有了依赖注入，我们可以在创建Student对象时动态的给他注入各种类型的子对象。这样就降低了代码耦合度，这也是依赖注入所带来的最大收益。 1234567public class SmallArm&#123; public SmallArm() &#123; System.out.println(\"细小的手臂被创建了...\"); &#125;&#125; 123456789101112131415public class StudentObj&#123; private Object arm; public StudentObj() &#123; this.arm = new SmallArm(); // 写死了手臂类型,对象之间紧密耦合 &#125; public void show() &#123; System.out.println(\"我是一个小浪仔..\"); &#125;&#125; 1234public static void main(String[] args)&#123; StudentObj stu = new StudentObj();&#125; 输出细小的手臂被创建了... 使用构造器依赖注入Arm我们给Student新增一个带参数的构造器，以让Spring对其进行注入Arm。 1234public StudentObj(Object arm)&#123; this.arm = arm;&#125; 通过Maven引入Spring相关jar，或直接往项目里导入jar。就不写了。 创建Spring配置文件 1234567891011121314151617&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-3.2.xsd\"&gt; &lt;bean id=\"student\" class=\"cn.edu.ncut.pojo.StudentObj\"&gt; &lt;constructor-arg ref=\"arm\"/&gt; &lt;/bean&gt; &lt;bean id=\"arm\" class=\"cn.edu.ncut.pojo.SuperArm\"&gt; &lt;/bean&gt;&lt;/beans&gt; 读取Spring配置文件并获取Student对象 12345public static void main(String[] args)&#123; ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext(\"classpath:spring/applicationContext.xml\"); ctx.getBean(StudentObj.class);&#125; 输出麒麟臂被创建了... 就此最简单的依赖注入Demo就完成了，简单吧。记住依赖注入最大的收益：让相互协作的组件保持松散耦合。 2.切面（AOP）切面有什么用呢？加入我们有一个一一大堆查询、插入、修改等函数，需求是需要在所有插入方法执行前先记录日志，传统的方法时在每一个insert函数的第一行去写这样的日志存储，就算聪明的你写一个工具类，最终在insert函数中以一行代码的方式完成了这个任务，但当这个工具方法还是会重复出现在各个inser函数中，很low好不好？而且类似日志记录、公共逻辑校验这样与业务无关的代码会让函数变得会乱，我们要做的就是在业务函数中只关心业务代码，其余的这些什么日志操作，别来烦老子。我们就用Spring的AOP来解决这个问题。传统的LowB案例建立一个公共方法类 1234567891011public class CommonUtils&#123; public static void beforeInsert() &#123; System.out.println(\"我是公共方法,我要在所有的插入操作前调用...\"); &#125; public static void afterInsert() &#123; System.out.println(\"我是公共方法,我要在所有的插入操作后调用...\"); &#125;&#125; Student类中新增这些方法，我们能完成任务了。很累啊有木有 123456789101112public void insertA()&#123; CommonUtils.beforeInsert(); System.out.println(\"我是Student.insertA()\"); CommonUtils.afterInsert();&#125;public void insertB()&#123; CommonUtils.beforeInsert(); System.out.println(\"我是Student.insertB()\"); CommonUtils.afterInsert();&#125; 主函数 12345678public static void main(String[] args)&#123; ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext(\"classpath:spring/applicationContext.xml\"); StudentObj student = ctx.getBean(StudentObj.class); student.insertA(); student.insertB();&#125; 输出麒麟臂被创建了... 我是公共方法,我要在所有的插入操作前调用... 我是Student.insertA() 我是公共方法,我要在所有的插入操作后调用... 我是公共方法,我要在所有的插入操作前调用... 我是Student.insertB() 我是公共方法,我要在所有的插入操作后调用... AOP最简单的例子Spring配置文件 123456789101112131415161718&lt;!--依赖注入--&gt;&lt;bean id=\"student\" class=\"cn.edu.ncut.pojo.StudentObj\"&gt; &lt;constructor-arg ref=\"arm\"/&gt;&lt;/bean&gt;&lt;bean id=\"arm\" class=\"cn.edu.ncut.pojo.SuperArm\"/&gt;&lt;bean id=\"commonUtils\" class=\"cn.edu.ncut.pojo.CommonUtils\"/&gt;&lt;!--AOP--&gt;&lt;aop:config&gt; &lt;aop:aspect ref=\"commonUtils\"&gt; &lt;!--定义切点--&gt; &lt;aop:pointcut id=\"showSomething\" expression=\"execution(* *.insert*(..))\" /&gt; &lt;!--定义前置通知--&gt; &lt;aop:before pointcut-ref=\"showSomething\" method=\"beforeInsert\"/&gt; &lt;!--定义后置通知--&gt; &lt;aop:after pointcut-ref=\"showSomething\" method=\"afterInsert\"/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; Student的插入方法工具代码已经去掉了 12345678public void insertA()&#123; System.out.println(\"我是Student.insertA()\");&#125;public void insertB()&#123; System.out.println(\"我是Student.insertB()\");&#125; 同样执行Main函数输出麒麟臂被创建了... 我是公共方法,我要在所有的插入操作前调用... 我是Student.insertA() 我是公共方法,我要在所有的插入操作后调用... 我是公共方法,我要在所有的插入操作前调用... 我是Student.insertB() 我是公共方法,我要在所有的插入操作后调用... 3.Spring容器3.1 应用上下文Spring自带了多种类型的应用上下文，我们最常使用的应用上下文是ClassPathXmlApplicationContext，它将从应用的类路径下加载上下文，以初始化Spring容器。 1ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext(\"classpath:spring/applicationContext.xml\"); 3.2 bean的生命周期传统的Java应用，我们创建对象使用new关键字，不用了以后由JVM的GC机制自动回收对象。简单的一匹。但是在Spring中，对象的生命周期就复杂很多了，如以下步骤： 1.Spring实例化bean 2.Spring将bean的引用注入到bean对应的属性中 3.如果bean实现了BeanNameAware接口，Spring将bean的ID传递给setBeanName()方法 4.如果bean实现了BeanFactoryAware接口，Spring将调用setBeanFactory()方法，将BeanFactory容器实例传入 5.如果bean…ApplicationContextAware…setApplicationContext 6….BeanPostProcessor…postProcessBeforeInitialization()… 7….InitializingBean…afterPropertiesSet()… 8….BeanPostProcessor…postProcessAfterInitialization() 9.此时bean已经准备就绪，可以被应用程序使用了，bean将一直存在应用上下文中，直到该应用上线文被销毁。 10….DisposableBean…destroy()…destroy-method()… 4.Spring模块Spring4.0的框架发布版本包括了20个不同的模块，每个模块会有3个JAR（二进制类库、源码的Jar、JavaDoc的Jar）这些模块一句其所属的功能可以划分为6类不同的功能。 Spring核心容器容器是Spring框架最核心的部分，管理Spring应用中bean的创建、配置、管理。SpringBean工厂、DI、E-mail、JNDI、EJB都与之相关。 SpringAOP模块面向切面开发的基础。 数据访问与集成抽象JDBC的样板化代码-&gt;JdbcTelmplate。构建数据库服务的错误信息为一个异常层。集成ORM框架-&gt;Hiberna、JPA、JDO、iBatis。基于JMS（JavaMessageService）的抽象层。使用AOP模块为Spring应用中的对象提供事务管理服务。 Web与远程调用MVC的基础。集成RMI（RemoteMethodInvocation）、Hessian、Burlap、JAX-WS。REST-API。 5.小结本章知识对Spring的内容有一个初步的认识，第二章将深入学习Spring装备Bean的内容。","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"},{"name":"Spring","slug":"Spring","permalink":"https://seawaylee.github.io/tags/Spring/"}]},{"title":"Oracle数据库运维、备份常用指令","date":"2017-03-09T14:04:00.000Z","path":"2017/03/09/Oracle/Oracle_1/","text":"1. Oracle数据泵备份导出1.1 准备工作在linux系统下创建导出结果存放的文件夹，切记要切换到oracle用户创建，否则会出现权限问题。 12su - oraclemkdir /home/oracle/dwcr_dumps 如果使用root创建了文件夹，可是使用命令来修改文件权限。 1chown /home/oracle/dumps oralce:oinstall 连接oracle，创建导出映射地址、用户授权先查看当前实例是不是你需要的 1echo $ORACLE_SID 如果不是，修改一下 1export ORACLE_SID=dwcr 进入oracle命令环境,使用管理员权限登陆，不需要密码. 1sqlplus / as sysdba Oracle参数举例: Key Value 用户名 dwcr 密码 password 表空间名 dwcr_tablespace 实例名 dwcrsid 导出路径 /home/oracle/dwcr_dumps 数据库IP 192.168.1.1100 创建导出地址的映射（内部别名-&gt;Linux文件系统） 1create or replace directory DWCRDUMP as '/home/oracle/dwcr_dumps'; 给用户授权 1grant read,write on directory DWCRDUMP to dwcr; 数据泵导出退出oracle命令环境后1expdp dwcr/dwcr@192.168.1.1100:1521/dwcrsid DIRECTORY=DWCRDUMP DUMPFILE=dwcr_20161206_1446.dmp LOGFILE=dwcr_20161206_1446.log SCHEMAS=dwcr 假如想从11g的数据库导出，并导入到10g的数据库，需要在结尾加一个参数 1version=10.2.0.1.0 等待导出正常结束后，就可以去linux绝对路径下拷贝dmp文件了。如果导出期间出现异常，根据ORA的错误提示，自行百度，无外乎就是那几种常见错误。 数据泵导入注意:也需要创建文件映射、对用户授权，需要先将dmp文件放到指定的linux路径中供Oracle读取。1impdp dwcr/dwcr@192.168.1.1200:1521/dwcrsid directory=DWCRDUMP schemas=dwcr dumpfile=dwcr_20161206_1446.dmp 2. 使用Python脚本导出/定时备份2.1 安装Python1、在官方网站下载python安装包，这里注意python.org/download路径被屏蔽，需要使用http://www.python.org/页面上的中文“下载”链接进行下载。这里下载了python最新的3.2.2版本：Python-3.2.2.tgz下载后，文件目录在/home/python/下，这也是我python的安装目录 2、解压： 123456789101112131415161718[oot@www python]# tar zxvf Python-3.2.2.tgz3、打开安装目录，执行：[root@www python]# cd Python-3.2.2[root@www Python-3.2.2]#./configure[root@www Python-3.2.2]# make[root@www Python-3.2.2]# makeinstall值此，安装完成。4、但此时输入”python”命令，仍然显示是旧版本的，这就需要创建软连接：[root@www bin]# cd /usr/bin[root@www bin]# ll |grep python[root@www bin]# rm -rf python[root@www bin]# ln -s /home/python/Python-3.2.2/python python[root@www bin]# pythonPython 3.2.2 (default, Oct 26 2011, 23:40:16)[GCC 4.1.2 20071124 (Red Hat 4.1.2-42)] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or&quot;license&quot; for more information. 看一下 如果出现了以上提示信息就说明安装成功了 按ctrl+z退出来就好了。 2.2 批量导出数据库脚本的编写前阵子有个需求，需要一个人对10+台Oracle进行备份，每个数据库的信息又不相同，因此编写了一个简单的python导出脚本来完成这个工作。呵呵，酸爽。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import osimport timeimport schedISOTIMEFORMAT = '%Y%m%d%'IP_ADDR = [ '192.168.1.169', '192.168.1.172', '192.168.1.173', '192.168.1.167', '192.168.1.168', '192.168.1.168', '192.168.1.170', '192.168.1.170', '192.168.1.170', '192.168.1.170', '192.168.1.171', '10.161.8192.168.16.71', '192.168.1.171']SID = [ 'dwcr', 'dwcrdci', 'dwcrweb', 'ISRC2', 'cpcc', 'cpcc', 'orcl', 'orcl', 'orcl2', 'orcl2', 'classcas', 'copyrigh', 'custody']USERNAME = [ 'dwcr1', 'dwcrdci', 'dwcrweb', 'isrc2', 'cpccfs_develop', 'cpccfs_opus', 'cac_develop', 'cac_opus', 'cac_develop', 'cac_opus', 'lab1107', 'lab1107', 'custody2']PASSWORD = [ 'dwcr1', 'dwcrdci', 'dwcrweb', 'isrc2', 'cpccfs', 'cpccfs', 'cac', 'cac', 'cac_develop_7112', 'cac_opus_7112', '237133', '237133', 'custody2']TABLESPACE_NAME = [ 'dwcr_dumps', 'dwcrdci_dumps', 'dwcrweb_dumps', 'isrc2', 'cpccfs_develop', 'cpccfs_opus', 'softnew', 'opusnew', 'softold', 'opusold', 'classcas', 'copyrigh', 'custody2']DUMP_FILENAME = [ 'DWCRDUMP', 'DWCRDCIDUMP', 'DWCRWEBDUMP', 'DP_ISRC2', 'CPCCFS_BACKUP', 'CPCCFS_BACKUP', 'DAILI_NEW', 'DAILI_NEW', 'DAILI_OLD', 'DAILI_OLD', 'THREEDUMP', 'THREEDUMP', 'THREEDUMP']if __name__ == \"__main__\": now_time = time.strftime(\"%Y%m%d%H%M\") print(\"当前系统时间:\", now_time) for i in range(IP_ADDR.__len__()): ip_addr = IP_ADDR[i] sid = SID[i] username = USERNAME[i] password = PASSWORD[i] tablespace_name = TABLESPACE_NAME[i] dump_filename = DUMP_FILENAME[i] # DWCR Keep 10 days file if sid == 'dwcr': dir_path = \"/home/oracle/dwcr_dumps\" all_dmpfiles = os.listdir(dir_path) all_dmpfiles.sort() print(all_dmpfiles) if all_dmpfiles.__len__() &gt; 20: del_file_num = all_dmpfiles.__len__() - 20 print(\"即将删除10天前的文件:\", del_file_num, \" 个\") for file in all_dmpfiles[:del_file_num]: print('rm -f %s' % dir_path + \"/\" + file) os.system('rm %s' % dir_path + \"/\" + file) # Export the new dmp file # expdp dwcr1/dwcr1@192.168.1.1105:1521/dwcr DIRECTORY=DWCRDUMP DUMPFILE=dwcr1_20161127.dmp LOGFILE=dwcr1_20161127.log SCHEMAS=dwcr1 print(\"即将泵出\", ip_addr, sid, username, tablespace_name, dump_filename) excmd = 'expdp %s/%s@%s:1521/%s DIRECTORY=%s DUMPFILE=%s_%s.dmp LOGFILE=%s_%s.log SCHEMAS=%s' % \\ (username, password, ip_addr, sid, dump_filename, tablespace_name, now_time, tablespace_name, now_time, username) print(excmd) result = os.system(excmd) print(\"执行结果:\", result) os.system('exit') print(\"计划内导出数据库文件的服务器IP列表为:\") for i in range(IP_ADDR.__len__()): print(i + 1, \"-&gt;\", IP_ADDR[i],\"-&gt;\",\"/home/oralce/\"+TABLESPACE_NAME[i]) print(\"请分别到各数据库服务器拷贝文件:\", '*' + now_time + \"*\\n\") print('''计划外导出数据库文件的服务器IP列表为: IP:192.168.1.166 -&gt; /home/oracle/autobackup/(当天夜里3点的文件) IP:192.168.1.13 -&gt; navicat/sqlyog连接 root/1Q2w3e4r@ 导出sql文件 ''') print('===========结束==========') 其中一下部分代码可以删除，因为有的数据库文件特别大，我每次又都懒得手动删除，因此写了这一段代码，将导出的文件按时间排序，删除超过n天的导出文件 1234567891011if sid == 'dwcr': dir_path = \"/home/oracle/dwcr_dumps\" all_dmpfiles = os.listdir(dir_path) all_dmpfiles.sort() print(all_dmpfiles) if all_dmpfiles.__len__() &gt; 20: del_file_num = all_dmpfiles.__len__() - 20 print(\"即将删除10天前的文件:\", del_file_num, \" 个\") for file in all_dmpfiles[:del_file_num]: print('rm -f %s' % dir_path + \"/\" + file) os.system('rm %s' % dir_path + \"/\" + file) 将脚本拷贝到/home/oracle下执行方法： 123su - oraclecd /home/oracle/python _export.py Lift is short , I need python.喝杯咖啡等着去吧~ 2.3 全自动定时数据库备份什么？你还不满意？不想天天去机房备份？yo~满足你 定时器版python备份脚本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import osimport timeimport schedISOTIMEFORMAT = '%Y%m%d%'IP_ADDR = ['192.168.1.169:1521', '192.168.1.172:1521', '192.168.1.173:1521']SID = ['dwcr', 'dwcrdci', 'dwcrweb']USERNAME = ['dwcr1', 'dwcrdci', 'dwcrweb']TABLESPACE_NAME = ['dwcr1', 'dwcrdci', 'dwcrweb']DUMP_FILENAME = ['DWCRDUMP', 'DWCRDCIDUMP', 'DWCRWEBDUMP']DIR_PATH = ['dwcr_dumps', 'dwcrdci_dumps', 'dwcrweb_dumps']SLEEP_TIME = 60 * 60 * 24def export_Oracle(): now_time = time.strftime(\"%Y%m%d%H%M\") print(\"当前系统时间:\", now_time) for i in range(3): ip_addr = IP_ADDR[i] sid = SID[i] username = USERNAME[i] tablespace_name = TABLESPACE_NAME[i] dump_filename = DUMP_FILENAME[i] # Keep 10 days file dir_path = \"/home/oracle/\" + DIR_PATH[i] all_dmpfiles = os.listdir(dir_path) all_dmpfiles.sort() print(all_dmpfiles) if all_dmpfiles.__len__() &gt; 20: del_file_num = all_dmpfiles.__len__() - 20 print(\"即将删除10天前的文件:\", del_file_num, \" 个\") for file in all_dmpfiles[:del_file_num]: print('rm -f %s' % dir_path + \"/\" + file) os.system('rm %s' % dir_path + \"/\" + file) # Export the new dmp file # expdp dwcr1/dwcr1@192.168.1.105:1521/dwcr DIRECTORY=DWCRDUMP DUMPFILE=dwcr1_20161127.dmp LOGFILE=dwcr1_20161127.log SCHEMAS=dwcr1 print(\"即将泵出\", ip_addr, sid, username, tablespace_name, dump_filename) excmd = 'expdp %s/%s@%s/%s DIRECTORY=%s DUMPFILE=%s_%s.dmp LOGFILE=%s_%s.log SCHEMAS=%s' % \\ (username, username, ip_addr, sid, dump_filename, tablespace_name, now_time, tablespace_name, now_time, username) print(excmd) result = os.system(excmd) print(\"执行结果:\", result) os.system('exit') print('===========结束==========')def run_function(): s = sched.scheduler(time.time, time.sleep) # 设置一个调度,因为time.sleep()的时间是一秒,所以timer的间隔时间就是sleep的时间,加上enter的第一个参数 s.enter(SLEEP_TIME, 2, export_Oracle) s.run()def timer(): while True: run_function()if __name__ == \"__main__\": timer() 其中以下这部分代码用来控制自动备份间隔时间，假如你今天早晨8点执行了脚本，备份会立即执行一次，然后下次执行就是明天早晨的8点了。自己按需配置吧。脚本几乎和之前的一样，只是套了一层定时器而已。 1SLEEP_TIME = 60 * 60 * 24 执行方法：其中 –fork 是将该进程设置为守护进程，防止系统误杀。1python /home/oracle/time_export.py --fork 3 常用Oracle语句首先进入Oralce命令环境 1sqlplus / as sysdba 3.1 创建表空间12345678create tablespace dwcr1loggingdatafile '/DWCR_DBV2/dwcr/dwcr1_01.dbf'size 800mautoextend onnext 100mmaxsize unlimitedextent management local; 3.2 创建用户、给用户分配表空间12create user dwcrdci identified by dwcrdcidefault tablespace dwcrdci; 3.3 给用户授权1grant dba,resource,connect to dwcrdci; 4 常用Oracle命令1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 查看dump文件映射select * from dba_directories;# 查看所有用户select * from dba_users;# 查看所有表空间select * from dba_tablespaces;# 查看用户的默认表空间select username, DEFAULT_TABLESPACE from dba_users where username='dwcr1';# 查看表空间所有文件路径select * from dba_data_files where tablespace_name='dwcr1';# 查看表在哪个表空间下select tablespace_name,table_name from user_talbes where table_name='td_product';# 查看表空间使用情况SELECT tbs 表空间名, sum(totalM) 总共大小M, sum(usedM) 已使用空间M, sum(remainedM) 剩余空间M, sum(usedM)/sum(totalM)*100 已使用百分比, sum(remainedM)/sum(totalM)*100 剩余百分比 FROM( SELECT b.file_id ID, b.tablespace_name tbs, b.file_name name, b.bytes/1024/1024 totalM, (b.bytes-sum(nvl(a.bytes,0)))/1024/1024 usedM, sum(nvl(a.bytes,0)/1024/1024) remainedM, sum(nvl(a.bytes,0)/(b.bytes)*100), (100 - (sum(nvl(a.bytes,0))/(b.bytes)*100)) FROM dba_free_space a,dba_data_files b WHERE a.file_id = b.file_id GROUP BY b.tablespace_name,b.file_name,b.file_id,b.bytes ORDER BY b.tablespace_name ) GROUP BY tbs # 重新调整表空间大小alter database datafile 'D:\\Oracle\\PRODUCT\\ORADATA\\TEST\\USERS01.DBF' resize 1000m;# 设置自动扩展大小alter database datafile 'D:\\ORACLE\\PRODUCT\\ORADATA\\TEST\\USERS01.DBF' autoextend on next 200m maxsize unlimited;# 增加数据文件alter tablespace yourtablespacename add datafile 'd:\\newtablespacefile_02.dbf' size 800m;# 查看和修改用户默认表空间alter user dwcr1 default tablespace dwcr1 temporary tablespace temp;alter user dwcr1 TEMPORARY TABLESPACE DWCR_TEMP;# 修改数据库默认表空间alter database default temporary tablespace temp;# 查看数据库最大连接数select value from v$parameter where name ='processes' # 查看当前连接数select count(*) from v$process;# 修改最大连接数alter system set processes = 300 scope =spfile;# 查看当前并发量select count(*) from v$session where status='ACTIVE'# 查看参数show parameter processes ;# 查看当前session连接数select count(*) from v$session# 查看哪些用户正在使用数据库SELECT osuser, a.username,cpu_time/executions/1000000||'s',b.sql_text,machinefrom v$session a, v$sqlarea bwhere a.sql_address =b.address order by cpu_time/executions desc;# 批量杀掉连接SELECT 'alter system kill session ' || '''' ||t.sid ||','||t.SERIAL#|| '''' FROM v$session t WHERE t.USERNAME='DWCR1';# 删除用户drop user dwcr cascade;# 彻底删除表空间(包括内容,文件)drop tablespace DWCR including contents and datafiles; 转载请注明出处，谢谢。","tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://seawaylee.github.io/tags/Oracle/"}]},{"title":"Curl实现Get、Post","date":"2017-03-09T14:01:00.000Z","path":"2017/03/09/Linux/Curl/","text":"Curl是Linux下一个很强大的http命令行工具，其功能十分强大。 一 CURL对HTTP的常规访问1. 访问网站$ curl http://www.linuxidc.com 回车之后，www.linuxidc.com 的html 显示在屏幕上了 2. 保存页面用curl option: -o $ curl -o page.html http://www.linuxidc.com 可以看到屏幕上出现一个下载页面进度指示，等到100%，就保存完成了。使用wget 也可以的~。 二 GET模式GET模式什么option都不用，只需要把变量写在url里面就可以了 例如： $ curl http://www.linuxidc.com/test.cgi?param1=nickwolfe&amp;param2=12345 三 POST模式使用 option -d， 例如: $ curl -d “param2=nickwolfe&amp;param2=12345” http://www.linuxidc.com/login.cgi 四 POST+Jsoncurl -l -H “Content-type: application/json” -X POST -d ‘{“phone”:”13521389587”,”password”:”test”}’ http://domain/apis/users.json","tags":[{"name":"Linux","slug":"Linux","permalink":"https://seawaylee.github.io/tags/Linux/"}]},{"title":"Redis学习（四） 数据安全与性能","date":"2017-03-09T13:50:00.000Z","path":"2017/03/09/Nosql/Redis/Redis_4/","text":"4.1 持久化选项持久化有两种方式： 快照 snapshot：将某一时刻的所有数据都写入到硬盘里面 只追加文件 append-only file：执行写命令时，将写命令复制到硬盘里面。 两种持久化方式可以同时或单独或都不使用。 基本命令 Snapshot save 60 1000 stop-writes-on-bgsave-error no rdbcompression yes dbfilename dump.rdb append-only file appendonly no appendsync everysec no-appendfsync-on-rewrite no auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb dir ./ 共享选项，决定了snapshot和AOF文件的保存位置。 4.1.1 快照持久化创建snapshot的方法 BGSAVE命令创建快照。Redis会调用fork来创建一个子进程来将快照写入硬盘。 SAVE命令创建快照，阻塞式写入硬盘。 save 60 1000 从Redis最近一次创建快照之后算起，“60秒内有1000次写入”条件满足时自动触发BGSAVE。如果有多个save配置，当任意一个配置满足时将触发BGSAVE。 Redis收到系统SHUTDOWN命令时，或标准TERM信号时，自动执行SAVE，阻塞所有客户端。完成后关闭服务器。 当一个Redis连接另一个Redis，并向对方发送SYNC命令开始复制时，如果主服务器目前没有BGSAVE执行，或并非刚刚完成BGSAVE，则主服务器自动执行BGSAVE。 4.1.2 AOF持久化AOP持久化会将被执行的写命令写到AOF文件的末尾，以此来记录数据发生的变化。 使用aooendonly yes配置选项打开AOF持久化 参数 always: 每个Redis写命令都同步到硬盘，严重降低性能。 everysec：每秒执行以此同步，显示地将多个写命令同步到硬盘 no：让操作系统来决定该合适进行同步 使用auto-aof-rewrite-percentage 100 配置AOF文件体积达到指定百分比时进行重写（子进程） 使用auto-aof-rewrite-min-size 64mb 配置AOF文件大于64MB时进行重写（子进程） 4.2 复制A：主服务器，B：从服务器 A:Server Start B:Server Start,Cli Start,slaveof IP(A) PORT(A) 主从复制时步骤 步骤 主服务器操作 从服务器操作 1 等待命令 连接主服务器，发送SYNC命令 2 执行BGSAVE，使用缓冲区记录BGSAVE之后执行的所有写命令 根据config来决定继续使用现有数据来返回给客户端请求，还是返回错误 3 BGSAVE执行完毕，向从服务器发送snapshot,使用缓冲区记录期间的写命令 丢弃旧数据，载入收到的snapshot文件 4 snapshot发送完毕，向从服务器发送缓冲区中的写命令 正常接受命令请求 5 缓冲区命令发送完毕，每执行一个写命令，就向从服务器发送相同的写命令 执行主服务器发来的缓冲区写命令，并从现在开始接收并执行主服务器传来的每个写命令 当需要更换主服务器时，可以直接将主服务器的dump文件复制到新的主服务器，从服务器重新指定新的主服务器。 4.3 Redis事务通过使用WATCH、MULTI/EXEC、UNWACH/DISCARD等命令，程序可以在执行某些重要操作的时候，通过确保自己正在使用的数据没有发生变化来避免数据出错。 一般执行流程如下 123456conn.pipeline()pipe.watch(item)pipe.multi()pipe.zadd()pipe.srem()pipe.execute() 4.4 非事务型流水线MULTI和EXEC可以让程序拥有事务性，有些情况下，我们不需要事务来将操作回滚，只是想减少应用和Redis之间的通讯次数。创建pipe的时候，传入False参数即可。 12345conn.pipeline(False)pipe.multi()pipe.zadd()pipe.srem()pipe.execute()","tags":[{"name":"Nosql","slug":"Nosql","permalink":"https://seawaylee.github.io/tags/Nosql/"},{"name":"Redis","slug":"Redis","permalink":"https://seawaylee.github.io/tags/Redis/"}]},{"title":"Redis学习（三）Java实现基于Jedis+Spring的通用工具类","date":"2017-03-09T13:49:00.000Z","path":"2017/03/09/Nosql/Redis/Redis_3/","text":"Spring整合Jedis Maven引入 1234567891011&lt;!--Spring Data Redis--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-redis&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-data-redis.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;$&#123;jedis.version&#125;&lt;/version&gt; &lt;/dependency&gt; Java配置类（也可以用XML） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374@Configuration@EnableCachingpublic class RedisCacheConfig extends CachingConfigurerSupport&#123; @Value(\"$&#123;redis_host&#125;\") String redisHost; @Value(\"$&#123;redis_port&#125;\") Integer redisPort; @Value(\"$&#123;redis_keynum&#125;\") Integer redisKeynum; Logger logger = LoggerFactory.getLogger(this.getClass().getSimpleName()); public RedisCacheConfig() &#123; System.out.println(\"Java配置类初始化\"); &#125; @Bean public JedisPoolConfig jedisPoolConfig() &#123; JedisPoolConfig jedisPoolConfig = new JedisPoolConfig(); jedisPoolConfig.setMaxWaitMillis(60 * 1000); jedisPoolConfig.setMaxTotal(1000); jedisPoolConfig.setMaxIdle(100); return jedisPoolConfig; &#125; @Bean public JedisPool jedisPool(JedisPoolConfig jedisPoolConfig) &#123; JedisPool jedisPool = new JedisPool(jedisPoolConfig,redisHost,redisPort,999999999); return jedisPool; &#125; @Bean public Jedis jedis(JedisPool jedisPool) &#123; Jedis jedis = jedisPool.getResource(); jedis.select(redisKeynum); return jedis; &#125; @Bean public JedisConnectionFactory redisConnectionFactory() &#123; JedisConnectionFactory factory = new JedisConnectionFactory(); System.out.println(redisHost + \":\" + redisPort + \"-\" + redisKeynum); logger.info(redisHost + \":\" + redisPort + \"-\" + redisKeynum); factory.setHostName(redisHost); factory.setPort(redisPort); factory.setDatabase(redisKeynum); return factory; &#125; @Bean public RedisTemplate&lt;String, String&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; RedisTemplate&lt;String,String&gt; template = new RedisTemplate&lt;&gt;(); template.setConnectionFactory(redisConnectionFactory); return template; &#125; @Bean public CacheManager cacheManager(RedisTemplate redisTemplate) &#123; RedisCacheManager cacheManager = new RedisCacheManager(redisTemplate); cacheManager.setDefaultExpiration(0);; return cacheManager; &#125;&#125; 封装Jedis工具类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176/** * Jedis工具类 * * @author NikoBelic * @create 21/01/2017 13:41 */@Componentpublic class JedisUtils&#123; @Autowired Jedis jedis; private JedisUtils() &#123; &#125; // ********************** String Oprations ************************* public String set(String key, String val) &#123; return jedis.set(key, val); &#125; public String get(String key) &#123; return jedis.get(key); &#125; public Long incrBy(String key, Integer increment) &#123; return jedis.incrBy(key, increment); &#125; public Long decrBy(String key, Integer decrement) &#123; return jedis.incrBy(key, decrement); &#125; // ********************** List Oprations ************************* /** * 从左侧推入元素 * @Author NikoBelic * @Date 21/01/2017 17:47 */ public &lt;T extends Serializable&gt; Long lPushObj(String key, T... obj) &#123; byte[] serializedObj; long count = 0; for (T t : obj) &#123; serializedObj = SerializationUtil.serialize(t); jedis.lpush(key.getBytes(),serializedObj); count++; &#125; return count; &#125; /** * 从列表左侧弹出数据 * @Author NikoBelic * @Date 21/01/2017 17:47 */ public &lt;T extends Serializable&gt; T lPop(String key) &#123; byte[] obj = jedis.lpop(key.getBytes()); return SerializationUtil.deserialize(obj); &#125; /** * 获取指定范围内的列表元素 * @Author NikoBelic * @Date 21/01/2017 17:47 */ public &lt;T extends Serializable&gt; List&lt;T&gt; lRange(String key,int from,int to) &#123; List&lt;byte[]&gt; byteList = jedis.lrange(key.getBytes(), from, to); List&lt;T&gt; objList = null; if (byteList.size() &gt; 0) &#123; objList = new ArrayList&lt;T&gt;(); for (byte[] obj : byteList) &#123; objList.add(SerializationUtil.deserialize(obj)); &#125; &#125; return objList; &#125; // ********************** Set Oprations ************************* /** * 向集合中添加元素 * @Author NikoBelic * @Date 21/01/2017 18:06 */ public &lt;T extends Serializable&gt; Long sAdd(String key, T... obj) &#123; byte[] serializedObj; long count = 0; for (T t : obj) &#123; serializedObj = SerializationUtil.serialize(t); jedis.sadd(key.getBytes(),serializedObj); count++; &#125; return count; &#125; /** * 返回集合中的所有元素 * @Author NikoBelic * @Date 21/01/2017 18:06 */ public &lt;T extends Serializable&gt; Set&lt;T&gt; sMembers(String key) &#123; Set&lt;byte[]&gt; byteList = jedis.smembers(key.getBytes()); Set&lt;T&gt; objList = null; if (byteList.size() &gt; 0) &#123; objList = new HashSet&lt;T&gt;(); for (byte[] obj : byteList) &#123; objList.add(SerializationUtil.deserialize(obj)); &#125; &#125; return objList; &#125; /** * 判断对象是否存在于集合中 * 注意:判断标准是列化后的字符串是否相同,即时不同的对象但序列化结果相同也将返回true * @Author NikoBelic * @Date 21/01/2017 18:04 */ public &lt;T extends Serializable&gt; Boolean sIsMember(String key, T obj) &#123; byte[] serializedObj = SerializationUtil.serialize(obj); return jedis.sismember(key.getBytes(),serializedObj); &#125; // ********************** Hash Oprations ************************* /** * 向哈希表存储键值对数据 * @Author NikoBelic * @Date 21/01/2017 18:41 */ public &lt;T extends Serializable&gt; String hmSet(String key, Map&lt;String,T&gt; map) &#123; Map&lt;byte[],byte[]&gt; objMap; if (map.size() &gt; 0) &#123; objMap = new HashMap&lt;&gt;(); for (Map.Entry&lt;String, T&gt; entry : map.entrySet()) &#123; objMap.put(entry.getKey().getBytes(), SerializationUtil.serialize(entry.getValue())); &#125; return jedis.hmset(key.getBytes(), objMap); &#125; return null; &#125; /** * 从Hash中取出键值对数据 * @Author NikoBelic * @Date 21/01/2017 18:41 */ public &lt;T extends Serializable&gt; List&lt;T&gt; hmGet(String key, String... fields) &#123; List&lt;T&gt; resObjs = new ArrayList&lt;T&gt;(); for (String field : fields) &#123; resObjs.add(SerializationUtil.deserialize(jedis.hget(key.getBytes(),field.getBytes()))); &#125; return resObjs; &#125; // ********************** ZSet Oprations *************************&#125; 测试方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &#123;\"classpath*:spring/applicationContext*.xml\"&#125;)public class TestRedis&#123; @Autowired private JedisUtils jedisUtils; @Test public void testString() &#123; System.out.println(jedisUtils.set(\"address\", \"ShangHai\")); System.out.println(jedisUtils.get(\"address\")); try &#123; System.out.println(jedisUtils.incrBy(\"address\", 100)); &#125; catch (Exception e) &#123; System.out.println(e.getMessage() + \"自增操作异常\"); &#125; jedisUtils.set(\"myint\", \"10\"); System.out.println(jedisUtils.incrBy(\"myint\", 100)); &#125; @Test public void testList() &#123; System.out.println(jedisUtils.lPushObj(\"strList\", \"NikoBelic\",\"Tom\",\"Helen\")); //System.out.println((String) jedisUtils.lPop(\"strList\")); //System.out.println((String) jedisUtils.lPop(\"strList\")); List&lt;String&gt; strList = jedisUtils.lRange(\"strList\", 0, -1); for (String s : strList) &#123; System.out.println(s); &#125; //System.out.println(jedisUtils.lPushObj(\"objList\", new User(\"NikoBelic\", 18), new User(\"Tom\", 15), new User(\"Marry\", 20))); //System.out.println((User) jedisUtils.lPop(\"objList\")); //System.out.println((User) jedisUtils.lPop(\"objList\")); //List&lt;User&gt; objList = jedisUtils.lRange(\"objList\", 0, -1); //for (User user : objList) //&#123; // System.out.println(user); //&#125; &#125; @Test public void testSet() &#123; //System.out.println(jedisUtils.sAdd(\"strSet\", \"NikoBelic\",\"Tom\",\"Helen\")); //Set&lt;String&gt; strList = jedisUtils.sMembers(\"strSet\"); //for (String s : strList) //&#123; // System.out.println(s); //&#125; //System.out.println(jedisUtils.sAdd(\"objSet\", new User(\"NikoBelic\", 18), new User(\"Tom\", 15), new User(\"Marry\", 20))); //Set&lt;User&gt; objList = jedisUtils.sMembers(\"objSet\"); //for (User user : objList) //&#123; // System.out.println(user); //&#125; //User myObj = new User(\"Exist Test\",20); //jedisUtils.sAdd(\"objSet\",myObj); System.out.println(jedisUtils.sIsMember(\"objSet\",new User(\"ExistTest\",20))); &#125; @Test public void testHash() &#123; //Map&lt;String,String&gt; map = new HashMap&lt;&gt;(); //map.put(\"A\",\"1\"); //map.put(\"B\",\"2\"); //map.put(\"C\",\"3\"); //System.out.println(jedisUtils.hmSet(\"strHash\",map)); //List&lt;String&gt; strs = jedisUtils.hmGet(\"strHash\", \"A\", \"B\", \"C\"); //for (String str : strs) //&#123; // System.out.println(str); //&#125; Map&lt;String,User&gt; map = new HashMap&lt;&gt;(); map.put(\"user1\",new User(\"Niko\",18)); map.put(\"user2\",new User(\"Tom\",20)); map.put(\"user3\",new User(\"Marry\",15)); System.out.println(jedisUtils.hmSet(\"objHash\",map)); List&lt;User&gt; userList = jedisUtils.hmGet(\"objHash\", \"user1\", \"user2\", \"user3\"); for (User user : userList) &#123; System.out.println(user); &#125; &#125;&#125;class User implements Serializable&#123; private String name; private int age; public User(String name, int age) &#123; this.name = name; this.age = age; &#125; @Override public String toString() &#123; return \"User&#123;\" + \"name='\" + name + '\\'' + \", age='\" + age + '\\'' + '&#125;'; &#125;&#125;","tags":[{"name":"Nosql","slug":"Nosql","permalink":"https://seawaylee.github.io/tags/Nosql/"},{"name":"Redis","slug":"Redis","permalink":"https://seawaylee.github.io/tags/Redis/"}]},{"title":"Redis学习（二）数据结构进阶","date":"2017-03-09T13:48:00.000Z","path":"2017/03/09/Nosql/Redis/Redis_2/","text":"2.1 字符串Redis处理自增自减的命令 命令 用例和描述 incr incr keyname 值+1 decr decr keyname 值-1 incrby incrby keyname amount 值+amount decrby decrby keyname amount 值-amount incrbyfloat incrbyfloat keyname amount 值+float类型的amount(after redis 2.6) 123456789101112131415161718192021222324127.0.0.1:6379[3]&gt; set mykey 1OK127.0.0.1:6379[3]&gt; get mykey&quot;1&quot;127.0.0.1:6379[3]&gt; incr mykey(integer) 2127.0.0.1:6379[3]&gt; incr mykey(integer) 3127.0.0.1:6379[3]&gt; get mykey&quot;3&quot;127.0.0.1:6379[3]&gt; set mystr abcOK127.0.0.1:6379[3]&gt; incr mystr(error) ERR value is not an integer or out of range127.0.0.1:6379[3]&gt; incrby mykey 100(integer) 103127.0.0.1:6379[3]&gt; decr mykey(integer) 102127.0.0.1:6379[3]&gt; decrby mykey 50(integer) 52127.0.0.1:6379[3]&gt; INCRBYFLOAT mykey 10&quot;62&quot;127.0.0.1:6379[3]&gt; get mykey&quot;62&quot; Redis处理子串和二进制位的命令 命令 用例和描述 append append keyname value 将值value追加到keyname当前值的末尾 getrange getrange keyname start end 截取字符串[start,end] setrange setrange keyname offset value 将从start开始的子串设置为给定值 getbit getbit keyname offset 将字节串看做是二进制位串，并返回位串中偏移量为offset的二进制位的值 setbit setbit keyname offset value 将二进制位串offset位置的值设置为value bitcount bitcount keyname [stard end] 统计二进制位串里面value=1的数量，如果给定了start和end，则统计该范围内的 bitop bitop operation destkey keyname [keyname…] 对一个或多个二进制位串执行包括 and、or、xor、not在内的人呢以一种按位运算操作(operation)，并将计算得到的记过保存在destkey中 123456789101112127.0.0.1:6379[3]&gt; set mystr &apos;Hello!! My name is NikoBelic&apos;OK127.0.0.1:6379[3]&gt; get mystr&quot;Hello!! My name is NikoBelic&quot;127.0.0.1:6379[3]&gt; append mystr &quot;, 18 years old&quot;(integer) 42127.0.0.1:6379[3]&gt; get mystr&quot;Hello!! My name is NikoBelic, 18 years old&quot;127.0.0.1:6379[3]&gt; setrange mystr 0 Hey~~!!(integer) 42127.0.0.1:6379[3]&gt; get mystr&quot;Hey~~!! My name is NikoBelic, 18 years old&quot; 2.2 列表rpush、lpush、rpop、lpop、lindex、lrange在第一篇文章介绍过，不多bb了。| 命令 | 用例和描述 || — | — || ltrim | ltrim keyname start end 对列表进行修剪，只保留[start,end]范围内的元素 | 12345678910111213141516171819202122127.0.0.1:6379[3]&gt; lpush mylist NikoBelic(integer) 3127.0.0.1:6379[3]&gt; rpush mylist Tom(integer) 4127.0.0.1:6379[3]&gt; lrange mylist 0 -11) &quot;NikoBelic&quot;2) &quot;a&quot;3) &quot;c&quot;4) &quot;Tom&quot;127.0.0.1:6379[3]&gt; ltrim mylist 0 3OK127.0.0.1:6379[3]&gt; lrange mylist 0 -11) &quot;NikoBelic&quot;2) &quot;a&quot;3) &quot;c&quot;4) &quot;Tom&quot;127.0.0.1:6379[3]&gt; ltrim mylist 0 2OK127.0.0.1:6379[3]&gt; lrange mylist 0 -11) &quot;NikoBelic&quot;2) &quot;a&quot;3) &quot;c&quot; 阻塞式列表弹出命令以及在列表之间移动元素命令 命令 用例和描述 blpop blpop keyname [keyname…] timeout 从第一个非空列表中弹出位于最左端的元素，或者在timeout秒之内阻塞并等待可弹出元素的出现 brpop … rpoplpush rpoplpush sourcekey destkey 从source列表中弹出位于最右端的元素，然后将这个而元素推入到dest brpoplpush brpoplpush sourcekey destkey timeout ….在timeout秒之内阻塞并等待可弹出的元素出现 123456789101112131415161718192021222324252627127.0.0.1:6379[3]&gt; rpush sourcekey A(integer) 1127.0.0.1:6379[3]&gt; rpush sourcekey B(integer) 2127.0.0.1:6379[3]&gt; rpush sourcekey C(integer) 3127.0.0.1:6379[3]&gt; lrange sourcekey(error) ERR wrong number of arguments for &apos;lrange&apos; command127.0.0.1:6379[3]&gt; lrange sourcekey 0 -11) &quot;A&quot;2) &quot;B&quot;3) &quot;C&quot;127.0.0.1:6379[3]&gt; rpoplpush sourcekey destkey&quot;C&quot;127.0.0.1:6379[3]&gt; rpoplpush sourcekey destkey&quot;B&quot;127.0.0.1:6379[3]&gt; lrange sourcekey 0 -11) &quot;A&quot;127.0.0.1:6379[3]&gt; lrange destkey 0 -11) &quot;B&quot;2) &quot;C&quot;127.0.0.1:6379[3]&gt; blpop sourcekey 101) &quot;sourcekey&quot;2) &quot;A&quot;127.0.0.1:6379[3]&gt; blpop sourcekey 10(nil)(10.07s) 测试多个keyname的blocking弹出，从结果可以看出 blpop多个keyname不是分别将两个list的内容弹出，而是优先弹出第一个key，如果弹出成功则完成，如果第一个key没有内容，弹出操作不会被block，而是到下一个key中尝试弹出，如果弹出成功则结束，如果弹出失败则block timeout秒的时间。说明我们可以使用Redis完成消息的队列的功能。 1234567891011121314151617181920212223127.0.0.1:6379[3]&gt; blpop sourcekey 101) &quot;sourcekey&quot;2) &quot;A&quot;127.0.0.1:6379[3]&gt; blpop sourcekey 10(nil)(10.07s)127.0.0.1:6379[3]&gt; lpush sourcekey blockingtest(integer) 1127.0.0.1:6379[3]&gt; blpop sourcekey destkey 101) &quot;sourcekey&quot;2) &quot;blockingtest&quot;127.0.0.1:6379[3]&gt; lrange destkey 0 -11) &quot;B&quot;2) &quot;C&quot;127.0.0.1:6379[3]&gt; blpop sourcekey destkey 101) &quot;destkey&quot;2) &quot;B&quot;127.0.0.1:6379[3]&gt; blpop sourcekey destkey 101) &quot;destkey&quot;2) &quot;C&quot;127.0.0.1:6379[3]&gt; blpop sourcekey destkey 10(nil)(10.08s) 2.3 集合 命令 用例和描述 sadd sadd keyname item [item…] 将元素添加到集合里面，并返回被添加元素当中原本不存在于集合中的元素数量 srem srem keyname item [item…] 从集合里面移除一个或多个元素，并返回被移除元素的个数 sismember sismember keyname 检查item是否存在于集合 scard scard keyname 返回集合包含元素的数量 smembers smsmbers keyname 返回集合包含的所有元素 spop spop keyname 随机的移除一个元素，并返回被移除的元素 smove smove sourcekey destkey item 如果source中包含item，则将item从source移除并添加到dest中；如果item被成功移除则返回1，否则返回0 srandmember srandmember keyname [count] 从集合里面随机返回count个元素。当count为正数时，返回的随机元素不会重复；当count为负数时，返回的元素可能出现重复。 123456789101112131415161718192021222324252627282930313233343536373839404142434445127.0.0.1:6379[3]&gt; SMEMBERS myset1) &quot;Marry&quot;2) &quot;Tom&quot;3) &quot;Helen&quot;127.0.0.1:6379[3]&gt; sadd myset Marry(integer) 0127.0.0.1:6379[3]&gt; sadd myset Marry2(integer) 1127.0.0.1:6379[3]&gt; srem myset asd(integer) 0127.0.0.1:6379[3]&gt; srem myset Marry2(integer) 1127.0.0.1:6379[3]&gt; scard myset(integer) 3127.0.0.1:6379[3]&gt; smembers myset1) &quot;Marry&quot;2) &quot;Tom&quot;3) &quot;Helen&quot;127.0.0.1:6379[3]&gt; spop myset&quot;Helen&quot;127.0.0.1:6379[3]&gt; smove myset newset Tom(integer) 1127.0.0.1:6379[3]&gt; smembers myset1) &quot;Marry&quot;127.0.0.1:6379[3]&gt; smembers newset1) &quot;Tom&quot;127.0.0.1:6379[3]&gt; sadd myset Niko(integer) 1127.0.0.1:6379[3]&gt; sadd myset Nicholas(integer) 1127.0.0.1:6379[3]&gt; sadd myset James(integer) 1127.0.0.1:6379[3]&gt; smembers myset1) &quot;Marry&quot;2) &quot;Nicholas&quot;3) &quot;Niko&quot;4) &quot;James&quot;127.0.0.1:6379[3]&gt; srandmember myset 31) &quot;Marry&quot;2) &quot;Niko&quot;3) &quot;Nicholas&quot;127.0.0.1:6379[3]&gt; srandmember myset -31) &quot;Marry&quot;2) &quot;James&quot;3) &quot;Marry&quot; 多个集合的操作 命令 用例和描述 sdiff sdiff keyname [keyname…] 返回那些存在于第一个结合但不存在于其他集合中的元素（差集） sdiffstore sdiff destkey keyname [keyname…] 将集合的差集存储在destkey中 sinter sinter keyname [keyname…] 返回存在于所有集合中的元素（交集） sinterstore sinterstore destkey keyname [keyname…] 将交集存储于destkey中 sunion sunion keyname [keyname…] 返回至少存在于一个集合中的元素(并集) sunionscore sunitonscore destkey keyname [keyname…] 将并集结果存储在destkey中 1234567891011121314151617181920127.0.0.1:6379[3]&gt; smembers myset1) &quot;Marry&quot;2) &quot;Nicholas&quot;3) &quot;Niko&quot;4) &quot;James&quot;127.0.0.1:6379[3]&gt; smembers newset1) &quot;Tom&quot;127.0.0.1:6379[3]&gt; sdiff myset newset1) &quot;Marry&quot;2) &quot;Niko&quot;3) &quot;Nicholas&quot;4) &quot;James&quot;127.0.0.1:6379[3]&gt; sinter myset newset(empty list or set)127.0.0.1:6379[3]&gt; sunion myset newset1) &quot;Marry&quot;2) &quot;Niko&quot;3) &quot;Nicholas&quot;4) &quot;James&quot;5) &quot;Tom&quot; 2.4 散列散列基本操作 命令 用例和描述 hmget hmget keyname key [key…] 从散列里面获取一个或多个键的值 hmset hmset keyname key value [key value…] 为散列里面的键设置值 hdel hdel keyname key [key…] 删除散列里面的一个或多个键对，并返回成功找到并删除的键值对数量 hlen hlen keyname 返回散列包含的键值对数量 散列高级操作 命令 用例和描述 hexists hexists keyname key 检查给定的键是否存在于散列中 hkeys hkeys keyname 获取散列包含的所有键 kvals hvals keyname 获取散列包含的所有值 hgetall hgetall keyname 获取散列包含的所有键值对 hincrby hincrby keyname key increment 将键key存储的值加上整数increment hincrbyfloat hincrbyfloat keyname key increment 将键key存储的值加上浮点数increment 123456789101112131415161718192021222324252627127.0.0.1:6379[3]&gt; hmset myhash notebook MacBookProOK127.0.0.1:6379[3]&gt; hmget myhash notebook1) &quot;MacBookPro&quot;127.0.0.1:6379[3]&gt; hexists myhash notebook(integer) 1127.0.0.1:6379[3]&gt; hkeys myhash1) &quot;address&quot;2) &quot;notebook&quot;127.0.0.1:6379[3]&gt; hvals myhash1) &quot;BeiJing&quot;2) &quot;MacBookPro&quot;127.0.0.1:6379[3]&gt; hgetall myhash1) &quot;address&quot;2) &quot;BeiJing&quot;3) &quot;notebook&quot;4) &quot;MacBookPro&quot;127.0.0.1:6379[3]&gt; hmset myhash count 1OK127.0.0.1:6379[3]&gt; hincr myhash count(error) ERR unknown command &apos;hincr&apos;127.0.0.1:6379[3]&gt; hincrby myhash count(error) ERR wrong number of arguments for &apos;hincrby&apos; command127.0.0.1:6379[3]&gt; hincrby myhash count 5(integer) 6127.0.0.1:6379[3]&gt; hmget myhash count1) &quot;6&quot; 2.4 有序集合 命令 用例和描述 zadd zadd keyname score member [score member] 将带有给定分支的成员添加到集合里面 zrem zrem keyname member [member…] 从有序集合里面移除给定的成员，并返回被移除成员的数据 zcard zcard keyname 返回有序集合包含的成员数量 zincrby zincrby keyname increment member 将member的分值加上increment zcount zcount keyname min max 返回分值介于min和max之间的成员数量 zrank zrank keyname member 返回成员member在有序集合中的排名 zscore zscore keyname member 返回member的分值 zrange zrange keyname start stop [withscores] 返回有序集合中排名介于start和stop之间的members，如果给定了withscores，那么成员的分值也一并返回 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556127.0.0.1:6379[3]&gt; zadd myorderset 50 James(integer) 1127.0.0.1:6379[3]&gt; zrange myorderset 0 -11) &quot;James&quot;2) &quot;Niko&quot;3) &quot;Tom&quot;4) &quot;Helen&quot;5) &quot;Marry&quot;6) &quot;test&quot;127.0.0.1:6379[3]&gt; zrange myorderset 0 -1 withscores 1) &quot;James&quot; 2) &quot;50&quot; 3) &quot;Niko&quot; 4) &quot;100&quot; 5) &quot;Tom&quot; 6) &quot;101&quot; 7) &quot;Helen&quot; 8) &quot;102&quot; 9) &quot;Marry&quot;10) &quot;103&quot;11) &quot;test&quot;12) &quot;1001&quot;127.0.0.1:6379[3]&gt; zrem myorderset Tom(integer) 1127.0.0.1:6379[3]&gt; zrange myorderset 0 -1 withscores 1) &quot;James&quot; 2) &quot;50&quot; 3) &quot;Niko&quot; 4) &quot;100&quot; 5) &quot;Helen&quot; 6) &quot;102&quot; 7) &quot;Marry&quot; 8) &quot;103&quot; 9) &quot;test&quot;10) &quot;1001&quot;127.0.0.1:6379[3]&gt; zcard myorderset(integer) 5127.0.0.1:6379[3]&gt; zincrby myorderset 99 James&quot;149&quot;127.0.0.1:6379[3]&gt; zrange myorderset 0 -1 withscores 1) &quot;Niko&quot; 2) &quot;100&quot; 3) &quot;Helen&quot; 4) &quot;102&quot; 5) &quot;Marry&quot; 6) &quot;103&quot; 7) &quot;James&quot; 8) &quot;149&quot; 9) &quot;test&quot;10) &quot;1001&quot;127.0.0.1:6379[3]&gt; zcount myorderset 100 200(integer) 4127.0.0.1:6379[3]&gt; zrank myorderset test(integer) 4127.0.0.1:6379[3]&gt; zscore myorderset test&quot;1001&quot; 有序集合的范围性命令 命令 用例和描述 zrevrank zrevrank keyname member 降序排列，获取member的排名 zrevrange zrevrange keyname start stop [withscores] 降序排列，返回集合中给定范围的元素 zrangebyscore zrangebyscore keyname min max [withscores][limit offset count] 返回分值位于min~max之间的所有成员 zrevrangebyscore zrevrangebyscore keyname max min [withscores][limit offset count] 降序排列，返回分支位于max~min之间的所有成员 zremrangebyrank zremrangebyrank keyname start stop 移除集合中排名位于start~stop之间的元素 zremrangebyscore zremrangebyscore keyname min max 移除集合中分值位于min~max之间的元素 zinterstore zinterstore destkey keycount key [key…] [weights weight [weight…]] [aggregate sum min max] 对给定的有序集合执行交集运算,keycount为显式指定的集合个数 zunionstore zunionstore destkey keycount key [key…] [weights weight [weight…]] [aggregate sum min max] 对给定的有序集合执行并集运算 123456789101112131415161718192021222324252627282930313233343536127.0.0.1:6379[3]&gt; zrange yourorderset 0 -1 withscores1) &quot;Tom&quot;2) &quot;100&quot;3) &quot;James&quot;4) &quot;101&quot;127.0.0.1:6379[3]&gt; zrange myorderset 0 -1 withscores 1) &quot;Niko&quot; 2) &quot;100&quot; 3) &quot;Helen&quot; 4) &quot;102&quot; 5) &quot;Marry&quot; 6) &quot;103&quot; 7) &quot;James&quot; 8) &quot;149&quot; 9) &quot;test&quot;10) &quot;1001&quot;127.0.0.1:6379[3]&gt; ZINTERSTORE interorderset 2 myorderset yourorderset(integer) 1127.0.0.1:6379[3]&gt; zrange interorderset 0 -1 withscores1) &quot;James&quot;2) &quot;250&quot;127.0.0.1:6379[3]&gt; zunionstore unionorderstore 2 myorderset yourorderset(integer) 6127.0.0.1:6379[3]&gt; zrange unionorderstore 0 -1 withscores 1) &quot;Niko&quot; 2) &quot;100&quot; 3) &quot;Tom&quot; 4) &quot;100&quot; 5) &quot;Helen&quot; 6) &quot;102&quot; 7) &quot;Marry&quot; 8) &quot;103&quot; 9) &quot;James&quot;10) &quot;250&quot;11) &quot;test&quot;12) &quot;1001&quot; 2.6 发布与订阅略 2,7 其他命令排序sort可以根据字符串、列表、集合、有序集合、散列这5种键里面存储着的数据对其排序。类似于关系数据库中的order by语句。| 命令 | 用例和描述 || — | — || sort | sort sourcekey [by pattern] [limit offset count] [get pattern [get pattern…]] [asc|desc] [alpha] [store dest-key] 根据给定的选项，对输入列表、集合或者有序集合进行排序，然后返回或者存储排序的结果 | 12345678910111213141516171819202122232425262728293031323334127.0.0.1:6379[3]&gt; rpush mysort 1 3 5 5 2 2 7 8 1 9 7(integer) 12127.0.0.1:6379[3]&gt; SORT mysort(error) ERR One or more scores can&apos;t be converted into double127.0.0.1:6379[3]&gt; lrange mysort 0 -1 1) &quot;1&quot; 2) &quot;3&quot; 3) &quot;5&quot; 4) &quot;5&quot; 5) &quot;2&quot; 6) &quot;2&quot; 7) &quot;7&quot; 8) &quot;8&quot; 9) &quot;1&quot;10) &quot;9&quot;11) &quot;7&quot;127.0.0.1:6379[3]&gt; sort mysort 1) &quot;1&quot; 2) &quot;1&quot; 3) &quot;2&quot; 4) &quot;2&quot; 5) &quot;3&quot; 6) &quot;5&quot; 7) &quot;5&quot; 8) &quot;7&quot; 9) &quot;7&quot;10) &quot;8&quot;11) &quot;9&quot;127.0.0.1:6379[3]&gt; sort mylist alpha1) &quot;NikoBelic&quot;2) &quot;a&quot;127.0.0.1:6379[3]&gt; sort mylist alpha desc1) &quot;a&quot;2) &quot;NikoBelic&quot; 2.8 键的过期时间 命令 示例和描述 persist persist keyname 移除键的过期时间 ttl ttl keyname 查看给定键距离过期还有多少秒 expire expire keyname seconds 让给定键在指定的秒数之后过期 expireat expireat keyname timestamp 将给定键的过期时间设置为给定的Unix时间戳 pttl pttl keyname 查看给定键距离过期时间还有多少毫秒，Redis2.6+ pexpire pexpire keyname milliseconds，让给定键在指定的毫秒数之后过期。Redis2.6+ pexpireat pexpireat keyname timestamp-milliseconds 将一个毫秒级精度的Unix时间戳设置为给定键的的过期时间，Redis2.6+ 下一章：编写Java版的Redis操作工具类。","tags":[{"name":"Nosql","slug":"Nosql","permalink":"https://seawaylee.github.io/tags/Nosql/"},{"name":"Redis","slug":"Redis","permalink":"https://seawaylee.github.io/tags/Redis/"}]},{"title":"Redis学习（一）数据结构","date":"2017-03-09T13:38:17.000Z","path":"2017/03/09/Nosql/Redis/Redis_1/","text":"1 Redis数据结构简介 结构类型 结构存储的值 结构的读写能力 String 字符串、整数、浮点数 对字符串或字符串的一部分执行操作；对整数、浮点数进行自增、自减 List 一个链表，链表上的每个节点都包含了一个字符串 从链表的两端push或者pop元素，根据index对链表进行trim；读取单个或多个元素；根据值查找或移除元素 Set 字符串无序collection，每个String独一无二 添加、获取、删除单个元素；检查元素是否存在于collection中；计算交集、并集、差集；从集合里随机获取元素 Hash 包含键值对的无序散列表 添加、获取、移除单个键值对；获取所有键值对 ZSet 字符串成员与浮点数score之间的有序映射，元素的排列顺序由大小决定 添加、获取、删除单个元素；根据score的range或者成员来获取元素 1.1 String123456127.0.0.1:6379[3]&gt; set hello worldOK127.0.0.1:6379[3]&gt; keys *1) &quot;hello&quot;127.0.0.1:6379[3]&gt; get hello&quot;world&quot; 1.2 List 命令 行为 rpush 右端添加 lpush 左端添加 lindex 获取列表在给定位置上的单个元素 lpop 从列表左端弹出一个值，并返回被弹出的值 push操作后会返回当前队列的长度pop操作后会返回被弹出的元素lindex操作会返回指定位置的值（注意index从0开始）lrange start end 获取指定范围内的元素，end=-1表示获取直到最后一个元素 123456789101112131415161718192021222324127.0.0.1:6379[3]&gt; lpush mylist a(integer) 1127.0.0.1:6379[3]&gt; lpush mylist b(integer) 2127.0.0.1:6379[3]&gt; lrange mylist 0 -11) &quot;b&quot;2) &quot;a&quot;127.0.0.1:6379[3]&gt; rpush mylist c(integer) 3127.0.0.1:6379[3]&gt; lrange mylist 0 -11) &quot;b&quot;2) &quot;a&quot;3) &quot;c&quot;127.0.0.1:6379[3]&gt; lpop mylist&quot;b&quot;127.0.0.1:6379[3]&gt; lrange mylist 0 -11) &quot;a&quot;2) &quot;c&quot;127.0.0.1:6379[3]&gt; lindex mylist 2(nil)127.0.0.1:6379[3]&gt; lindex mylist 1&quot;c&quot;127.0.0.1:6379[3]&gt; lindex mylist 0&quot;a&quot; 1.3 Set集合 命令 行为 sadd 将元素添加到集合 smembers 返回集合包含的所有元素 sismember 检查给定元素是否存在与集合中 srem 如果给定的元素存在于集合中，那么移除这个元素 123456789101112131415161718192021222324252627127.0.0.1:6379[3]&gt; sadd myset NikoBelic(integer) 1127.0.0.1:6379[3]&gt; SMEMBERS myset1) &quot;NikoBelic&quot;127.0.0.1:6379[3]&gt; sadd myset Tom(integer) 1127.0.0.1:6379[3]&gt; sadd myset Helen(integer) 1127.0.0.1:6379[3]&gt; sadd myset Marry(integer) 1127.0.0.1:6379[3]&gt; SMEMBERS myset1) &quot;Marry&quot;2) &quot;Tom&quot;3) &quot;Helen&quot;4) &quot;NikoBelic&quot;127.0.0.1:6379[3]&gt; SISMEMBER myset Tom(integer) 1127.0.0.1:6379[3]&gt; SISMEMBER myset Tom222(integer) 0127.0.0.1:6379[3]&gt; srem myset NikoBelic(integer) 1127.0.0.1:6379[3]&gt; srem myset NikoBelic222(integer) 0127.0.0.1:6379[3]&gt; SMEMBERS myset1) &quot;Marry&quot;2) &quot;Tom&quot;3) &quot;Helen&quot; 1.4 散列Redis的散列更像是一个猥琐扮的memcache数据库 命令 行为 hset 在散列里面关联起给定的键值对 hget 获取指定散列键的值 hgetall 获取散列包含的所有键值对 hdel 如果给定键存在于散列里面，那么移除这个键 12345678910111213141516171819202122127.0.0.1:6379[3]&gt; hset myhash name NikoBelic(integer) 1127.0.0.1:6379[3]&gt; hset myhash age 18(integer) 1127.0.0.1:6379[3]&gt; hset myhash address BeiJing(integer) 1127.0.0.1:6379[3]&gt; HGETALL myhash1) &quot;name&quot;2) &quot;NikoBelic&quot;3) &quot;age&quot;4) &quot;18&quot;5) &quot;address&quot;6) &quot;BeiJing&quot;127.0.0.1:6379[3]&gt; hget myhash name&quot;NikoBelic&quot;127.0.0.1:6379[3]&gt; hdel myhash name(integer) 1127.0.0.1:6379[3]&gt; hdel myhash age(integer) 1127.0.0.1:6379[3]&gt; hgetall myhash1) &quot;address&quot;2) &quot;BeiJing&quot; 1.5 有序集合 命令 行为 zadd 将一个带有给定分值的成员添加到有序集合里面 zrange 根据元素在有序排列中所处的位置，从有序集合里面获取多个元素 zrangebyscore 获取有序集合在给定分值范围内的所有元素 zrem 如果给定成员存在于有序集合，那么移除这个元素 1234567891011121314151617181920212223242526272829127.0.0.1:6379[3]&gt; zadd myorderset 100 NikoBelic(integer) 1127.0.0.1:6379[3]&gt; zadd myorderset 101 Tom(integer) 1127.0.0.1:6379[3]&gt; zadd myorderset 102 Helen(integer) 1127.0.0.1:6379[3]&gt; zadd myorderset 10 Marry(integer) 1127.0.0.1:6379[3]&gt; zrange myorderset 0 -11) &quot;Marry&quot;2) &quot;NikoBelic&quot;3) &quot;Tom&quot;4) &quot;Helen&quot;127.0.0.1:6379[3]&gt; zrange myorderset 0 -1 withscores1) &quot;Marry&quot;2) &quot;10&quot;3) &quot;NikoBelic&quot;4) &quot;100&quot;5) &quot;Tom&quot;6) &quot;101&quot;7) &quot;Helen&quot;8) &quot;102&quot;127.0.0.1:6379[3]&gt; ZRANGEBYSCORE myorderset 100 103 withscores1) &quot;NikoBelic&quot;2) &quot;100&quot;3) &quot;Tom&quot;4) &quot;101&quot;5) &quot;Helen&quot;6) &quot;102&quot; 小结以上是对Redis的5中数据结果的基本操作进行的简要概述，还有一些其他操作，例如自增(INCR)等操作以后会说，但以上都是日常开发中最最常用的操作，需要熟记于心。","tags":[{"name":"Nosql","slug":"Nosql","permalink":"https://seawaylee.github.io/tags/Nosql/"},{"name":"Redis","slug":"Redis","permalink":"https://seawaylee.github.io/tags/Redis/"}]},{"title":"Ngxin学习笔记","date":"2017-03-09T12:21:06.000Z","path":"2017/03/09/高并发/Ngxin学习笔记/","text":"1 什么是NginxNginx (“engine x”) 是一个高性能的HTTP和反向代理服务器，也是一个IMAP/POP3/SMTP服务器。其特点是占有内存少，并发能力强，事实上nginx的并发能力确实在同类型的网页服务器中表现较好，中国大陆使用nginx网站用户有：百度、京东、新浪、网易、腾讯、淘宝等。 2 特点2.1 反向代理我们经常使用正向代理，即用户手动设置代理服务器的ip和端口号，通过代理服务器去请求目标服务器，目标服务器会认为请求者为代理服务器，VPN就是这个意思。反向代理是用来代理服务器的，用户不需要设置，用户认为访问的就是目标服务器。 2.2 负载均衡原理就是数据流量分摊到多个服务器上执行，减轻每台服务器的压力，多台服务器共同完成工作任务，从而提高了数据吞吐量。 3 安装Nginx 官网下载nginx源码包 解压缩 tar -zxvf nginx.zip 编译、安装 ./configue &amp;&amp; make &amp;&amp; make install 启动 ./usr/local/bin/nginx 访问localhost看到欢迎界面 4 Nginx命令 启动 nginx 强制关闭 nginx -s stop 正常关闭 nginx -s quit 动态加载配置文件 nginx -s reload 5 Nginx配置proxy_pass 配置代理upstream 配置集群 123456789101112131415161718192021222324252627282930313233343536373839worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; upstream server_tomcat&#123; server 127.0.0.1:8080 weight=1;#weight表示多少个 server 127.0.0.1:8081 weight=1; #ip_hash; &#125; server &#123; listen 80; server_name localhost; location / &#123; # / 表示拦截所有请求 root html; index index.html index.htm; proxy_pass http://server_tomcat; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 当访问localhost时，由于没有指定负载均衡算法，所以默认为轮询，浏览器每次请求都会被轮流转发到8080和8081两个端口。 可能出现 防火墙问题,开放80端口/sbin/iptables -I INPUT -p tcp –dport 80 -j ACCEPT/etc/rc.d/init.d/iptables save 5.1 动静分离动态资源123location ~ .*\\.(jsp|do|action)$ &#123; proxy_pass http://tomcat-01.itcast.cn:8080;&#125; 静态资源123location ~ .*\\.(html|js|css|gif|jpg|jpeg|png)$ &#123; expires 3d;&#125; 5.2 KeepAlived高可用5.2.1 HA概念HA(High Available), 高可用性集群，是保证业务连续性的有效解决方案，一般有两个或两个以上的节点，且分为活动节点及备用节点。 5.2.2 KeepAlivedkeepalive是一款可以实现高可靠的软件，通常部署在2台服务器上，分为一主一备。Keepalived可以对本机上的进程进行检测，一旦Master检测出某个进程出现问题，将自己切换成Backup状态，然后通知另外一个节点切换成Master状态。 5.2.3 安装 wget http://www.keepalived.org/software/keepalived-1.2.19.tar.gz ./configure –prefix=/usr/local/keepalived &amp;&amp; make &amp;&amp; make install 将软件添加到系统服务中，便于启动 cp /usr/local/keepalived/sbin/keepalived /usr/sbin/ 拷贝执行文件 cp /usr/local/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/keepalived 将init.d文件拷贝到etc下,加入开机启动项 cp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/ 将keepalived文件拷贝到etc下 mkdir -p /etc/keepalived 创建keepalived文件夹 cp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf 将keepalived配置文件拷贝到etc下 chmod +x /etc/init.d/keepalived 添加可执行权限 快捷操作12345678cp /usr/local/keepalived/sbin/keepalived /usr/sbin/cp /usr/local/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/keepalivedcp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/ mkdir -p /etc/keepalivedcp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/keepalived.confchmod +x /etc/init.d/keepalivedchkconfig --add keepalived chkconfig keepalived on 5.2.4 配置虚拟IP修改配置文件： /etc/keepalived/keepalived.conf MASTER节点 12345678910111213141516global_defs &#123;&#125;vrrp_instance VI_1 &#123; state MASTER #指定A节点为主节点 备用节点上设置为BACKUP即可 interface eth0 #绑定虚拟IP的网络接口 virtual_router_id 51 #VRRP组名，两个节点的设置必须一样，以指明各个节点属于同一VRRP组 priority 100 #主节点的优先级（1-254之间），备用节点必须比主节点优先级低 advert_int 1 #组播信息发送间隔，两个节点设置必须一样 authentication &#123; #设置验证信息，两个节点必须一致 auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; #指定虚拟IP, 两个节点设置必须一样 192.168.33.60/24 #如果两个nginx的ip分别是192.168.33.61,,...62，则此处的虚拟ip跟它俩同一个网段即可 &#125;&#125; BACKUP节点12345678910111213141516global_defs &#123;&#125;vrrp_instance VI_1 &#123; state BACKUP interface eth0 virtual_router_id 51 priority 99 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.33.60/24 &#125;&#125; 分别启动两台机器上的keepalived service keepalived start 测试： 杀掉master上的keepalived进程，你会发现，在slave机器上的eth0网卡多了一个ip地址 查看ip地址的命令： ip addr 5.2.5 配置keepalived心跳检查 Keepalived并不跟nginx耦合，它俩完全不是一家人 但是keepalived提供一个机制：让用户自定义一个shell脚本去检测用户自己的程序，返回状态给keepalived就可以了 MASTER 1234567891011121314151617181920212223242526272829303132global_defs &#123;&#125;vrrp_script chk_health &#123; script &quot;[[ `ps -ef | grep nginx | grep -v grep | wc -l` -ge 2 ]] &amp;&amp; exit 0 || exit 1&quot; interval 1 #每隔1秒执行上述的脚本，去检查用户的程序ngnix weight -2&#125;vrrp_instance VI_1 &#123; state MASTER interface eth0 virtual_router_id 1 priority 100 advert_int 2 authentication &#123; auth_type PASS auth_pass 1111 &#125; track_script &#123; chk_health &#125; virtual_ipaddress &#123; 10.0.0.10/24 &#125; notify_master &quot;/usr/local/keepalived/sbin/notify.sh master&quot; notify_backup &quot;/usr/local/keepalived/sbin/notify.sh backup&quot; notify_fault &quot;/usr/local/keepalived/sbin/notify.sh fault&quot;&#125; 切换通知脚本vi /usr/local/keepalived/sbin/notify.sh 123456789101112131415161718192021#!/bin/bashcase &quot;$1&quot; in master) /usr/local/nginx/sbin/nginx exit 0 ;;backup) /usr/local/nginx/sbin/nginx -s stop /usr/local/nginx/sbin/nginx exit 0 ;; fault) /usr/local/nginx/sbin/nginx -s stop exit 0 ;; *) echo &apos;Usage: notify.sh &#123;master|backup|fault&#125;&apos; exit 1 ;;esac 添加执行权限’chmod +x /usr/local/keepalived/sbin/notify.sh’ 在第二台机器上添加notify.sh脚本、将同样的心跳检测部分配置添加到BACKUP机器上分别在两台机器上启动keepalivedservice keepalived startchkconfig keepalived on 测试： 主动停止Master-ngxin，立即用浏览器访问虚地址（1秒内），发现请求到了备机的nginx上，一秒后再刷新，发现又请求到了主机nginx上（KeepAlived自动恢复了nginx） 6 Session共享问题使用Nginx反向代理集群会引出Session共享问题，有两种解决方案。 方案一：(只在windows下有效) web服务器解决（广播机制） 注意：tomcat下性能低 修改两个地方 修改tomcat的server.xml支持共享。将引擎标签下的&lt;Cluster className=&quot;org.apache.catalina.ha.tcp.SimpleTcpCluster&quot;/&gt;注释去掉. 修改项目的配置文件 web.xml中添加一个节点&lt;distributable/&gt; 方案二： 可以将session的id放入redis中 方案三： 保证一个ip永远访问的是同一台服务器，就不存在session共享问题了。 在nginx配置文件中添加 ip_hash;","tags":[{"name":"高并发","slug":"高并发","permalink":"https://seawaylee.github.io/tags/高并发/"}]},{"title":"Iterm2快捷键","date":"2017-03-09T12:21:06.000Z","path":"2017/03/09/Mac工具/Iterm快捷键/","text":"标签 新建标签：command + t 关闭标签：command + w 切换标签：command + 数字 command + 左右方向键 切换全屏：command + enter 查找：command + f 分屏 垂直分屏：command + d 水平分屏：command + shift + d 切换屏幕：command + option + 方向键 command + [ 或 command + ] 查看历史命令：command + ; 查看剪贴板历史：command + shift + h 其他 清除当前行：ctrl + u 到行首：ctrl + a 到行尾：ctrl + e 前进后退：ctrl + f/b (相当于左右方向键) 上一条命令：ctrl + p 搜索命令历史：ctrl + r 删除当前光标的字符：ctrl + d 删除光标之前的字符：ctrl + h 删除光标之前的单词：ctrl + w 删除到文本末尾：ctrl + k 交换光标处文本：ctrl + t 清屏1：command + r 清屏2：ctrl + l","tags":[{"name":"Mac工具","slug":"Mac工具","permalink":"https://seawaylee.github.io/tags/Mac工具/"}]},{"title":"Maven命令小记","date":"2017-03-09T12:21:06.000Z","path":"2017/03/09/JavaWeb/Maven命令小记/","text":"1 基本命令 命令 阶段 功能 mvn clean 清理 清理输出的class文件 mvn compile 编译 将java代码编译成class文件 mvn test 测试 test下的单元测试代码一次性测试 mvn package 打包 java项目生成jar、web项目生成war mvn install 安装 将maven打包成jar或war发布到本地仓库 mvn tomcat:run 部署 以tomcat为容器启动项目 执行后面阶段的命令，求安眠的命令会自动执行 2 仓库conf/settings.xml 本地仓库 私服 中央仓库 3 创建maven项目1.使用IDE创建MavenProject GroupId：公司名 ArtifactId：项目名 Version：版本号 2.依赖作用范围 依赖范围 含义 举例 compile 编译范围，默认，在编译、测试、运行时都会被使用 spring-core provided 只有当JDK或者容器已经提供该依赖之后才会使用 servlet-api runtime 依赖子啊运行时和测试系统时需要，编译时不需要 jdbc test 测试时需要的依赖，编译和运行时都不需要 junit system 与provided类似，但是必须提供一个本地系统中jar文件的路径 不推荐使用","tags":[{"name":"JavaWeb","slug":"JavaWeb","permalink":"https://seawaylee.github.io/tags/JavaWeb/"}]},{"title":"ElasticSearch学习笔记（一）- 快速入门","date":"2017-03-09T03:59:15.000Z","path":"2017/03/09/搜索/ElasticSearch学习笔记-概览/","text":"1 IntroductionElasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。不过，Elasticsearch不仅仅是Lucene和全文搜索，我们还能这样去描述它： 分布式的实时文件存储，每个字段都被索引并可被搜索 分布式的实时分析搜索引擎 可以扩展到上百台服务器，处理PB级结构化或非结构化数据 2 Installation安装ES123curl -L -O http://download.elasticsearch.org/PATH/TO/VERSION.zip &lt;1&gt;unzip elasticsearch-$VERSION.zipcd elasticsearch-$VERSION 安装MavelMarvel是Elasticsearch的管理和监控工具，在开发环境下免费使用。它包含了一个叫做Sense的交互式控制台，使用户方便的通过浏览器直接与Elasticsearch进行交互。 安装Kibana 3 APIJava API Elasticsearch为Java用户提供了两种内置客户端：节点客户端(node client)： 节点客户端以无数据节点(none data node)身份加入集群，换言之，它自己不存储任何数据，但是它知道数据在集群中的具体位置，并且能够直接转发请求到对应的节点上。传输客户端(Transport client)： 这个更轻量的传输客户端能够发送请求到远程集群。它自己不加入集群，只是简单转发请求给集群中的节点。两个Java客户端都通过9300端口与集群交互，使用Elasticsearch传输协议(Elasticsearch Transport Protocol)。集群中的节点之间也通过9300端口进行通信。如果此端口未开放，你的节点将不能组成集群。 基于HTTP协议，以JSON为数据交互格式的RESTful API 1curl -X&lt;VERB&gt; '&lt;PROTOCOL&gt;://&lt;HOST&gt;:&lt;PORT&gt;/&lt;PATH&gt;?&lt;QUERY_STRING&gt;' -d '&lt;BODY&gt;' VERB HTTP方法：GET, POST, PUT, HEAD, DELETE PROTOCOL http或者https协议（只有在Elasticsearch前面有https代理的时候可用） HOST Elasticsearch集群中的任何一个节点的主机名，如果是在本地的节点，那么就叫localhost PORT Elasticsearch HTTP服务所在的端口，默认为9200 PATH API路径（例如_count将返回集群中文档的数量），PATH可以包含多个组件，例如_cluster/stats或者_nodes/stats/jvm QUERY_STRING 一些可选的查询请求参数，例如?pretty参数将使请求返回更加美观易读的JSON数据 BODY 一个JSON格式的请求主体（如果请求需要的话） $ curl http://localhost:9200/_count\\?pretty12345678&#123; \"count\" : 1, \"_shards\" : &#123; \"total\" : 1, \"successful\" : 1, \"failed\" : 0 &#125;&#125; $ curl http://localhost:9200\\?pretty12345678910111213&#123; \"name\" : \"Fb_2umy\", \"cluster_name\" : \"rhino-application\", \"cluster_uuid\" : \"E1E1h5v5SNGkuMn--DgcPQ\", \"version\" : &#123; \"number\" : \"5.2.0\", \"build_hash\" : \"24e05b9\", \"build_date\" : \"2017-01-24T19:52:35.800Z\", \"build_snapshot\" : false, \"lucene_version\" : \"6.4.0\" &#125;, \"tagline\" : \"You Know, for Search\"&#125; 4 文档Elasticsearch是面向文档(document oriented)的，这意味着它可以存储整个对象或文档(document)。然而它不仅仅是存储，还会索引(index)每个文档的内容使之可以被搜索。在Elasticsearch中，你可以对文档（而非成行成列的数据）进行索引、搜索、排序、过滤。这种理解数据的方式与以往完全不同，这也是Elasticsearch能够执行复杂的全文搜索的原因之一。 开始第一步 我们现在开始进行一个简单教程，它涵盖了一些基本的概念介绍，比如索引(indexing)、搜索(search)以及聚合(aggregations)。通过这个教程，我们可以让你对Elasticsearch能做的事以及其易用程度有一个大致的感觉。我们接下来将陆续介绍一些术语和基本的概念，但就算你没有马上完全理解也没有关系。我们将在本书的各个章节中更加深入的探讨这些内容。所以，坐下来，开始以旋风般的速度来感受Elasticsearch的能力吧！让我们建立一个员工目录 假设我们刚好在Megacorp工作，这时人力资源部门出于某种目的需要让我们创建一个员工目录，这个目录用于促进人文关怀和用于实时协同工作，所以它有以下不同的需求： 数据能够包含多个值的标签、数字和纯文本。 检索任何员工的所有信息。 支持结构化搜索，例如查找30岁以上的员工。 支持简单的全文搜索和更复杂的短语(phrase)搜索 高亮搜索结果中的关键字 能够利用图表管理分析这些数据 索引员工文档 我们首先要做的是存储员工数据，每个文档代表一个员工。在Elasticsearch中存储数据的行为就叫做索引(indexing)，不过在索引之前，我们需要明确数据应该存储在哪里。在Elasticsearch中，文档归属于一种类型(type),而这些类型存在于索引(index)中，我们可以画一些简单的对比图来类比传统关系型数据库： Relational DB -&gt; Databases -&gt; Tables -&gt; Rows -&gt; ColumnsElasticsearch -&gt; Indices -&gt; Types -&gt; Documents -&gt; FieldsElasticsearch集群可以包含多个索引(indices)（数据库），每一个索引可以包含多个类型(types)（表），每一个类型包含多个文档(documents)（行），然后每个文档包含多个字段(Fields)（列）。「索引」含义的区分 你可能已经注意到索引(index)这个词在Elasticsearch中有着不同的含义，所以有必要在此做一下区分: 索引（名词） 如上文所述，一个索引(index)就像是传统关系数据库中的数据库，它是相关文档存储的地方，index的复数是indices 或indexes。 索引（动词） 「索引一个文档」表示把一个文档存储到索引（名词）里，以便它可以被检索或者查询。这很像SQL中的INSERT关键字，差别是，如果文档已经存在，新的文档将覆盖旧的文档。 倒排索引 传统数据库为特定列增加一个索引，例如B-Tree索引来加速检索。Elasticsearch和Lucene使用一种叫做倒排索引(inverted index)的数据结构来达到相同目的。 默认情况下，文档中的所有字段都会被索引（拥有一个倒排索引），只有这样他们才是可被搜索的。我们将会在倒排索引章节中更详细的讨论。所以为了创建员工目录，我们将进行如下操作：为每个员工的文档(document)建立索引，每个文档包含了相应员工的所有信息。每个文档的类型为employee。employee类型归属于索引megacorp。megacorp索引存储在Elasticsearch集群中。实际上这些都是很容易的（尽管看起来有许多步骤）。我们能通过一个命令执行完成的操作： 12345678PUT /megacorp/employee/1&#123; \"first_name\" : \"John\", \"last_name\" : \"Smith\", \"age\" : 25, \"about\" : \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ]&#125; 请求实体（JSON文档），包含了这个员工的所有信息。他的名字叫“John Smith”，25岁，喜欢攀岩。很简单吧！它不需要你做额外的管理操作，比如创建索引或者定义每个字段的数据类型。我们能够直接索引文档，Elasticsearch已经内置所有的缺省设置，所有管理操作都是透明的。接下来，让我们在目录中加入更多员工信息： 1234567891011121314151617PUT /megacorp/employee/2&#123; \"first_name\" : \"Jane\", \"last_name\" : \"Smith\", \"age\" : 32, \"about\" : \"I like to collect rock albums\", \"interests\": [ \"music\" ]&#125;PUT /megacorp/employee/3&#123; \"first_name\" : \"Douglas\", \"last_name\" : \"Fir\", \"age\" : 35, \"about\": \"I like to build cabinets\", \"interests\": [ \"forestry\" ]&#125; 5 检索文档我们通过HTTP方法GET来检索文档，同样的，我们可以使用DELETE方法删除文档，使用HEAD方法检查某文档是否存在。如果想更新已存在的文档，我们只需再PUT一次。 123456789101112131415161718GET /megacorp/employee/3&#123; \"_index\": \"megacrop\", \"_type\": \"employee\", \"_id\": \"3\", \"_version\": 1, \"found\": true, \"_source\": &#123; \"first_name\": \"Douglas\", \"last_name\": \"Fir\", \"age\": 35, \"about\": \"I like to build cabinets\", \"interests\": [ \"forestry\" ] &#125;&#125; 简单搜索 GET请求非常简单——你能轻松获取你想要的文档。让我们来进一步尝试一些东西，比如简单的搜索！我们尝试一个最简单的搜索全部员工的请求：GET /megacorp/employee/_search你可以看到我们依然使用megacorp索引和employee类型，但是我们在结尾使用关键字_search来取代原来的文档ID。响应内容的hits数组中包含了我们所有的三个文档。默认情况下搜索会返回前10个结果。+ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263GET /megacorp/employee/_search&#123; \"took\": 20, \"timed_out\": false, \"_shards\": &#123; \"total\": 5, \"successful\": 5, \"failed\": 0 &#125;, \"hits\": &#123; \"total\": 3, \"max_score\": 1.0, \"hits\": [ &#123; \"_index\": \"megacrop\", \"_type\": \"employee\", \"_id\": \"2\", \"_score\": 1.0, \"_source\": &#123; \"first_name\": \"Jane\", \"last_name\": \"Smith\", \"age\": 32, \"about\": \"I like to collect rock albums\", \"interests\": [ \"music\" ] &#125; &#125;, &#123; \"_index\": \"megacrop\", \"_type\": \"employee\", \"_id\": \"1\", \"_score\": 1.0, \"_source\": &#123; \"first_name\": \"John\", \"last_name\": \"Smith\", \"age\": 25, \"about\": \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ] &#125; &#125;, &#123; \"_index\": \"megacrop\", \"_type\": \"employee\", \"_id\": \"3\", \"_score\": 1.0, \"_source\": &#123; \"first_name\": \"Douglas\", \"last_name\": \"Fir\", \"age\": 35, \"about\": \"I like to build cabinets\", \"interests\": [ \"forestry\" ] &#125; &#125; ] &#125;&#125; 加入检索条件我们在请求中依旧使用_search关键字，然后将查询语句传递给参数q=。这样就可以得到所有姓氏为Smith的结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647GET /megacorp/employee/_search?q=last_name:Smith&#123; \"took\": 4, \"timed_out\": false, \"_shards\": &#123; \"total\": 5, \"successful\": 5, \"failed\": 0 &#125;, \"hits\": &#123; \"total\": 2, \"max_score\": 0.2876821, \"hits\": [ &#123; \"_index\": \"megacrop\", \"_type\": \"employee\", \"_id\": \"2\", \"_score\": 0.2876821, \"_source\": &#123; \"first_name\": \"Jane\", \"last_name\": \"Smith\", \"age\": 32, \"about\": \"I like to collect rock albums\", \"interests\": [ \"music\" ] &#125; &#125;, &#123; \"_index\": \"megacrop\", \"_type\": \"employee\", \"_id\": \"1\", \"_score\": 0.2876821, \"_source\": &#123; \"first_name\": \"John\", \"last_name\": \"Smith\", \"age\": 25, \"about\": \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ] &#125; &#125; ] &#125;&#125; 使用DSL语句查询查询字符串搜索便于通过命令行完成特定(ad hoc)的搜索，但是它也有局限性（参阅简单搜索章节）。Elasticsearch提供丰富且灵活的查询语言叫做DSL查询(Query DSL),它允许你构建更加复杂、强大的查询。DSL(Domain Specific Language特定领域语言)以JSON请求体的形式出现。我们可以这样表示之前关于“Smith”的查询: 12345678POST /megacorp/employee/_search&#123; \"query\": &#123; \"match\": &#123; \"last_name\": \"Smith\" &#125; &#125;&#125; 更复杂的搜索我们让搜索稍微再变的复杂一些。我们依旧想要找到姓氏为“Smith”的员工，但是我们只想得到年龄大于30岁的员工。我们的语句将添加过滤器(filter),它使得我们高效率的执行一个结构化搜索： 123456789101112131415161718192021222324POST /megacorp/employee/_search&#123; \"query\": &#123; \"bool\": &#123; \"must\": [ &#123; \"match\": &#123; \"last_name\": \"smith\" &#125; &#125; ], \"filter\": [ &#123; \"range\": &#123; \"age\": &#123; \"gte\": \"30\" &#125; &#125; &#125; ] &#125; &#125;&#125; 全文搜索 12345678POST /megacorp/employee/_search&#123; \"query\" : &#123; \"match\" : &#123; \"about\" : \"rock climbing\" &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&#123; &quot;took&quot;: 7, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.53484553, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;megacrop&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.53484553, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;megacrop&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 0.26742277, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;Jane&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 32, &quot;about&quot;: &quot;I like to collect rock albums&quot;, &quot;interests&quot;: [ &quot;music&quot; ] &#125; &#125; ] &#125;&#125; 默认情况下，Elasticsearch根据结果相关性评分来对结果集进行排序，所谓的「结果相关性评分」就是文档与查询条件的匹配程度。很显然，排名第一的John Smith的about字段明确的写到“rock climbing”。但是为什么Jane Smith也会出现在结果里呢？原因是“rock”在她的abuot字段中被提及了。因为只有“rock”被提及而“climbing”没有，所以她的_score要低于John。3这个例子很好的解释了Elasticsearch如何在各种文本字段中进行全文搜索，并且返回相关性最大的结果集。相关性(relevance)的概念在Elasticsearch中非常重要，而这个概念在传统关系型数据库中是不可想象的，因为传统数据库对记录的查询只有匹配或者不匹配。 短语搜索 目前我们可以在字段中搜索单独的一个词，这挺好的，但是有时候你想要确切的匹配若干个单词或者短语(phrases)。例如我们想要查询同时包含”rock”和”climbing”（并且是相邻的）的员工记录。要做到这个，我们只要将match查询变更为match_phrase查询即可: 12345678POST /megacorp/employee/_search&#123; \"query\" : &#123; \"match_phrase\" : &#123; \"about\" : \"rock climbing\" &#125; &#125;&#125; 高亮搜搜 12345678910111213POST /megacorp/employee/_search&#123; \"query\" : &#123; \"match_phrase\" : &#123; \"about\" : \"rock climbing\" &#125; &#125;, \"highlight\": &#123; \"fields\" : &#123; \"about\" : &#123;&#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536&#123; &quot;took&quot;: 4, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 0.53484553, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;megacrop&quot;, &quot;_type&quot;: &quot;employee&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.53484553, &quot;_source&quot;: &#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ] &#125;, &quot;highlight&quot;: &#123; &quot;about&quot;: [ &quot;I love to go &lt;em&gt;rock&lt;/em&gt; &lt;em&gt;climbing&lt;/em&gt;&quot; ] &#125; &#125; ] &#125;&#125; 6 聚合最后，我们还有一个需求需要完成：允许管理者在职员目录中进行一些分析。 Elasticsearch有一个功能叫做聚合(aggregations)，它允许你在数据上生成复杂的分析统计。它很像SQL中的GROUP BY但是功能更强大。举个例子，让我们找到所有职员中最大的共同点（兴趣爱好）是什么： 12345678POST megacrop/employee/_search&#123; \"aggregations\": &#123; \"all_interests\": &#123; \"terms\": &#123; \"field\": \"interests\" &#125; &#125; &#125;&#125; 如果报错 1234&#123; \"type\": \"illegal_argument_exception\", \"reason\": \"Fielddata is disabled on text fields by default. Set fielddata=true on [interests] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.\"&#125; 则配置属性 123456789POST megacrop/_mapping/employee&#123; \"properties\": &#123; \"interests\": &#123; \"type\": \"text\", \"fielddata\": true &#125; &#125;&#125; 返回聚合结果 123456789101112131415161718192021\"aggregations\": &#123; \"all_interests\": &#123; \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ &#123; \"key\": \"music\", \"doc_count\": 2 &#125;, &#123; \"key\": \"forestry\", \"doc_count\": 1 &#125;, &#123; \"key\": \"sports\", \"doc_count\": 1 &#125; ] &#125;&#125; 嵌套聚合查询 12345678910111213POST /megacorp/employee/_search&#123; \"aggs\" : &#123; \"all_interests\" : &#123; \"terms\" : &#123; \"field\" : \"interests\" &#125;, \"aggs\" : &#123; \"avg_age\" : &#123; \"avg\" : &#123; \"field\" : \"age\" &#125; &#125; &#125; &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829\"aggregations\": &#123; \"all_interests\": &#123; \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ &#123; \"key\": \"music\", \"doc_count\": 2, \"avg_age\": &#123; \"value\": 28.5 &#125; &#125;, &#123; \"key\": \"forestry\", \"doc_count\": 1, \"avg_age\": &#123; \"value\": 35.0 &#125; &#125;, &#123; \"key\": \"sports\", \"doc_count\": 1, \"avg_age\": &#123; \"value\": 25.0 &#125; &#125; ] &#125; &#125; 7 分布式特性简介在章节的开始我们提到Elasticsearch可以扩展到上百（甚至上千）的服务器来处理PB级的数据。然而我们的教程只是给出了一些使用Elasticsearch的例子，并未涉及相关机制。Elasticsearch为分布式而生，而且它的设计隐藏了分布式本身的复杂性。Elasticsearch在分布式概念上做了很大程度上的透明化，在教程中你不需要知道任何关于分布式系统、分片、集群发现或者其他大量的分布式概念。所有的教程你既可以运行在你的笔记本上，也可以运行在拥有100个节点的集群上，其工作方式是一样的。Elasticsearch致力于隐藏分布式系统的复杂性。以下这些操作都是在底层自动完成的：将你的文档分区到不同的容器或者分片(shards)中，它们可以存在于一个或多个节点中。将分片均匀的分配到各个节点，对索引和搜索做负载均衡。冗余每一个分片，防止硬件故障造成的数据丢失。将集群中任意一个节点上的请求路由到相应数据所在的节点。无论是增加节点，还是移除节点，分片都可以做到无缝的扩展和迁移。 8 小结现在你对Elasticsearch可以做些什么以及其易用程度有了大概的了解。Elasticsearch致力于降低学习成本和轻松配置。学习Elasticsearch最好的方式就是开始使用它：开始索引和检索吧！当然，你越是了解Elasticsearch，你的生产力就越高。你越是详细告诉Elasticsearch你的应用的数据特点，你就越能得到准确的输出。本书其余部分将帮助你从新手晋级到专家。每一个章节都会阐述一个要点，并且会包含专家级别的技巧。如果你只是刚起步，那么这些技巧可能暂时和你无关。Elasticsearch有合理的默认配置而且可以在没有用户干预的情况下做正确的事情。当需要提升性能时你可以随时回顾这些章节。","tags":[{"name":"搜索","slug":"搜索","permalink":"https://seawaylee.github.io/tags/搜索/"}]},{"title":"Lucene学籍笔记","date":"2017-03-08T16:55:41.000Z","path":"2017/03/09/搜索/Lucene学习笔记/","text":"1 全文检索 和 索引1.1 索引将非结构化数据中的一部分信息 取出来，重新组织，使其变得有一定结构，然后对此有 一定结构的数据进行搜索，从而达到搜索相对较快的目的。这部分从非结构化数据中 取出的然 后重新组织的信息，我们称之索引。 例如:字典。字典的拼音表和部首检字表就相当于字典的索引，对每一个字的解释是非结 构化的，如果字典没有音节表和部首检字表，在茫茫辞海中找一个字只能顺序扫 。然而字的某 些信息可以 取出来进行结构化处理，比如读音，就比较结构化，分声母和韵母，分别只有几种 可以一一列举，于是将读音拿出来按一定的顺序排列，每一项读音都指向此字的详细解释的页数。 我们搜索时按结构化的拼音搜到读音，然后按其指向的页数，便可找到我们的非结构化数据—— 也即对字的解释。 1.2 全文检索这种先建立索引，再对索引进行搜索的过程就叫全文检索(Full-text Search)。 2 Lucene实现全文检索的流程2.1 索引和搜索流程图 2.2 创建索引将用户需要搜索的文档内容进行索引，索引存储在索引库中。索引库位于本地磁盘中。 2.2.1 获得原始文档可以使用网络爬虫采集一定量的文本数据，比如新闻信息。采集后将数据从数据库中转移到本地磁盘中。 2.2.2 创建文档对象获取原始内容的目的是为了索引，在索引前需要将原始内容创建成文档（Document），文档包括多个域Field和域中存储的内容。我们可以将磁盘中的一个文件当成一个document，document中包含field和对应的value。 2.2.3 分析文档分析的过程是经过对原始文档的提取单词、字母大小写转换、去除标点符号、去除停用词等过程成圣最终的与会单词。比如下面的文档的分析过程如下： 原文：Hello,welcome to the java world!分析结果：Hello、welcome、java、world 每一个单词是一个term，不同的域中拆分出来的相同单词是不同的term。term中包含field和value。分析过程中会涉及到分析器，用来根据语义进行分词，中文的较为复杂。 2.2.4 创建索引对所有文档分析得到的单词进行索引，索引的目的是为了搜索，最终要实现只搜索被索引的单词而找到Document。传统的搜索方式是顺序扫描法，在Lucene中所使用的索引方法是倒排索引法。即根据索引值去查找文档，而不是去遍历文档来扫描关键字。 3 程序实现3.1 准备工作 创建原始文档：在本地磁盘中创建一个目录，将原始文本存储于其中。 创建索引库：在本地磁盘中创建一个空目录，用于存储索引。 3.2 引入相关jar包（POM）1234567891011121314151617181920212223242526272829303132333435363738&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-core&lt;/artifactId&gt; &lt;version&gt;4.10.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.lucene/lucene-queryparser --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-queryparser&lt;/artifactId&gt; &lt;version&gt;4.10.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.lucene/lucene-analyzers-common --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-analyzers-common&lt;/artifactId&gt; &lt;version&gt;4.10.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/commons-io/commons-io --&gt; &lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 3.3 了解相关Field 3.4 测试代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179public class LuceneTest&#123; /** * 创建索引 * * @Author NikoBelic * @Date 07/03/2017 19:01 */ @Test public void testIndexCreate() throws IOException &#123; // 指定文档和索引的存储目录 Directory indexDir = FSDirectory.open(new File(\"/Users/lixiwei-mac/Documents/DataSet/lucene/index\")); // 标准分词器(英文效果好,中文单字分词) Analyzer analyzer = new IKAnalyzer(); IndexWriterConfig config = new IndexWriterConfig(Version.LATEST, analyzer); IndexWriter indexWriter = new IndexWriter(indexDir, config); // 采集文档中的数据放入Lucene中 File sourceDir = new File(\"/Users/lixiwei-mac/Documents/DataSet/lucene/searchsource\"); for (File f : sourceDir.listFiles()) &#123; String fileName = f.getName(); String fileContent = FileUtils.readFileToString(f); String filePath = f.getPath(); long fileSize = FileUtils.sizeOf(f); // (field_name,field_value,need_stored?) Field fileNameField = new TextField(\"filename\", fileName, Field.Store.YES); Field fileContentField = new TextField(\"content\", fileContent, Field.Store.NO); Field filepPthField = new StoredField(\"path\", filePath); Field fileSizeField = new LongField(\"size\", fileSize, Field.Store.YES); Document document = new Document(); document.add(fileNameField); document.add(fileContentField); document.add(filepPthField); document.add(fileSizeField); // 这里会自动创建索引 indexWriter.addDocument(document); &#125; indexWriter.close(); &#125; /** * 使用索引搜索 * * @Author NikoBelic * @Date 07/03/2017 19:01 */ @Test public void testIndexSearch() throws IOException, ParseException &#123; // 指定索引库存放路径 Directory indexDir = FSDirectory.open(new File(\"/Users/lixiwei-mac/Documents/DataSet/lucene/index\")); // 创建索引Reader、Searcher对象 IndexReader indexReader = DirectoryReader.open(indexDir); IndexSearcher indexSearcher = new IndexSearcher(indexReader); // 创建查询 // 方法1 Query query = new TermQuery(new Term(\"content\", \"java\")); // 执行查询,(查询对象,查询结果返回最大值) // 方法2 // 创建分词器(必须和创建索引所用分词器一致) Analyzer analyzer = new IKAnalyzer(); // 默认搜索域作用:如果搜索语法中没有指定域名,则使用默认域名搜索 QueryParser queryParser = new QueryParser(\"filename\", analyzer); // 查询语法:域名:搜索关键字 Query query2 = queryParser.parse(\"apache\"); TopDocs topDocs = indexSearcher.search(query2, 5); System.out.println(\"查询结果的总条数:\" + topDocs.totalHits); // 遍历查询结果 for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; // scoreDoc.doc = 自动生成的文档ID Document document = indexSearcher.doc(scoreDoc.doc); System.out.println(document.get(\"filename\")); System.out.println(scoreDoc.toString()); System.out.println(\"======================================================\"); &#125; indexReader.close(); &#125; @Test public void testDelIndex() throws IOException &#123; Analyzer analyzer = new IKAnalyzer(); Directory indexDir = FSDirectory.open(new File(\"/Users/lixiwei-mac/Documents/DataSet/lucene/index\")); IndexWriterConfig config = new IndexWriterConfig(Version.LATEST, analyzer); IndexWriter indexWriter = new IndexWriter(indexDir, config); // 删除所有 //indexWriter.deleteAll(); // Term 词源,(域名,删除含有这些关键词的数据) indexWriter.deleteDocuments(new Term(\"filename\", \"apache\")); indexWriter.commit(); indexWriter.close(); &#125; /** * 更新就是按照传入的Term进行搜索,如果找到结果那么删除,将更新的内容重新生成一个Document对象 * 如果没有搜索到结果,那么将更新的内容直接添加一个新的Document对象 * * @Author NikoBelic * @Date 07/03/2017 20:57 */ @Test public void testUpdateIndex() throws IOException &#123; Analyzer analyzer = new IKAnalyzer(); Directory indexDir = FSDirectory.open(new File(\"/Users/lixiwei-mac/Documents/DataSet/lucene/index\")); IndexWriterConfig config = new IndexWriterConfig(Version.LATEST, analyzer); IndexWriter indexWriter = new IndexWriter(indexDir, config); Document doc = new Document(); doc.add(new TextField(\"filename\", \"更新检索测试.txt\", Field.Store.YES)); doc.add(new TextField(\"content\", \"文件内容测试\", Field.Store.NO)); doc.add(new LongField(\"size\", 100L, Field.Store.YES)); indexWriter.updateDocument(new Term(\"filename\", \"检索\"), doc); indexWriter.commit(); indexWriter.close(); &#125; /** * 根据索引查询,多种查询 * @Author NikoBelic * @Date 08/03/2017 13:02 */ @Test public void testSearch() throws IOException, ParseException &#123; Directory indexDir = FSDirectory.open(new File(\"/Users/lixiwei-mac/Documents/DataSet/lucene/index\")); IndexReader indexReader = DirectoryReader.open(indexDir); IndexSearcher indexSearcher = new IndexSearcher(indexReader); // 根据文本查询 Query termQuery = new TermQuery(new Term(\"filename\", \"apache\")); // 根据数字范围查询 Query numQuery = NumericRangeQuery.newLongRange(\"size\", 100L, 800L, true, true); // Bool查询 BooleanQuery boolQuery = new BooleanQuery(); boolQuery.add(termQuery, BooleanClause.Occur.MUST); // 独自使用MUST_NOT没有任何意义 boolQuery.add(numQuery, BooleanClause.Occur.MUST); // 查询所有文档 MatchAllDocsQuery matchAllDocsQuery = new MatchAllDocsQuery(); // 多个域的查询,或 关系 String[] fields = &#123;\"filename\", \"content\"&#125;; MultiFieldQueryParser multiFieldQueryParser = new MultiFieldQueryParser(fields, new IKAnalyzer()); Query multiFieldQuery = multiFieldQueryParser.parse(\"apache\"); //TopDocs topDocs = indexSearcher.search(boolQuery, 10); //TopDocs topDocs = indexSearcher.search(matchAllDocsQuery, 10); TopDocs topDocs = indexSearcher.search(multiFieldQuery, 10); System.out.println(\"符合条件的文档数:\" + topDocs.totalHits); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; Document document = indexSearcher.doc(scoreDoc.doc); System.out.println(document.get(\"filename\")); System.out.println(document.get(\"size\")); System.out.println(\"======================================================================\"); &#125; &#125;&#125; 3.5 使用Luke工具查看索引库","tags":[{"name":"搜索","slug":"搜索","permalink":"https://seawaylee.github.io/tags/搜索/"}]},{"title":"面试-手撕代码（持续更新）","date":"2016-09-18T17:28:10.000Z","path":"2016/09/19/面试/知识点/手撕代码/","text":"快排 1234567891011121314151617181920212223242526272829303132333435363738394041public class QuickSort&#123; public static void quickSort(int[] a, int left, int right) &#123; if (left &lt; right) &#123; int i = left, j = right; // 以中间数为基准 //SortUtil.swap(a, left, (left + right) / 2); // 以随机数为基准 SortUtil.swap(a, left, SortUtil.getRandomIndex(left, right)); int x = a[left]; // a[left] 就是被挖出的第一个坑 while (i &lt; j) &#123; // 从右向左找小于x的数来填a[i] while (i &lt; j &amp;&amp; a[j] &gt; x) j--; if (i &lt; j) a[i++] = a[j]; // 将a[j]填到a[i]，a[j]形成了新的坑 // 从左向右找大于x的数来填a[j] while (i &lt; j &amp;&amp; a[i] &lt; x) i++; if (i &lt; j) a[j--] = a[i];// 将[i]天到a[j],a[i]形成了新的坑 &#125; a[i] = x;// 退出时，i等于j，将x填到这个坑 quickSort(a, left, i - 1); quickSort(a, i + 1, right); &#125; &#125; public static void main(String[] args) &#123; int[] a = SortUtil.getRandomArray(10); SortUtil.printArray(a); quickSort(a, 0, a.length - 1); SortUtil.printArray(a); &#125;&#125; 二分查找 12345678910111213141516171819202122232425262728293031323334package interview;import java.util.Arrays;import java.util.Random;/** * @author NikoBelic * @create 05/01/2017 10:01 */public class Search&#123; /** * 折半查找 * @Author NikoBelic * @Date 09/01/2017 01:31 */ public static &lt;T extends Comparable&lt;T&gt;&gt; int binarySearch(T[] list,T key) &#123; int mid = 0; int high = list.length - 1; int low = 0; while (low &lt;= high) &#123; mid = (high + low) &gt;&gt;&gt; 1; if (list[mid].compareTo(key) == 0) return mid; if (list[mid].compareTo(key) &gt; 0) high = mid - 1; if (list[mid].compareTo(key) &lt; 0) low = mid + 1; &#125; return -1; &#125;&#125;","tags":[{"name":"面试","slug":"面试","permalink":"https://seawaylee.github.io/tags/面试/"}]},{"title":"面试知识点（持续更新）","date":"2016-09-18T17:25:44.000Z","path":"2016/09/19/面试/知识点/面试知识点(持续更新)/","text":"知识点链接：https://www.nowcoder.com/discuss/29890 1基础知识： 算法和数据结构 数组、链表、二叉树、队列、栈的各种操作（性能，场景） 二分查找和各种变种的二分查找 各类排序算法以及复杂度分析（快排、归并、堆） 各类算法题（手写） 理解并可以分析时间和空间复杂度。 动态规划（笔试回回有。。）、贪心。 红黑树、AVL树、Hash树、Tire树、B树、B+树。 图算法（比较少，也就两个最短路径算法理解吧） 计算机网络 OSI7层模型（TCP4层） 每层的协议 url到页面的过程 HTTP http/https 1.0、1.1、2.0 get/post 以及幂等性 区别 GET一般用于获取/查询资源信息，POST一般用于更新资源信息。 http 协议头相关 网络攻击（CSRF、XSS） TCP/IP 三次握手、四次挥手 拥塞控制（过程、阈值） 流量控制与滑动窗口 TCP与UDP比较 子网划分（一般只有笔试有） DDos攻击 (B)IO/NIO/AIO 三者原理，各个语言是怎么实现的 IO InputStream、OutputStream、Reader、Writer 阻塞IO、使用多个线程处理每一个请求、适用于请求请求量小且数据大的情况 NIO ByteBuffer、Channel、Selector 非阻塞IO、使用单线程管理多个通道、适用于请求量大且数据小的情况 Netty Linux内核select poll epoll 数据库（最多的还是mysql，Nosql有redis） 索引（包括分类及优化方式，失效条件，底层结构） sql语法（join，union，子查询，having，group by） Oracle分页 1234567SELECT * FROM ( SELECT A.*, ROWNUM RN FROM (SELECT * FROM TABLE_NAME) A WHERE ROWNUM &lt;= 40 ) WHERE RN &gt;= 21 引擎对比（InnoDB，MyISAM） 数据库的锁（行锁，表锁，页级锁，意向锁，读锁，写锁，悲观锁，乐观锁，以及加锁的select sql方式） 锁相关内容 表级锁、行级锁、页级锁这三种是在锁的颗粒度上的对比 共享锁、排他锁是在锁的级别上来划分； 共享锁 SELECT …LOCK IN SHARE MODE; 当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请共享锁，否则会被阻塞。; 排他锁 SELECT …LOCK FOR UPDATE; 当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请排他锁，否则会被阻塞。行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁 乐观锁、悲观锁 针对的是使用方式； 悲观锁 如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。 具体响应方式由开发者根据实际需要决定。悲观并发控制主要用于数据争用激烈的环境 乐观锁 在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。如果其他事务有更新的话，正在提交的事务会进行回滚。乐观并发控制多数用于数据争用不大、冲突较少的环境中 隔离级别，依次解决的问题（脏读、不可重复读、幻读） DEFAULT 默认隔离级别，每种数据库支持的事务隔离级别不一样，如果Spring配置事务时将isolation设置为这个值的话，那么将使用底层数据库的默认事务隔离级别。顺便说一句，如果使用的MySQL，可以使用”select @@tx_isolation”来查看默认的事务隔离级别 READ_UNCOMMITTED 读未提交，即能够读取到没有被提交的数据，所以很明显这个级别的隔离机制无法解决脏读、不可重复读、幻读中的任何一种，因此很少使用 READ_COMMITED 读已提交，即能够读到那些已经提交的数据，自然能够防止脏读，但是无法限制不可重复读和幻读 REPEATABLE_READ 重复读取，即在数据读出来之后加锁，类似”select * from XXX for update”，明确数据读取出来就是为了更新用的，所以要加一把锁，防止别人修改它。REPEATABLE_READ 的意思也类似，读取了一条数据，这个事务不结束，别的事务就不可以改这条记录，这样就解决了脏读、不可重复读的问题，但是幻读的问题还是无法解决 SERLALIZABLE 串行化，最高的事务隔离级别，不管多少事务，挨个运行完一个事务的所有子事务之后才可以执行另外一个事务里面的所有子事务，这样就解决了脏读、不可重复读和幻读的问题了 事务的ACID ACID是原子性（atomicity） 一致性（consistency） 隔离性（isolation） 持久性（durability） B树、B+树 B-树 所有键值分布在整颗树中； 任何一个关键字出现且只出现在一个结点中； 搜索有可能在非叶子结点结束； 在关键字全集内做一次查找,性能逼近二分查找； B+ 树 所有关键字存储在叶子节点出现,内部节点(非叶子节点并不存储真正的 data) 为所有叶子结点增加了一个链指针 为什么使用B+树最为Mysql引擎 B+树更适合外部存储,由于内节点无 data 域,一个结点可以存储更多的内结点,每个节点能索引的范围更大更精确,也意味着 B+树单次磁盘IO的信息量大于B-树,I/O效率更高。 Mysql是一种关系型数据库，区间访问是常见的一种情况，B+树叶节点增加的链指针,加强了区间访问性，可使用在范围区间查询等，而B-树每个节点 key 和 data 在一起，则无法区间查找。 优化（explain，慢查询，show profile） 数据库的范式。 分库分表，主从复制，读写分离。 Nosql相关（redis和memcached区别之类的，如果你熟悉redis，redis还有一堆要问的） memcached - 一致性Hash算法 操作系统： 进程通信IPC（几种方式），与线程区别 OS的几种策略（页面置换，进程调度等，每个里面有几种算法） 互斥与死锁相关的 linux常用命令（问的时候都会给具体某一个场景） Linux内核相关（select、poll、epoll） 编程语言（这里只说Java）： 把我之后的面经过一遍，Java感觉覆盖的就差不多了，不过下面还是分个类。 Java基础（面向对象、四个特性、重载重写、static和final等等很多东西） 集合（HashMap、ConcurrentHashMap、各种List，最好结合源码看） 并发和多线程（线程池、SYNC和Lock锁机制、线程通信、volatile、ThreadLocal、CyclicBarrier、Atom包、CountDownLatch、AQS、CAS原理等等） JVM（内存模型、GC垃圾回收，包括分代，GC算法，收集器、类加载和双亲委派、JVM调优，内存泄漏和内存溢出） 内存模型 程序计数器 它是当前线程所执行的字节码的行号指示器 虚拟机栈 生命周期和线程相同。每个方法执行的同时都会创建一个栈帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息，每一个方法从调用直至执行完毕的过程，就对应着一个栈帧在虚拟机中入栈到出栈的过程。 本地方法栈 和虚拟机栈起的作用一样，只不过方法栈为虚拟机使用到的Native方法服务。虚拟机规范并没有对这个区域有什么强制规定，因此我们使用的HotSpot虚拟机，就干脆没有这块区域了，它和虚拟机栈是一起的。 堆 此内存唯一的目的就是存放对象实 方法区 存储虚拟机 加载的类信息、常量、静态变量、即时编译器编译后的代码等数据 哪些对象需要回收？ 1、引用计数法 2、可达性分析法（一下列出GCROOT对象） 虚拟机栈中引用的对象 方法区中静态属性引用的对象 方法区中常量引用的对象 本地方法栈中JNI（即Native方法）引用的对象 4种引用状态 强引用 代码中普遍存在的类似”Object obj = new Object()”这类的引用，只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象 软引用 描述有些还有用但并非必需的对象。在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围进行二次回收。如果这次回收还没有足够的内存，才会抛出内存溢出异常。Java中的类SoftReference表示软引用 弱引用 描述非必需对象。被弱引用关联的对象只能生存到下一次垃圾回收之前，垃圾收集器工作之后，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。Java中的类WeakReference表示弱引用 虚引用 这个引用存在的唯一目的就是在这个对象被收集器回收时收到一个系统通知，被虚引用关联的对象，和其生存时间完全没关系。Java中的类PhantomReference表示虚引用 方法区回收 常量回收条件 当前系统中没有任何一处引用该常量 类回收条件 1、该类所有实例都已经被回收，也就是说Java堆中不存在该类的任何实例 2、加载该类的ClassLoader已经被回收 3、该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法 垃圾回收算法 1、标记-清除（Mark-Sweep）算法 2、复制（Copying）算法 现在的商用虚拟机都采用这种算法来回收新生代 新生代的内存被划分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor。 3、标记-整理（Mark-Compact）算法 垃圾收集器 1、Serial收集器 反射和代理、异常、Java8相关、序列化 反射 1、反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为java语言的反射机制。 2、作用：1在运行时判断任意一个对象所属的类；2在运行时构造任意一个类的对象；3在运行时判断任意一个类所具有的成员变量和方法；4在运行时调用任意一个对象的方法；5生成动态代理。 动态代理 InvocationHandler JDK MethodInterceptor CGLIB 设计模式（常用的，jdk中有的） Web相关（servlet、cookie/session、Spring、Mybatis、Tomcat、Hibernate等） Spring 依赖注入（DI） 反转控制（IOC） 面向切面编程（AOP） 1、创建被调用者工作由spring完成，注入调用者，称为依赖注入 2、反转控制指不通过调用者new被调用者，通过反射实现 3、AOP跟动态代理差不多，源代码无关性和各个步骤之间良好的隔离性 Servlet 加载和实例化 在第一次请求Servlet时，Servlet容器将会创建Servlet实例 初始化阶段 调用init()方法 响应客户请求阶段 调用service()方法 终止阶段 调用destroy()方法 DispatcherServlet作用 1、文件上传解析，如果请求类型是multipart将通过MultipartResolver进行文件上传解析； 2、通过HandlerMapping，将请求映射到处理器（返回一个HandlerExecutionChain，它包括一个处理器、多个HandlerInterceptor拦截器）； 3、通过HandlerAdapter支持多种类型的处理器(HandlerExecutionChain中的处理器)； 4、通过ViewResolver解析逻辑视图名到具体视图实现； 5、本地化解析； 6、渲染具体的视图等； 7、如果执行过程中遇到异常将交给HandlerExceptionResolver来解析。 从以上我们可以看出DispatcherServlet主要负责流程的控制（而且在流程中的每个关键点都是很容易扩展的）。 8、DispatcherServlet重写了doService（抽象方法），doDispatch中通过mappedHandler进行方法定位 项目经历 这个每个人的项目不同，覆盖的技术也不一样，所以不能统一去说。 这里的技巧呢，在下面也会详细说明。 无非是找到自己项目中的亮点，简历上叙述的简练并且吸引眼球，同时自己要很熟悉这个点（毕竟可以提前准备） 最好自己多练，就像有个剧本或者稿子一样，保证面试中可以很熟练通俗地讲出，并且让人听着很舒服。 其他扩展技能（这个方方面面太多了，全部掌握基本上不可能，只是作为大家其他时间扩充技能的参考） 分布式架构：（了解原理就行，如果真的有实践经验更好） CAP原理和BASE理论。 CAP 1、Consistency(一致性), 数据一致更新，所有数据变动都是同步的 2、Availability(可用性), 好的响应性能 3、Partition tolerance(分区容错性) 可靠性 Nosql与KV存储（redis，hbase，mongodb，memcached等） 服务化理论（包括服务发现、治理等，zookeeper、etcd、springcloud微服务、） 负载均衡（原理、cdn、一致性hash） RPC框架（包括整体的一些框架理论，通信的netty，序列化协议thrift，protobuff等） 消息队列（原理、kafka，activeMQ，rocketMQ） 分布式存储系统（GFS、HDFS、fastDFS）、存储模型（skipList、LSM等） 分布式事务、分布式锁等 分布式锁 基于数据库表做乐观锁，用于分布式锁。 使用memcached的add()方法，用于分布式锁。 使用redis的setnx()、expire()方法，用于分布式锁。 使用redis的setnx()、get()、getset()方法，用于分布式锁。 大数据与数据分析： hadoop生态圈(hive、hbase、hdfs、zookeeper、storm、kafka) spark体系 语言：python、R、scala 搜索引擎与技术 机器学习算法： 模型和算法很多。不细说了，如果很熟练就去投算法，国内很多公司都算法岗都很稀缺，其他岗可以大概了解下理论。 其他工具的理论和使用： git、docker、maven/gradle、Jenkins","tags":[{"name":"面试","slug":"面试","permalink":"https://seawaylee.github.io/tags/面试/"}]}]